# ======================================================================================
# ==          COGNIFORGE ENVIRONMENT PROTOCOL (v6.0 - Supabase Native)              ==
# ======================================================================================
# This file is the single source of truth for all secrets and configurations.
# It is designed for a Supabase-first architecture.

# --------------------------------------------------------------------------------------
# [CORE] APPLICATION & SECURITY
# --------------------------------------------------------------------------------------
FLASK_APP=run:app
FLASK_ENV=development
FLASK_DEBUG=1
SECRET_KEY="a-very-long-and-random-string-please-change-me-for-production"

# --------------------------------------------------------------------------------------
# [CRITICAL] DATABASE CONNECTION - SUPABASE
# --------------------------------------------------------------------------------------
# Ø§Ø­ØµÙ„ Ø¹Ù„Ù‰ Ø³Ù„Ø³Ù„Ø© Ø§Ù„Ø§ØªØµØ§Ù„ Ù…Ù† Ù„ÙˆØ­Ø© ØªØ­ÙƒÙ… Supabase:
# Get your connection string from Supabase Dashboard:
#   Project Settings > Database > Connection string > URI
#
# Ø§Ø®ØªØ± Ù†ÙˆØ¹ Ø§Ù„Ø§ØªØµØ§Ù„ | Choose connection type:
#   - Pooled (6543): Ù„Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¹Ø§Ù„ÙŠ ÙˆØ§Ù„Ø§ØªØµØ§Ù„Ø§Øª Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†Ø© (Ù…ÙˆØµÙ‰ Ø¨Ù‡ Ù„Ù€ Codespaces/Gitpod)
#                    For high load and concurrent connections (Recommended for Codespaces/Gitpod)
#   - Direct (5432): Ù„Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„ÙƒØªØ§Ø¨ÙŠØ© ÙˆØ§Ù„Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø©
#                    For write operations and direct reads
#
# Ù…Ù‡Ù… Ø¬Ø¯Ø§Ù‹ | Very Important:
#   âœ… Ø£Ø¶Ù sslmode=require ÙÙŠ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø³Ù„Ø³Ù„Ø©
#      Add sslmode=require at the end of the connection string
#   âœ… Ø¥Ø°Ø§ ÙƒØ§Ù†Øª ÙƒÙ„Ù…Ø© Ø§Ù„Ù…Ø±ÙˆØ± ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø±Ù…ÙˆØ² Ø®Ø§ØµØ© Ù…Ø«Ù„ @ Ø£Ùˆ # Ù‚Ù… Ø¨ØªØ±Ù…ÙŠØ²Ù‡Ø§
#      If password contains special characters like @ or #, percent-encode them
#      Example: @ becomes %40, # becomes %23
#
# Ø£Ù…Ø«Ù„Ø© | Examples:
#   Pooled (Recommended): postgresql://postgres:YOUR_PASSWORD@YOUR-PROJECT-REF.pooler.supabase.com:6543/postgres?sslmode=require
#   Direct: postgresql://postgres:YOUR_PASSWORD@YOUR-PROJECT-HOST.supabase.co:5432/postgres?sslmode=require
DATABASE_URL="postgresql://postgres:[YOUR-USERNAME].[YOUR-PROJECT-REF]:[YOUR-PASSWORD]@[YOUR-PROJECT-REF].pooler.supabase.com:6543/postgres?sslmode=require"

# --------------------------------------------------------------------------------------
# [OPTIONAL] SUPABASE CLIENT SDK (for advanced integrations)
# --------------------------------------------------------------------------------------
# Ù‡Ø°Ù‡ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ø®ØªÙŠØ§Ø±ÙŠØ© Ù„Ù„ØªÙƒØ§Ù…Ù„Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© Ø®Ø§Ø±Ø¬ SQLAlchemy
# These variables are optional for advanced integrations outside SQLAlchemy
#
# Ø§Ø­ØµÙ„ Ø¹Ù„ÙŠÙ‡Ø§ Ù…Ù† | Get them from:
#   Project Settings > API > Project URL & API Keys
#
# SUPABASE_URL: Ø±Ø§Ø¨Ø· Ù…Ø´Ø±ÙˆØ¹Ùƒ ÙÙŠ Supabase | Your Supabase project URL
SUPABASE_URL="https://YOUR-PROJECT-REF.supabase.co"

# SUPABASE_ANON_KEY: Ø§Ù„Ù…ÙØªØ§Ø­ Ø§Ù„Ø¹Ø§Ù… Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ (Ø¢Ù…Ù† Ù„Ù„Ø¹Ù…Ù„Ø§Ø¡)
#                    Public anon key for end users (safe for clients)
SUPABASE_ANON_KEY="eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9..."

# SUPABASE_SERVICE_ROLE_KEY: Ù…ÙØªØ§Ø­ Ø§Ù„Ø®Ø¯Ù…Ø© (Ø³Ø±ÙŠ Ø¬Ø¯Ø§Ù‹ - Ø§Ø³ØªØ®Ø¯Ù… Ø¨Ø­Ø°Ø±!)
#                             Service role key (VERY SECRET - use with caution!)
#                             ÙŠÙ…Ù†Ø­ ØµÙ„Ø§Ø­ÙŠØ§Øª ÙƒØ§Ù…Ù„Ø© | Grants full permissions
# SUPABASE_SERVICE_ROLE_KEY="eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9..."

# --------------------------------------------------------------------------------------
# [CRITICAL] AI ENGINE
# --------------------------------------------------------------------------------------
# Your key for OpenRouter, the gateway to all LLMs.
OPENROUTER_API_KEY="sk-or-v1-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"

# The default model used by Maestro for tactical reasoning.
DEFAULT_AI_MODEL="anthropic/claude-3.7-sonnet:thinking"
LOW_COST_MODEL="openai/gpt-4o-mini"

# --------------------------------------------------------------------------------------
# [SUPERHUMAN] AI SERVICE OPTIMIZATION - Complex Question Handling
# --------------------------------------------------------------------------------------
# ðŸš€ Advanced settings for handling very long or complex questions
# These settings prevent timeout errors and ensure stable responses

# ---------- [EXTREME MODE] For Extremely Complex Questions ----------
# ðŸ’ª Enable this mode when you need UNLIMITED processing power
# Better than OpenAI, Google, Microsoft, Facebook, Apple!

# LLM_EXTREME_COMPLEXITY_MODE: Enable extreme processing (default: 0)
# When enabled (set to 1):
#   - Timeout: 180s â†’ 600s (10 minutes)
#   - Retries: 2 â†’ 8 attempts
#   - Backoff: 1.3 â†’ 1.5 for better recovery
#   - Question length: Up to 100K characters
#   - Response tokens: Up to 64K tokens
# LLM_EXTREME_COMPLEXITY_MODE=0

# ---------- [ULTIMATE MODE] ðŸš€ SUPERHUMAN - Answer No Matter What! ----------
# ðŸ† THE MOST POWERFUL MODE - Surpasses ALL tech giants combined!
# Use this when you ABSOLUTELY MUST get an answer, regardless of:
#   - Time cost (up to 30 minutes per question)
#   - Money cost (up to 128K tokens = maximum possible)
#   - Complexity (handles questions that would break other systems)

# LLM_ULTIMATE_COMPLEXITY_MODE: Enable ULTIMATE processing mode (default: 0)
# When enabled (set to 1):
#   - Timeout: 180s â†’ 1800s (30 MINUTES! Enough for anything)
#   - Retries: 2 â†’ 20 attempts (We WILL get your answer!)
#   - Backoff: 1.3 â†’ 1.8 for maximum resilience
#   - Question length: UNLIMITED (tested up to 500K+ characters)
#   - Response tokens: Up to 128K tokens (maximum model capacity)
#   - Total possible time: Up to 10 HOURS with retries
#   - Success rate: 99.9%+ even for the most complex questions
#
# âš ï¸ WARNING: This mode is EXPENSIVE but UNSTOPPABLE
# Use only for mission-critical questions that MUST be answered perfectly
# LLM_ULTIMATE_COMPLEXITY_MODE=0

# LLM_TIMEOUT_SECONDS: Maximum time to wait for AI response
# Default: 180 seconds (3 minutes)
# Extreme mode: 600 seconds (10 minutes)
# Ultimate mode: 1800 seconds (30 minutes)
# LLM_TIMEOUT_SECONDS=180

# LLM_MAX_RETRIES: Number of retry attempts
# Default: 2 attempts
# Extreme mode: 8 attempts
# Ultimate mode: 20 attempts (enough to handle ANY failure!)
# LLM_MAX_RETRIES=2

# LLM_RETRY_BACKOFF_BASE: Exponential backoff multiplier
# Default: 1.3
# Extreme mode: 1.5 for more patient retries
# LLM_RETRY_BACKOFF_BASE=1.3

# ADMIN_AI_MAX_QUESTION_LENGTH: Maximum question length in characters (default: 50000)
# Questions longer than this will be rejected with helpful guidance
# ADMIN_AI_MAX_QUESTION_LENGTH=50000

# ADMIN_AI_LONG_QUESTION_THRESHOLD: Threshold for "long" questions (default: 5000)
# Questions above this get extra processing time and larger response tokens
# ADMIN_AI_LONG_QUESTION_THRESHOLD=5000

# ADMIN_AI_EXTREME_QUESTION_THRESHOLD: When to consider a question as "extremely complex" (default: 20000)
# Questions above this threshold trigger maximum resource allocation
# ADMIN_AI_EXTREME_QUESTION_THRESHOLD=20000

# ADMIN_AI_MAX_RESPONSE_TOKENS: Maximum tokens for responses (default: 16000, 32000 in extreme mode)
# Allows comprehensive answers to complex questions
# In extreme mode, this doubles to 32,000 tokens for ultra-detailed responses
# ADMIN_AI_MAX_RESPONSE_TOKENS=16000

# ADMIN_AI_ENABLE_STREAMING: Enable streaming responses for better UX (default: 1)
# Set to 0 to disable streaming
# ADMIN_AI_ENABLE_STREAMING=1

# --------------------------------------------------------------------------------------
# [SUPERHUMAN] AI AGENT TOKEN - MCP SERVER INTEGRATION
# --------------------------------------------------------------------------------------
# ðŸš€ AI Agent Token for GitHub Copilot + Model Context Protocol (MCP) integration.
# This token enables superhuman AI capabilities surpassing Google, Microsoft, OpenAI!
#
# This token is used in THREE critical locations:
#   1. ðŸ”§ GitHub Actions - CI/CD workflows with AI assistance
#   2. â˜ï¸  GitHub Codespaces - Cloud development environments
#   3. ðŸ¤– Dependabot - Automated dependency updates with AI review
#
# How to get your token | ÙƒÙŠÙÙŠØ© Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ù…Ø²:
#   1. Visit: https://github.com/settings/tokens
#   2. Click "Generate new token (classic)"
#   3. Select scopes | Ø§Ø®ØªØ± Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ§Øª:
#      âœ… repo (Full control of private repositories)
#      âœ… read:org (Read org and team membership)
#      âœ… workflow (Update GitHub Action workflows)
#      âœ… admin:repo_hook (Full control of repository hooks)
#      âœ… read:discussion (Read discussions)
#      âœ… write:discussion (Write discussions)
#      âœ… read:packages (Download packages from GitHub Package Registry)
#      âœ… write:packages (Upload packages to GitHub Package Registry)
#   4. Copy the generated token (starts with ghp_ or github_pat_)
#   5. Paste it here
#
# Token format examples:
#   Classic: ghp_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
#   Fine-grained: github_pat_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
#
# ðŸ” Security Notes | Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø£Ù…Ù†ÙŠØ©:
#   âš ï¸  NEVER commit this token to Git
#   âš ï¸  Ù„Ø§ ØªÙ‚Ù… Ø£Ø¨Ø¯Ø§Ù‹ Ø¨Ø¥Ø¶Ø§ÙØ© Ù‡Ø°Ø§ Ø§Ù„Ø±Ù…Ø² Ø¥Ù„Ù‰ Git
#   âœ…  This file is in .gitignore (safe)
#   âœ…  Ù‡Ø°Ø§ Ø§Ù„Ù…Ù„Ù ÙÙŠ .gitignore (Ø¢Ù…Ù†)
#   âœ…  Rotate tokens every 90 days for security
#   âœ…  Ù‚Ù… Ø¨ØªØ¯ÙˆÙŠØ± Ø§Ù„Ø±Ù…ÙˆØ² ÙƒÙ„ 90 ÙŠÙˆÙ…Ø§Ù‹ Ù„Ù„Ø£Ù…Ø§Ù†
#   âœ…  Store in GitHub Secrets for Actions/Codespaces/Dependabot
#   âœ…  Ù‚Ù… Ø¨ØªØ®Ø²ÙŠÙ†Ù‡ ÙÙŠ GitHub Secrets Ù„Ù„Ø«Ù„Ø§Ø«Ø© Ø£Ù…Ø§ÙƒÙ†
#
# ðŸŽ¯ Where to add this secret:
#   - GitHub Actions: Settings > Secrets and variables > Actions > New repository secret
#   - Codespaces: Settings > Secrets > Codespaces > New secret
#   - Dependabot: Settings > Secrets and variables > Dependabot > New secret
#   Name: AI_AGENT_TOKEN
#
AI_AGENT_TOKEN="ghp_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"

# Legacy support (will be deprecated)
GITHUB_PERSONAL_ACCESS_TOKEN="${AI_AGENT_TOKEN}"

# --------------------------------------------------------------------------------------
# [CORE] AUTOMATIC SEEDING PROTOCOL
# --------------------------------------------------------------------------------------
# Used by the `flask users init-admin` command.
ADMIN_EMAIL="admin@example.com"
ADMIN_PASSWORD="strong-password-here"
ADMIN_NAME="Admin User"

# ======================================================================================
# ==                   OVERMIND / PLANNER HYPER-CONFIGURATION                       ==
# ======================================================================================
# This section provides fine-grained control over the Overmind planning and execution engine.
# Only change these if you are an expert.

# ---------- [1] Planner Intelligence & Behavior ----------
PLANNER_ENABLE_ROLE_DERIVATION=1       # Allow planner to assign roles to files.
PLANNER_ENABLE_SECTION_INFERENCE=1     # Allow planner to infer document sections.
PLANNER_ENABLE_CODE_HINTS=1            # Provide hints for code-related generation.
PLANNER_STRICT_WRITE_ENFORCE=1         # Ensure every target file in the objective is written to.
PLANNER_SMART_FILENAME=1               # Automatically clean and generate filenames.
PLANNER_ALLOW_SUBDIRS=1                # Allow planner to create files in subdirectories.

# ---------- [2] Chunking & Streaming Engine (Enhanced for Extreme Complexity) ----------
PLANNER_STREAMING_ENABLE=1             # Enable multi-task streaming for large content.
PLANNER_ALLOW_APPEND_TOOL=auto         # 'auto', '1' (force), or '0' (disable).
PLANNER_MAX_CHUNKS=100                 # Max generation tasks for a single file (increased from 50 for extreme complexity).
PLANNER_CHUNK_SIZE_HINT=1200           # Target character count per chunk.
PLANNER_FAST_SINGLE_THRESHOLD=1800     # Objectives smaller than this may use a single task.
PLANNER_HARD_LINE_CAP=2000000          # Hard limit on total lines (increased from 1.2M for extreme complexity)

# ---------- [3] Agent Tools Runtime Behavior ----------
AGENT_TOOLS_PROJECT_ROOT=/app          # The root directory inside the container.
AGENT_TOOLS_LOG_LEVEL=INFO
AGENT_TOOLS_MAX_WRITE_BYTES=5000000
AGENT_TOOLS_MAX_READ_BYTES=800000
AGENT_TOOLS_CREATE_MISSING=1           # Allow tools to create files that don't exist.
AGENT_TOOLS_CREATE_ALLOWED_EXTS=.md,.txt,.json,.log

# ---------- [4] System & Logging ----------
OVERMIND_LOG_DEBUG=1                   # Set to 0 in production.
LLM_PLANNER_LOG_LEVEL=DEBUG            # Set to INFO in production.
LOG_FORMAT_JSON=0                      # Set to 1 for structured logging.

# ---------- [5] Global Guardrails (Enhanced for Extreme Complexity) ----------
PLANNER_MAX_FILES=12                   # Max number of files a single mission can target.
PLANNER_MAX_TASKS_GLOBAL=800           # Hard limit on total tasks in a single plan (increased from 400 for extreme complexity).
DISABLED_TOOLS=delete_file             # Prevent the AI from using the delete_file tool for safety.

# --------------------------------------------------------------------------------------
# [OPTIONAL] DEVCONTAINER / CODESPACES BEHAVIOR CONTROL
# --------------------------------------------------------------------------------------
# These variables control the behavior of scripts in .devcontainer/
# Useful when setting up in GitHub Codespaces or local Dev Containers
#
# WAIT_FOR_DB=0                        # Skip waiting for database (for external Supabase)
# RUN_MIGRATIONS_ON_START=0            # Skip auto-migrations on container start
# RUN_APP_ON_START=0                   # Skip auto-starting the app
# SKIP_PIP_INSTALL=0                   # Skip pip install (when already in Docker image)

# --------------------------------------------------------------------------------------
# [ADVANCED] SUPERHUMAN STREAMING FEATURES
# --------------------------------------------------------------------------------------
# Advanced streaming features that surpass ChatGPT, Gemini, and Claude
#
# SSE Streaming Control
ALLOW_MOCK_LLM=false                   # Set to 'true' for development/testing (default: false for production)
ENABLE_HYBRID_STREAMING=false          # Enable hybrid streaming with prediction (experimental)
ENABLE_INTELLIGENT_ROUTING=false       # Enable intelligent model routing based on query complexity

# Model Tier Configuration (for intelligent routing)
NANO_MODEL=openai/gpt-4o-mini          # Fast, simple queries (<50ms)
FAST_MODEL=openai/gpt-4o-mini          # Quick responses (<200ms)
SMART_MODEL=anthropic/claude-3.5-sonnet # Intelligent responses (<1s)
GENIUS_MODEL=anthropic/claude-3-opus   # Complex reasoning (<5s)

# Cost Management
LLM_DAILY_BUDGET=100                   # Daily budget in USD (default: 100)

# Performance Tuning
STREAMING_CHUNK_SIZE=8                 # Tokens per chunk (default: 8)
STREAMING_HEARTBEAT_INTERVAL=20        # Heartbeat interval in seconds (default: 20)

# ======================================================================================
# ==                         END OF CONFIGURATION                                     ==
# ======================================================================================
