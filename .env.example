# ======================================================================================
# ==          COGNIFORGE ENVIRONMENT PROTOCOL (v6.0 - Supabase Native)              ==
# ======================================================================================
# This file is the single source of truth for all secrets and configurations.
# It is designed for a Supabase-first architecture.

# --------------------------------------------------------------------------------------
# [CORE] APPLICATION & SECURITY
# --------------------------------------------------------------------------------------
SECRET_KEY="a-very-long-and-random-string-please-change-me-for-production"

# --------------------------------------------------------------------------------------
# [CRITICAL] DATABASE CONNECTION - SUPABASE
# --------------------------------------------------------------------------------------
# ÿßÿ≠ÿµŸÑ ÿπŸÑŸâ ÿ≥ŸÑÿ≥ŸÑÿ© ÿßŸÑÿßÿ™ÿµÿßŸÑ ŸÖŸÜ ŸÑŸàÿ≠ÿ© ÿ™ÿ≠ŸÉŸÖ Supabase:
# Get your connection string from Supabase Dashboard:
#   Project Settings > Database > Connection string > URI
#
# ÿßÿÆÿ™ÿ± ŸÜŸàÿπ ÿßŸÑÿßÿ™ÿµÿßŸÑ | Choose connection type:
#   - Pooled (6543): ŸÑŸÑÿ™ÿ≠ŸÖŸäŸÑ ÿßŸÑÿπÿßŸÑŸä ŸàÿßŸÑÿßÿ™ÿµÿßŸÑÿßÿ™ ÿßŸÑŸÖÿ™ÿ≤ÿßŸÖŸÜÿ© (ŸÖŸàÿµŸâ ÿ®Ÿá ŸÑŸÄ Codespaces/Gitpod)
#                    For high load and concurrent connections (Recommended for Codespaces/Gitpod)
#   - Direct (5432): ŸÑŸÑÿπŸÖŸÑŸäÿßÿ™ ÿßŸÑŸÉÿ™ÿßÿ®Ÿäÿ© ŸàÿßŸÑŸÇÿ±ÿßÿ°ÿ© ÿßŸÑŸÖÿ®ÿßÿ¥ÿ±ÿ©
#                    For write operations and direct reads
#
# ŸÖŸáŸÖ ÿ¨ÿØÿßŸã | Very Important:
#   ‚úÖ ÿ£ÿ∂ŸÅ sslmode=require ŸÅŸä ŸÜŸáÿßŸäÿ© ÿßŸÑÿ≥ŸÑÿ≥ŸÑÿ©
#      Add sslmode=require at the end of the connection string
#   ‚úÖ ÿ•ÿ∞ÿß ŸÉÿßŸÜÿ™ ŸÉŸÑŸÖÿ© ÿßŸÑŸÖÿ±Ÿàÿ± ÿ™ÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ÿ±ŸÖŸàÿ≤ ÿÆÿßÿµÿ© ŸÖÿ´ŸÑ @ ÿ£Ÿà # ŸÇŸÖ ÿ®ÿ™ÿ±ŸÖŸäÿ≤Ÿáÿß
#      If password contains special characters like @ or #, percent-encode them
#      Example: @ becomes %40, # becomes %23
#
# ÿ£ŸÖÿ´ŸÑÿ© | Examples:
#   Pooled (Recommended): postgresql://postgres:YOUR_PASSWORD@YOUR-PROJECT-REF.pooler.supabase.com:6543/postgres?sslmode=require
#   Direct: postgresql://postgres:YOUR_PASSWORD@YOUR-PROJECT-HOST.supabase.co:5432/postgres?sslmode=require
DATABASE_URL="postgresql://postgres:[YOUR-USERNAME].[YOUR-PROJECT-REF]:[YOUR-PASSWORD]@[YOUR-PROJECT-REF].pooler.supabase.com:6543/postgres?sslmode=require"

# --------------------------------------------------------------------------------------
# [OPTIONAL] SUPABASE CLIENT SDK (for advanced integrations)
# --------------------------------------------------------------------------------------
# Ÿáÿ∞Ÿá ÿßŸÑŸÖÿ™ÿ∫Ÿäÿ±ÿßÿ™ ÿßÿÆÿ™Ÿäÿßÿ±Ÿäÿ© ŸÑŸÑÿ™ŸÉÿßŸÖŸÑÿßÿ™ ÿßŸÑŸÖÿ™ŸÇÿØŸÖÿ© ÿÆÿßÿ±ÿ¨ SQLAlchemy
# These variables are optional for advanced integrations outside SQLAlchemy
#
# ÿßÿ≠ÿµŸÑ ÿπŸÑŸäŸáÿß ŸÖŸÜ | Get them from:
#   Project Settings > API > Project URL & API Keys
#
# SUPABASE_URL: ÿ±ÿßÿ®ÿ∑ ŸÖÿ¥ÿ±ŸàÿπŸÉ ŸÅŸä Supabase | Your Supabase project URL
SUPABASE_URL="https://YOUR-PROJECT-REF.supabase.co"

# SUPABASE_ANON_KEY: ÿßŸÑŸÖŸÅÿ™ÿßÿ≠ ÿßŸÑÿπÿßŸÖ ŸÑŸÑŸÖÿ≥ÿ™ÿÆÿØŸÖ ÿßŸÑŸÜŸáÿßÿ¶Ÿä (ÿ¢ŸÖŸÜ ŸÑŸÑÿπŸÖŸÑÿßÿ°)
#                    Public anon key for end users (safe for clients)
SUPABASE_ANON_KEY="eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9..."

# SUPABASE_SERVICE_ROLE_KEY: ŸÖŸÅÿ™ÿßÿ≠ ÿßŸÑÿÆÿØŸÖÿ© (ÿ≥ÿ±Ÿä ÿ¨ÿØÿßŸã - ÿßÿ≥ÿ™ÿÆÿØŸÖ ÿ®ÿ≠ÿ∞ÿ±!)
#                             Service role key (VERY SECRET - use with caution!)
#                             ŸäŸÖŸÜÿ≠ ÿµŸÑÿßÿ≠Ÿäÿßÿ™ ŸÉÿßŸÖŸÑÿ© | Grants full permissions
# SUPABASE_SERVICE_ROLE_KEY="eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9..."

# --------------------------------------------------------------------------------------
# [CRITICAL] AI ENGINE
# --------------------------------------------------------------------------------------
# Your key for OpenRouter, the gateway to all LLMs.
OPENROUTER_API_KEY="sk-or-v1-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# üß† AI MODEL CONFIGURATION
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# ‚ö†Ô∏è  IMPORTANT: AI Models are NOW configured in CODE, not here!
# ‚ö†Ô∏è  ŸÖŸáŸÖ: ŸÜŸÖÿßÿ∞ÿ¨ ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸä ÿ™ŸèŸÉŸàŸéŸëŸÜ ÿßŸÑÿ¢ŸÜ ŸÅŸä ÿßŸÑŸÉŸàÿØ ŸàŸÑŸäÿ≥ ŸáŸÜÿß!
#
# üìç LOCATION: app/core/ai_config.py ‚Üí class ActiveModels
# üìç ÿßŸÑŸÖŸàŸÇÿπ: app/core/ai_config.py ‚Üí class ActiveModels
#
# üîß To change AI models:
#    1. Open: app/core/ai_config.py
#    2. Find: class ActiveModels
#    3. Change the model values directly
#    4. Save and restart
#
# üîß ŸÑÿ™ÿ∫ŸäŸäÿ± ŸÜŸÖÿßÿ∞ÿ¨ AI:
#    1. ÿßŸÅÿ™ÿ≠: app/core/ai_config.py
#    2. ÿßÿ®ÿ≠ÿ´ ÿπŸÜ: class ActiveModels
#    3. ÿ∫ŸäŸëÿ± ŸÇŸäŸÖ ÿßŸÑŸÜŸÖÿßÿ∞ÿ¨ ŸÖÿ®ÿßÿ¥ÿ±ÿ©
#    4. ÿßÿ≠ŸÅÿ∏ Ÿàÿ£ÿπÿØ ÿßŸÑÿ™ÿ¥ÿ∫ŸäŸÑ
#
# üìã Available Models | ÿßŸÑŸÜŸÖÿßÿ∞ÿ¨ ÿßŸÑŸÖÿ™ÿßÿ≠ÿ©:
#    openai/gpt-4o, openai/gpt-4o-mini, openai/gpt-4-turbo
#    anthropic/claude-3.7-sonnet:thinking, anthropic/claude-3.5-sonnet, anthropic/claude-3-opus
#    google/gemini-pro, meta-llama/llama-3-70b-instruct
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

# --------------------------------------------------------------------------------------
# [SUPERHUMAN] AI SERVICE OPTIMIZATION - Complex Question Handling
# --------------------------------------------------------------------------------------
# üöÄ Advanced settings for handling very long or complex questions
# These settings prevent timeout errors and ensure stable responses

# ---------- [EXTREME MODE] For Extremely Complex Questions ----------
# üí™ Enable this mode when you need UNLIMITED processing power
# Better than OpenAI, Google, Microsoft, Facebook, Apple!

# LLM_EXTREME_COMPLEXITY_MODE: Enable extreme processing (default: 0)
# When enabled (set to 1):
#   - Timeout: 180s ‚Üí 600s (10 minutes)
#   - Retries: 2 ‚Üí 8 attempts
#   - Backoff: 1.3 ‚Üí 1.5 for better recovery
#   - Question length: Up to 100K characters
#   - Response tokens: Up to 64K tokens
# LLM_EXTREME_COMPLEXITY_MODE=0

# ---------- [ULTIMATE MODE] üöÄ SUPERHUMAN - Answer No Matter What! ----------
# üèÜ THE MOST POWERFUL MODE - Surpasses ALL tech giants combined!
# Use this when you ABSOLUTELY MUST get an answer, regardless of:
#   - Time cost (up to 30 minutes per question)
#   - Money cost (up to 128K tokens = maximum possible)
#   - Complexity (handles questions that would break other systems)

# LLM_ULTIMATE_COMPLEXITY_MODE: Enable ULTIMATE processing mode (default: 0)
# When enabled (set to 1):
#   - Timeout: 180s ‚Üí 1800s (30 MINUTES! Enough for anything)
#   - Retries: 2 ‚Üí 20 attempts (We WILL get your answer!)
#   - Backoff: 1.3 ‚Üí 1.8 for maximum resilience
#   - Question length: UNLIMITED (tested up to 500K+ characters)
#   - Response tokens: Up to 128K tokens (maximum model capacity)
#   - Total possible time: Up to 10 HOURS with retries
#   - Success rate: 99.9%+ even for the most complex questions
#
# ‚ö†Ô∏è WARNING: This mode is EXPENSIVE but UNSTOPPABLE
# Use only for mission-critical questions that MUST be answered perfectly
# LLM_ULTIMATE_COMPLEXITY_MODE=0

# LLM_TIMEOUT_SECONDS: Maximum time to wait for AI response
# Default: 180 seconds (3 minutes)
# Extreme mode: 600 seconds (10 minutes)
# Ultimate mode: 1800 seconds (30 minutes)
# LLM_TIMEOUT_SECONDS=180

# LLM_MAX_RETRIES: Number of retry attempts
# Default: 2 attempts
# Extreme mode: 8 attempts
# Ultimate mode: 20 attempts (enough to handle ANY failure!)
# LLM_MAX_RETRIES=2

# LLM_RETRY_BACKOFF_BASE: Exponential backoff multiplier
# Default: 1.3
# Extreme mode: 1.5 for more patient retries
# LLM_RETRY_BACKOFF_BASE=1.3

# ADMIN_AI_MAX_QUESTION_LENGTH: Maximum question length in characters (default: 50000)
# Questions longer than this will be rejected with helpful guidance
# ADMIN_AI_MAX_QUESTION_LENGTH=50000

# ADMIN_AI_LONG_QUESTION_THRESHOLD: Threshold for "long" questions (default: 5000)
# Questions above this get extra processing time and larger response tokens
# ADMIN_AI_LONG_QUESTION_THRESHOLD=5000

# ADMIN_AI_EXTREME_QUESTION_THRESHOLD: When to consider a question as "extremely complex" (default: 20000)
# Questions above this threshold trigger maximum resource allocation
# ADMIN_AI_EXTREME_QUESTION_THRESHOLD=20000

# --------------------------------------------------------------------------------------
# [MICROSERVICES] SERVICE BOUNDARIES
# --------------------------------------------------------------------------------------
# ÿ•ÿπÿØÿßÿØÿßÿ™ ÿßŸÑÿÆÿØŸÖÿßÿ™ ÿßŸÑŸÖÿµÿ∫ÿ±ÿ© ÿßŸÑŸÖÿ≥ÿ™ŸÇŸÑÿ©. ŸÉŸÑ ÿÆÿØŸÖÿ© ŸÑŸáÿß ŸÇÿßÿπÿØÿ© ÿ®ŸäÿßŸÜÿßÿ™ ÿÆÿßÿµÿ© ŸàÿπŸÜÿßŸàŸäŸÜ ÿ™ŸàÿßÿµŸÑ Ÿàÿßÿ∂ÿ≠ÿ©.
ORCHESTRATOR_DATABASE_URL="sqlite+aiosqlite:///./orchestrator.db"
PLANNING_DATABASE_URL="sqlite+aiosqlite:///./planning_agent.db"
MEMORY_DATABASE_URL="sqlite+aiosqlite:///./memory_agent.db"
USER_DATABASE_URL="sqlite+aiosqlite:///./user_service.db"

# ÿ±Ÿàÿßÿ®ÿ∑ ÿßŸÑŸàŸÉŸÑÿßÿ° ÿßŸÑÿ™Ÿä Ÿäÿ≥ÿ™ÿÆÿØŸÖŸáÿß ÿßŸÑŸÄ Orchestrator ŸÑŸÑÿ™ŸÜÿ≥ŸäŸÇ ÿπÿ®ÿ± API
ORCHESTRATOR_PLANNING_AGENT_URL="http://planning-agent:8001"
ORCHESTRATOR_MEMORY_AGENT_URL="http://memory-agent:8002"
ORCHESTRATOR_USER_SERVICE_URL="http://user-service:8003"

# ADMIN_AI_MAX_RESPONSE_TOKENS: Maximum tokens for responses (default: 16000, 32000 in extreme mode)
# Allows comprehensive answers to complex questions
# In extreme mode, this doubles to 32,000 tokens for ultra-detailed responses
# ADMIN_AI_MAX_RESPONSE_TOKENS=16000

# ADMIN_AI_ENABLE_STREAMING: Enable streaming responses for better UX (default: 1)
# Set to 0 to disable streaming
# ADMIN_AI_ENABLE_STREAMING=1

# --------------------------------------------------------------------------------------
# [SUPERHUMAN] AI AGENT TOKEN - MCP SERVER INTEGRATION
# --------------------------------------------------------------------------------------
# üöÄ AI Agent Token for GitHub Copilot + Model Context Protocol (MCP) integration.
# This token enables superhuman AI capabilities surpassing Google, Microsoft, OpenAI!
#
# This token is used in THREE critical locations:
#   1. üîß GitHub Actions - CI/CD workflows with AI assistance
#   2. ‚òÅÔ∏è  GitHub Codespaces - Cloud development environments
#   3. ü§ñ Dependabot - Automated dependency updates with AI review
#
# How to get your token | ŸÉŸäŸÅŸäÿ© ÿßŸÑÿ≠ÿµŸàŸÑ ÿπŸÑŸâ ÿßŸÑÿ±ŸÖÿ≤:
#   1. Visit: https://github.com/settings/tokens
#   2. Click "Generate new token (classic)"
#   3. Select scopes | ÿßÿÆÿ™ÿ± ÿßŸÑÿµŸÑÿßÿ≠Ÿäÿßÿ™:
#      ‚úÖ repo (Full control of private repositories)
#      ‚úÖ read:org (Read org and team membership)
#      ‚úÖ workflow (Update GitHub Action workflows)
#      ‚úÖ admin:repo_hook (Full control of repository hooks)
#      ‚úÖ read:discussion (Read discussions)
#      ‚úÖ write:discussion (Write discussions)
#      ‚úÖ read:packages (Download packages from GitHub Package Registry)
#      ‚úÖ write:packages (Upload packages to GitHub Package Registry)
#   4. Copy the generated token (starts with ghp_ or github_pat_)
#   5. Paste it here
#
# Token format examples:
#   Classic: ghp_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
#   Fine-grained: github_pat_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
#
# üîê Security Notes | ŸÖŸÑÿßÿ≠ÿ∏ÿßÿ™ ÿ£ŸÖŸÜŸäÿ©:
#   ‚ö†Ô∏è  NEVER commit this token to Git
#   ‚ö†Ô∏è  ŸÑÿß ÿ™ŸÇŸÖ ÿ£ÿ®ÿØÿßŸã ÿ®ÿ•ÿ∂ÿßŸÅÿ© Ÿáÿ∞ÿß ÿßŸÑÿ±ŸÖÿ≤ ÿ•ŸÑŸâ Git
#   ‚úÖ  This file is in .gitignore (safe)
#   ‚úÖ  Ÿáÿ∞ÿß ÿßŸÑŸÖŸÑŸÅ ŸÅŸä .gitignore (ÿ¢ŸÖŸÜ)
#   ‚úÖ  Rotate tokens every 90 days for security
#   ‚úÖ  ŸÇŸÖ ÿ®ÿ™ÿØŸàŸäÿ± ÿßŸÑÿ±ŸÖŸàÿ≤ ŸÉŸÑ 90 ŸäŸàŸÖÿßŸã ŸÑŸÑÿ£ŸÖÿßŸÜ
#   ‚úÖ  Store in GitHub Secrets for Actions/Codespaces/Dependabot
#   ‚úÖ  ŸÇŸÖ ÿ®ÿ™ÿÆÿ≤ŸäŸÜŸá ŸÅŸä GitHub Secrets ŸÑŸÑÿ´ŸÑÿßÿ´ÿ© ÿ£ŸÖÿßŸÉŸÜ
#
# üéØ Where to add this secret:
#   - GitHub Actions: Settings > Secrets and variables > Actions > New repository secret
#   - Codespaces: Settings > Secrets > Codespaces > New secret
#   - Dependabot: Settings > Secrets and variables > Dependabot > New secret
#   Name: AI_AGENT_TOKEN
#
AI_AGENT_TOKEN="ghp_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"

# Legacy support (will be deprecated)
GITHUB_PERSONAL_ACCESS_TOKEN="${AI_AGENT_TOKEN}"

# --------------------------------------------------------------------------------------
# [CORE] AUTOMATIC SEEDING PROTOCOL
# --------------------------------------------------------------------------------------
# Used by the `flask users init-admin` command.
ADMIN_EMAIL="admin@example.com"
ADMIN_PASSWORD="strong-password-here"
ADMIN_NAME="Admin User"

# ======================================================================================
# ==                   OVERMIND / PLANNER HYPER-CONFIGURATION                       ==
# ======================================================================================
# This section provides fine-grained control over the Overmind planning and execution engine.
# Only change these if you are an expert.

# ---------- [1] Planner Intelligence & Behavior ----------
PLANNER_ENABLE_ROLE_DERIVATION=1       # Allow planner to assign roles to files.
PLANNER_ENABLE_SECTION_INFERENCE=1     # Allow planner to infer document sections.
PLANNER_ENABLE_CODE_HINTS=1            # Provide hints for code-related generation.
PLANNER_STRICT_WRITE_ENFORCE=1         # Ensure every target file in the objective is written to.
PLANNER_SMART_FILENAME=1               # Automatically clean and generate filenames.
PLANNER_ALLOW_SUBDIRS=1                # Allow planner to create files in subdirectories.

# ---------- [2] Chunking & Streaming Engine (Enhanced for Extreme Complexity) ----------
PLANNER_STREAMING_ENABLE=1             # Enable multi-task streaming for large content.
PLANNER_ALLOW_APPEND_TOOL=auto         # 'auto', '1' (force), or '0' (disable).
PLANNER_MAX_CHUNKS=100                 # Max generation tasks for a single file (increased from 50 for extreme complexity).
PLANNER_CHUNK_SIZE_HINT=1200           # Target character count per chunk.
PLANNER_FAST_SINGLE_THRESHOLD=1800     # Objectives smaller than this may use a single task.
PLANNER_HARD_LINE_CAP=2000000          # Hard limit on total lines (increased from 1.2M for extreme complexity)

# ---------- [3] Agent Tools Runtime Behavior ----------
AGENT_TOOLS_PROJECT_ROOT=/app          # The root directory inside the container.
AGENT_TOOLS_LOG_LEVEL=INFO
AGENT_TOOLS_MAX_WRITE_BYTES=5000000
AGENT_TOOLS_MAX_READ_BYTES=800000
AGENT_TOOLS_CREATE_MISSING=1           # Allow tools to create files that don't exist.
AGENT_TOOLS_CREATE_ALLOWED_EXTS=.md,.txt,.json,.log

# ---------- [4] System & Logging ----------
OVERMIND_LOG_DEBUG=1                   # Set to 0 in production.
LLM_PLANNER_LOG_LEVEL=DEBUG            # Set to INFO in production.
LOG_FORMAT_JSON=0                      # Set to 1 for structured logging.

# ---------- [5] Global Guardrails (Enhanced for Extreme Complexity) ----------
PLANNER_MAX_FILES=12                   # Max number of files a single mission can target.
PLANNER_MAX_TASKS_GLOBAL=800           # Hard limit on total tasks in a single plan (increased from 400 for extreme complexity).
DISABLED_TOOLS=delete_file             # Prevent the AI from using the delete_file tool for safety.

# --------------------------------------------------------------------------------------
# [OPTIONAL] DEVCONTAINER / CODESPACES BEHAVIOR CONTROL
# --------------------------------------------------------------------------------------
# These variables control the behavior of scripts in .devcontainer/
# Useful when setting up in GitHub Codespaces or local Dev Containers
#
# WAIT_FOR_DB=0                        # Skip waiting for database (for external Supabase)
# RUN_MIGRATIONS_ON_START=0            # Skip auto-migrations on container start
# RUN_APP_ON_START=0                   # Skip auto-starting the app
# SKIP_PIP_INSTALL=0                   # Skip pip install (when already in Docker image)

# --------------------------------------------------------------------------------------
# [ADVANCED] SUPERHUMAN STREAMING FEATURES
# --------------------------------------------------------------------------------------
# Advanced streaming features that surpass ChatGPT, Gemini, and Claude
#
# SSE Streaming Control
ALLOW_MOCK_LLM=false                   # Set to 'true' for development/testing (default: false for production)
ENABLE_HYBRID_STREAMING=false          # Enable hybrid streaming with prediction (experimental)
ENABLE_INTELLIGENT_ROUTING=false       # Enable intelligent model routing based on query complexity

# ‚ö†Ô∏è LEGACY: Model Tier Configuration (DEPRECATED - use AI_*_MODEL above instead)
# These are kept for backward compatibility only
# NANO_MODEL, FAST_MODEL, SMART_MODEL, GENIUS_MODEL are now: AI_NANO_MODEL, AI_FAST_MODEL, etc.

# Cost Management
LLM_DAILY_BUDGET=100                   # Daily budget in USD (default: 100)

# Performance Tuning
STREAMING_CHUNK_SIZE=8                 # Tokens per chunk (default: 8)
STREAMING_HEARTBEAT_INTERVAL=20        # Heartbeat interval in seconds (default: 20)

# ======================================================================================
# ==                         END OF CONFIGURATION                                     ==
# ======================================================================================
