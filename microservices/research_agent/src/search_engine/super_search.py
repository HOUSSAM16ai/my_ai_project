"""
Super Search Orchestrator - The Autonomous Research Agent.
---------------------------------------------------------
This module implements the cognitive research infrastructure, replacing simple keyword search
with a multi-step, autonomous research process.

Features:
1. Decomposition: Breaking queries into research plans.
2. Parallel Execution: Searching multiple angles simultaneously.
3. Deep Ingestion: Scraping and reading content.
4. Synthesis: Generating citation-backed reports.
"""

import asyncio
import os
from typing import Any

import httpx
from bs4 import BeautifulSoup
from langchain_community.utilities import DuckDuckGoSearchAPIWrapper
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from pydantic import BaseModel, Field

from microservices.research_agent.src.logging import get_logger

logger = get_logger("super-search-orchestrator")

# Optional Imports for "Superhuman" Tools
try:
    from tavily import TavilyClient
except ImportError:
    TavilyClient = None

try:
    from firecrawl import FirecrawlApp
except ImportError:
    FirecrawlApp = None


# --- Data Models ---

class ResearchPlan(BaseModel):
    """The research plan generated by the LLM."""
    sub_queries: list[str] = Field(description="3-5 precise search queries covering different angles.")
    required_info: list[str] = Field(description="Specific facts or data points to extract.")

class SearchResult(BaseModel):
    """Internal structure for a single piece of gathered evidence."""
    content: str
    source_url: str
    credibility_score: float = 0.5

# --- Components ---

class SimpleWebScraper:
    """A lightweight scraper using httpx and BeautifulSoup (Fallback)."""
    async def scrape(self, url: str) -> str:
        try:
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
            }
            async with httpx.AsyncClient(follow_redirects=True, timeout=10.0, headers=headers) as client:
                response = await client.get(url)
                response.raise_for_status()
                soup = BeautifulSoup(response.text, "html.parser")
                for script in soup(["script", "style", "nav", "footer", "header", "noscript", "iframe"]):
                    script.extract()
                text = soup.get_text(separator="\n")
                lines = (line.strip() for line in text.splitlines())
                chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                text = "\n".join(chunk for chunk in chunks if chunk)
                return text[:15000]
        except Exception as e:
            logger.warning(f"SimpleWebScraper failed for {url}: {e}")
            return ""

class SuperSearchOrchestrator:
    """
    The Cognitive Research Infrastructure.
    Orchestrates the full research lifecycle: Plan -> Search -> Read -> Synthesize.
    """

    def __init__(
        self,
        llm: Any | None = None,
        search_tool: Any | None = None,
        web_scraper: Any | None = None
    ):
        # 1. Initialize LLM
        if llm:
            self.llm = llm
        else:
            api_key = os.environ.get("OPENROUTER_API_KEY")
            base_url = "https://openrouter.ai/api/v1"
            model_name = os.environ.get("PRIMARY_MODEL", "openai/gpt-4o")
            self.llm = ChatOpenAI(
                model=model_name,
                openai_api_key=api_key,
                openai_api_base=base_url,
                temperature=0.0,
            )

        # 2. Initialize Search Tool (Prioritize Tavily)
        if search_tool:
            self.search_tool = search_tool
        else:
            tavily_key = os.environ.get("TAVILY_API_KEY")
            if TavilyClient and tavily_key:
                logger.info("Using Tavily Search.")
                self.search_tool = TavilyClient(api_key=tavily_key)
            else:
                logger.info("Using DuckDuckGo Search (Fallback).")
                self.search_tool = DuckDuckGoSearchAPIWrapper(max_results=5)

        # 3. Initialize Scraper (Prioritize Firecrawl)
        if web_scraper:
            self.scraper = web_scraper
        else:
            firecrawl_key = os.environ.get("FIRECRAWL_API_KEY")
            if FirecrawlApp and firecrawl_key:
                logger.info("Using Firecrawl Scraper.")
                self.scraper = FirecrawlApp(api_key=firecrawl_key)
            else:
                logger.info("Using SimpleWebScraper (Fallback).")
                self.scraper = SimpleWebScraper()

    async def execute(self, user_query: str) -> str:
        """Executes the full research mission."""
        logger.info(f"ðŸš€ Initiating Super-Search for: {user_query}")

        # --- Phase 1: Strategic Planning ---
        logger.info("Phase 1: Planning...")
        try:
            plan = await self._create_plan(user_query)
            logger.info(f"ðŸ“‹ Plan generated: {len(plan.sub_queries)} sub-queries.")
        except Exception as e:
            logger.error(f"Planning failed: {e}. Fallback to direct search.")
            plan = ResearchPlan(sub_queries=[user_query], required_info=[])

        # --- Phase 2: Parallel Execution ---
        logger.info("Phase 2: Searching...")
        try:
            raw_results = await self._parallel_search(plan.sub_queries)
        except Exception as e:
            logger.error(f"Search failed: {e}")
            raw_results = []

        # Deduplicate URLs
        unique_urls = list({r.get("url") or r.get("link") for r in raw_results if (r.get("url") or r.get("link"))})[:5]
        logger.info(f"Found {len(unique_urls)} unique sources.")

        # --- Phase 3: Deep Ingestion ---
        logger.info("Phase 3: Deep Ingestion...")
        full_knowledge_base = await self._deep_dive(unique_urls, user_query)

        if not full_knowledge_base:
            logger.warning("No deep content found. Using snippets.")
            full_knowledge_base = [
                SearchResult(
                    content=r.get("content") or r.get("snippet", ""),
                    source_url=r.get("url") or r.get("link", ""),
                    credibility_score=0.3
                )
                for r in raw_results
            ]

        if not full_knowledge_base:
             return "Unable to find sufficient information to answer the query."

        # --- Phase 4: Synthesis ---
        logger.info("Phase 4: Synthesis...")
        return await self._synthesize_report(user_query, full_knowledge_base)

    async def _create_plan(self, query: str) -> ResearchPlan:
        """Generates a research plan using the LLM."""
        structured_llm = self.llm.with_structured_output(ResearchPlan)
        prompt = ChatPromptTemplate.from_template("You are a PhD researcher. Plan a search strategy for: {query}")
        chain = prompt | structured_llm
        return await chain.ainvoke({"query": query})

    async def _parallel_search(self, queries: list[str]) -> list[dict]:
        """Executes searches in parallel."""
        async def run_search(q):
            try:
                # Handle Tavily
                if TavilyClient and isinstance(self.search_tool, TavilyClient):
                    # Tavily sync client, wrap in thread
                    # Using search_depth="advanced" for deep research if possible, or basic.
                    resp = await asyncio.to_thread(self.search_tool.search, query=q, search_depth="basic", max_results=3)
                    # Tavily returns dict with 'results' key which is a list
                    return resp.get("results", [])

                # Handle DuckDuckGo (langchain wrapper)
                if hasattr(self.search_tool, "results"):
                     return await asyncio.to_thread(self.search_tool.results, q, max_results=3)

                # Handle Generic/Mock (e.g. from user script)
                if hasattr(self.search_tool, "search"):
                     # Some search tools might return a list directly, or a dict.
                     # We assume list of dicts.
                     # If it's a sync function, wrap it.
                     if asyncio.iscoroutinefunction(self.search_tool.search):
                        res = await self.search_tool.search(q, max_results=3)
                     else:
                        res = await asyncio.to_thread(self.search_tool.search, q, max_results=3)

                     return res if isinstance(res, list) else []

                return []
            except Exception as e:
                logger.warning(f"Search failed for '{q}': {e}")
                return []

        tasks = [run_search(q) for q in queries]
        results_list = await asyncio.gather(*tasks)

        flat_results = []
        for res in results_list:
            if res:
                flat_results.extend(res)
        return flat_results

    async def _deep_dive(self, urls: list[str], query: str) -> list[SearchResult]:
        """Scrapes and summarizes content from URLs."""
        async def process_url(url):
            if not url:
                return None
            try:
                content = ""
                # Handle Firecrawl
                if FirecrawlApp and isinstance(self.scraper, FirecrawlApp):
                    # Firecrawl scrape_url returns dict with 'markdown' or 'content'
                    scrape_res = await asyncio.to_thread(self.scraper.scrape_url, url, params={'formats': ['markdown']})
                    content = scrape_res.get('markdown', "") or scrape_res.get('text', "")

                # Handle SimpleWebScraper
                elif hasattr(self.scraper, "scrape"):
                    if asyncio.iscoroutinefunction(self.scraper.scrape):
                        content = await self.scraper.scrape(url)
                    else:
                        content = await asyncio.to_thread(self.scraper.scrape, url)

                else:
                    content = ""

                if not content or len(content) < 100:
                    return None

                # Summarize
                summary_prompt = (
                    f"Extract detailed info relevant to '{query}' from the text below.\n"
                    f"Text: {content[:8000]}..."
                )
                summary_msg = await self.llm.ainvoke(summary_prompt)
                summary = summary_msg.content

                return SearchResult(content=str(summary), source_url=url, credibility_score=0.9)
            except Exception as e:
                logger.warning(f"Deep dive failed for {url}: {e}")
                return None

        tasks = [process_url(url) for url in urls]
        results = await asyncio.gather(*tasks)
        return [r for r in results if r is not None]

    async def _synthesize_report(self, query: str, knowledge: list[SearchResult]) -> str:
        context_str = "\n\n".join([f"### Source: {k.source_url}\n{k.content}" for k in knowledge])
        prompt = (
            f"You are an elite analyst. Answer the query based ONLY on the context below.\n"
            f"Query: {query}\n\n"
            "Rules:\n"
            "1. Answer in high-quality markdown.\n"
            "2. Cite sources using [1], [2] format inline.\n"
            "3. Create a 'References' section at the end with URLs matching the citations.\n"
            "4. If data is conflicting, mention the conflict.\n"
            f"Context:\n{context_str}"
        )
        response = await self.llm.ainvoke(prompt)
        return str(response.content)
