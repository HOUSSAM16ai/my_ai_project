"""
Super Search Orchestrator - The Autonomous Research Agent.
---------------------------------------------------------
This module implements the cognitive research infrastructure, replacing simple keyword search
with a multi-step, autonomous research process.

Features:
1. Decomposition: Breaking queries into research plans.
2. Parallel Execution: Searching multiple angles simultaneously.
3. Deep Ingestion: Scraping and reading content.
4. Synthesis: Generating citation-backed reports.
"""

import asyncio
import os
from typing import List

from pydantic import BaseModel, Field
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_community.utilities import DuckDuckGoSearchAPIWrapper
import httpx
from bs4 import BeautifulSoup

from microservices.research_agent.src.logging import get_logger

logger = get_logger("super-search-orchestrator")

# --- Data Models ---

class ResearchPlan(BaseModel):
    """The research plan generated by the LLM."""
    sub_queries: List[str] = Field(description="3-5 precise search queries covering different angles.")
    required_info: List[str] = Field(description="Specific facts or data points to extract.")

class ResearchResult(BaseModel):
    """Internal structure for a single piece of gathered evidence."""
    content: str
    source_url: str
    credibility_score: float = 0.5

# --- Components ---

class SimpleWebScraper:
    """A lightweight scraper using httpx and BeautifulSoup."""

    async def scrape(self, url: str) -> str:
        """Fetches and extracts text from a URL."""
        try:
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
            }
            async with httpx.AsyncClient(follow_redirects=True, timeout=10.0, headers=headers) as client:
                response = await client.get(url)
                response.raise_for_status()

                soup = BeautifulSoup(response.text, 'html.parser')

                # Remove script and style elements
                for script in soup(["script", "style", "nav", "footer", "header", "noscript", "iframe"]):
                    script.extract()

                # Get text
                text = soup.get_text(separator='\n')

                # Clean up whitespace
                lines = (line.strip() for line in text.splitlines())
                chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                text = '\n'.join(chunk for chunk in chunks if chunk)

                return text[:15000] # Limit content length
        except Exception as e:
            logger.warning(f"Failed to scrape {url}: {e}")
            return ""

class SuperSearchOrchestrator:
    """
    The Cognitive Research Infrastructure.
    Orchestrates the full research lifecycle: Plan -> Search -> Read -> Synthesize.
    """

    def __init__(self, llm: ChatOpenAI | None = None):
        # Initialize LLM
        if llm:
            self.llm = llm
        else:
            api_key = os.environ.get("OPENROUTER_API_KEY")
            base_url = "https://openrouter.ai/api/v1"
            # Default to a strong model if not provided
            model_name = os.environ.get("PRIMARY_MODEL", "openai/gpt-4o")

            self.llm = ChatOpenAI(
                model=model_name,
                openai_api_key=api_key,
                openai_api_base=base_url,
                temperature=0.0
            )

        # Initialize Search Tool (DuckDuckGo as fallback/primary)
        # We use the wrapper to get results with metadata (snippets/links)
        self.search_tool = DuckDuckGoSearchAPIWrapper(max_results=3)

        # Initialize Scraper
        self.scraper = SimpleWebScraper()

    async def execute(self, user_query: str) -> str:
        """
        Executes the full research mission.
        """
        logger.info(f"ðŸš€ Initiating Super-Search for: {user_query}")

        # --- Phase 1: Strategic Planning ---
        logger.info("Phase 1: Planning...")
        try:
            plan = await self._create_plan(user_query)
            logger.info(f"ðŸ“‹ Plan generated: {len(plan.sub_queries)} sub-queries.")
        except Exception as e:
            logger.error(f"Planning failed: {e}. Fallback to direct search.")
            plan = ResearchPlan(sub_queries=[user_query], required_info=[])

        # --- Phase 2: Parallel Execution ---
        logger.info("Phase 2: Searching...")
        try:
            raw_results = await self._parallel_search(plan.sub_queries)
        except Exception as e:
             logger.error(f"Search failed: {e}")
             raw_results = []

        # Deduplicate URLs
        unique_urls = list(set([r['link'] for r in raw_results if 'link' in r]))[:5] # Limit to top 5
        logger.info(f"Found {len(unique_urls)} unique sources.")

        # --- Phase 3: Deep Ingestion ---
        logger.info("Phase 3: Deep Ingestion...")
        full_knowledge_base = await self._deep_dive(unique_urls, user_query)

        if not full_knowledge_base:
             logger.warning("No deep content found. Using snippets.")
             # Fallback to snippets if scraping failed
             full_knowledge_base = [
                 ResearchResult(content=r.get('snippet', ''), source_url=r.get('link', ''), credibility_score=0.3)
                 for r in raw_results
             ]

        if not full_knowledge_base:
            return "Unable to find sufficient information to answer the query."

        # --- Phase 4: Synthesis ---
        logger.info("Phase 4: Synthesis...")
        final_answer = await self._synthesize_report(user_query, full_knowledge_base)

        return final_answer

    async def _create_plan(self, query: str) -> ResearchPlan:
        """Generates a research plan using the LLM."""
        structured_llm = self.llm.with_structured_output(ResearchPlan)
        prompt = ChatPromptTemplate.from_template(
            "You are a PhD researcher. Plan a search strategy for: {query}"
        )
        chain = prompt | structured_llm
        return await chain.ainvoke({"query": query})

    async def _parallel_search(self, queries: List[str]) -> List[dict]:
        """Executes searches in parallel."""
        # DuckDuckGo run is synchronous in some versions, but we can wrap in thread
        # wrapper.results returns list of dicts {snippet, title, link}

        async def run_search(q):
            try:
                # wrapper.results is sync
                return await asyncio.to_thread(self.search_tool.results, q, max_results=3)
            except Exception as e:
                logger.warning(f"Search failed for '{q}': {e}")
                return []

        tasks = [run_search(q) for q in queries]
        results_list = await asyncio.gather(*tasks)

        # Flatten results
        flat_results = []
        for res in results_list:
            if res:
                flat_results.extend(res)
        return flat_results

    async def _deep_dive(self, urls: List[str], query: str) -> List[ResearchResult]:
        """Scrapes and summarizes content from URLs."""

        async def process_url(url):
            content = await self.scraper.scrape(url)
            if not content or len(content) < 100:
                return None

            # Summarize/Extract relevance
            try:
                summary_prompt = (
                    f"Extract detailed info relevant to '{query}' from the text below.\n"
                    f"Text: {content[:8000]}..." # Limit context
                )
                # Use standard invoke (not structured) for summary
                summary_msg = await self.llm.ainvoke(summary_prompt)
                summary = summary_msg.content

                return ResearchResult(content=str(summary), source_url=url, credibility_score=0.8)
            except Exception as e:
                logger.warning(f"Summary generation failed for {url}: {e}")
                # Fallback to raw text truncated
                return ResearchResult(content=content[:2000], source_url=url, credibility_score=0.5)

        tasks = [process_url(url) for url in urls]
        results = await asyncio.gather(*tasks)
        return [r for r in results if r is not None]

    async def _synthesize_report(self, query: str, knowledge: List[ResearchResult]) -> str:
        """Synthesizes the final report."""
        context_str = "\n\n".join([f"### Source: {k.source_url}\n{k.content}" for k in knowledge])

        prompt = (
            f"You are an elite analyst. Answer the query based ONLY on the context below.\n"
            f"Query: {query}\n\n"
            "Rules:\n"
            "1. Answer in high-quality markdown.\n"
            "2. Cite sources using [1], [2] format inline.\n"
            "3. Create a 'References' section at the end with URLs matching the citations.\n"
            "4. If data is conflicting, mention the conflict.\n"
            "5. Be comprehensive and detailed.\n\n"
            f"Context:\n{context_str}"
        )

        response = await self.llm.ainvoke(prompt)
        return str(response.content)
