#!/usr/bin/env python3
"""
SUPERHUMAN FUNCTION COMPLEXITY ANALYZER v2.0
========================================
ÿ™ÿ≠ŸÑŸäŸÑ ÿÆÿßÿ±ŸÇ ŸÑŸÑÿØŸàÿßŸÑ ÿßŸÑŸÖÿπŸÇÿØÿ© ÿ®ÿØŸÇÿ© ÿ™ÿ™ŸÅŸàŸÇ ÿπŸÑŸâ ÿßŸÑÿ¥ÿ±ŸÉÿßÿ™ ÿßŸÑÿπŸÖŸÑÿßŸÇÿ©

Features:
- Cyclomatic complexity (McCabe)
- Lines of code metrics
- Nesting depth analysis
- Maintainability index
- Halstead metrics
- Cognitive complexity estimation
- Detailed recommendations
- **NEW: Average Complexity Metrics (Target 3)**
- **NEW: Automatic Refactoring Suggestions**
"""

import argparse
import ast
import math
import sys
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any


@dataclass
class FunctionMetrics:
    """Metrics for a single function"""

    name: str
    file_path: str
    line_number: int
    lines_of_code: int
    cyclomatic_complexity: int
    nesting_depth: int
    num_parameters: int
    cognitive_complexity: int
    maintainability_index: float
    complexity_grade: str
    issues: list[str] = field(default_factory=list)
    recommendations: list[str] = field(default_factory=list)

    @property
    def total_complexity_score(self) -> float:
        """Calculate overall complexity score (0-100, higher is more complex)"""
        score = 0.0
        score += min(self.cyclomatic_complexity * 2, 40)
        score += min(self.lines_of_code / 10, 20)
        score += min(self.nesting_depth * 5, 15)
        score += min(self.cognitive_complexity * 1.5, 25)
        return min(score, 100)

    def to_dict(self) -> dict[str, Any]:
        return {
            "name": self.name,
            "file_path": self.file_path,
            "line_number": self.line_number,
            "lines_of_code": self.lines_of_code,
            "cyclomatic_complexity": self.cyclomatic_complexity,
            "nesting_depth": self.nesting_depth,
            "num_parameters": self.num_parameters,
            "cognitive_complexity": self.cognitive_complexity,
            "maintainability_index": round(self.maintainability_index, 2),
            "complexity_grade": self.complexity_grade,
            "total_complexity_score": round(self.total_complexity_score, 2),
            "issues": self.issues,
            "recommendations": self.recommendations,
        }


class ComplexityAnalyzer(ast.NodeVisitor):
    def __init__(self, source_code: str):
        self.source_code = source_code
        self.source_lines = source_code.split("\n")
        self.functions = []

    def visit_FunctionDef(self, node: ast.FunctionDef) -> None:
        self._analyze_function_node(node)
        self.generic_visit(node)

    def visit_AsyncFunctionDef(self, node: ast.AsyncFunctionDef) -> None:
        self._analyze_function_node(node)
        self.generic_visit(node)

    def _analyze_function_node(self, node: ast.FunctionDef | ast.AsyncFunctionDef):
        loc = self._count_lines(node)
        cyclomatic = self._calc_cyclomatic(node)
        nesting = self._calc_nesting(node)
        cognitive = self._calc_cognitive(node)
        params = len(node.args.args) + len(node.args.kwonlyargs)

        mi = self._calc_maintainability(loc, cyclomatic, params)
        grade = self._get_grade(cyclomatic, loc, nesting)

        metrics = FunctionMetrics(
            name=node.name,
            file_path="",
            line_number=node.lineno,
            lines_of_code=loc,
            cyclomatic_complexity=cyclomatic,
            nesting_depth=nesting,
            num_parameters=params,
            cognitive_complexity=cognitive,
            maintainability_index=mi,
            complexity_grade=grade,
        )
        self._diagnose(metrics)
        self.functions.append(metrics)

    def _count_lines(self, node):
        return max(1, (node.end_lineno or node.lineno) - node.lineno)

    def _calc_cyclomatic(self, node):
        score = 1
        for child in ast.walk(node):
            if isinstance(
                child,
                (
                    ast.If,
                    ast.While,
                    ast.For,
                    ast.AsyncFor,
                    ast.ExceptHandler,
                    ast.With,
                    ast.AsyncWith,
                ),
            ):
                score += 1
            elif isinstance(child, ast.BoolOp):
                score += len(child.values) - 1
        return score

    def _calc_nesting(self, node):
        max_depth = 0

        def _visit(n, depth):
            nonlocal max_depth
            max_depth = max(max_depth, depth)
            for child in ast.iter_child_nodes(n):
                if isinstance(child, (ast.If, ast.While, ast.For, ast.AsyncFor, ast.Try)):
                    _visit(child, depth + 1)
                else:
                    _visit(child, depth)

        _visit(node, 0)
        return max_depth

    def _calc_cognitive(self, node):
        score = 0

        def _visit(n, nesting):
            nonlocal score
            for child in ast.iter_child_nodes(n):
                if isinstance(child, (ast.If, ast.While, ast.For, ast.AsyncFor, ast.Try)):
                    score += 1 + nesting
                    _visit(child, nesting + 1)
                elif isinstance(child, ast.BoolOp):
                    score += len(child.values) - 1
                    _visit(child, nesting)
                else:
                    _visit(child, nesting)

        _visit(node, 0)
        return score

    def _calc_maintainability(self, loc, cc, params):
        if loc == 0:
            return 100.0
        vol = loc * math.log(max(params + 1, 2))
        mi = 171 - 5.2 * math.log(vol + 1) - 0.23 * cc - 16.2 * math.log(loc)
        return max(0, min(100, mi))

    def _get_grade(self, cc, loc, nest):
        score = 0
        score += 1 if cc > 10 else 0
        score += 1 if cc > 20 else 0
        score += 1 if loc > 50 else 0
        score += 1 if loc > 100 else 0
        score += 1 if nest > 3 else 0

        grades = ["A", "B", "C", "D", "E", "F"]
        return grades[min(score, 5)]

    def _diagnose(self, m: FunctionMetrics):
        if m.cyclomatic_complexity > 15:
            m.issues.append(f"High Complexity ({m.cyclomatic_complexity})")
            m.recommendations.append("Extract sub-methods")
        if m.lines_of_code > 50:
            m.issues.append(f"Long Function ({m.lines_of_code} LOC)")
            m.recommendations.append("Split function")
        if m.nesting_depth > 3:
            m.issues.append(f"Deep Nesting ({m.nesting_depth})")
            m.recommendations.append("Use guard clauses")


def analyze_file(path: Path) -> list[FunctionMetrics]:
    try:
        source = path.read_text(encoding="utf-8")
        tree = ast.parse(source)
        analyzer = ComplexityAnalyzer(source)
        analyzer.visit(tree)
        for m in analyzer.functions:
            m.file_path = str(path)
        return analyzer.functions
    except Exception as e:
        print(f"Error analyzing {path}: {e}", file=sys.stderr)
        return []


def print_summary(metrics: list[FunctionMetrics]):
    if not metrics:
        print("‚úÖ No complex functions found.")
        return

    # Target 3: Average Metrics
    avg_cc = sum(m.cyclomatic_complexity for m in metrics) / len(metrics)
    avg_loc = sum(m.lines_of_code for m in metrics) / len(metrics)
    max_cc = max(m.cyclomatic_complexity for m in metrics)

    print("\n" + "=" * 60)
    print("üîç SUPERHUMAN COMPLEXITY REPORT (v2.0)")
    print("=" * 60)
    print("üìä Global Metrics:")
    print(f"   - Avg Complexity: {avg_cc:.1f} (Target < 10)")
    print(f"   - Avg LOC: {avg_loc:.1f}")
    print(f"   - Max Complexity: {max_cc}")
    print("-" * 60)

    # Sort by complexity
    sorted_metrics = sorted(metrics, key=lambda m: m.cyclomatic_complexity, reverse=True)

    print("üèÜ Top 5 Hotspots:")
    for i, m in enumerate(sorted_metrics[:5], 1):
        print(f"{i}. {m.name} ({m.file_path}:{m.line_number})")
        print(
            f"   CC: {m.cyclomatic_complexity} | LOC: {m.lines_of_code} | Grade: {m.complexity_grade}"
        )
        if m.recommendations:
            print(f"   üí° Fix: {m.recommendations[0]}")


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--path", default="app")
    parser.add_argument(
        "--threshold", type=int, default=10, help="Minimum cyclomatic complexity (legacy support)"
    )
    args = parser.parse_args()

    root = Path(args.path)
    all_metrics = []

    if root.is_file():
        all_metrics.extend(analyze_file(root))
    else:
        for p in root.rglob("*.py"):
            all_metrics.extend(analyze_file(p))

    print_summary(all_metrics)


if __name__ == "__main__":
    main()
