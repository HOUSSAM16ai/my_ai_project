#!/usr/bin/env python3
"""
Ù…Ø­Ù„Ù„ ØªÙØµÙŠÙ„ÙŠ Ù„Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø© Ø§Ù„Ù…Ø´ØªØ¨Ù‡ Ø¨Ù‡Ø§
"""

import ast
import json
from pathlib import Path


def analyze_specific_file(filepath: str) -> dict:
    """ØªØ­Ù„ÙŠÙ„ ØªÙØµÙŠÙ„ÙŠ Ù„Ù…Ù„Ù Ù…Ø­Ø¯Ø¯"""
    with open(filepath, 'r', encoding='utf-8') as f:
        content = f.read()
        
    tree = ast.parse(content, filename=filepath)
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªØ¹Ø±ÙŠÙØ§Øª
    functions = []
    classes = []
    imports = []
    
    for node in ast.walk(tree):
        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
            # Ø­Ø³Ø§Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø³Ø·Ø±
            if hasattr(node, 'end_lineno'):
                lines = node.end_lineno - node.lineno + 1
            else:
                lines = 0
                
            functions.append({
                'name': node.name,
                'lineno': node.lineno,
                'end_lineno': getattr(node, 'end_lineno', None),
                'lines': lines,
                'is_async': isinstance(node, ast.AsyncFunctionDef),
                'decorators': [get_decorator_name(d) for d in node.decorator_list],
                'args': [arg.arg for arg in node.args.args],
                'docstring': ast.get_docstring(node)
            })
            
        elif isinstance(node, ast.ClassDef):
            if hasattr(node, 'end_lineno'):
                lines = node.end_lineno - node.lineno + 1
            else:
                lines = 0
                
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙŠØ«ÙˆØ¯Ø§Øª
            methods = []
            for item in node.body:
                if isinstance(item, (ast.FunctionDef, ast.AsyncFunctionDef)):
                    methods.append(item.name)
                    
            classes.append({
                'name': node.name,
                'lineno': node.lineno,
                'end_lineno': getattr(node, 'end_lineno', None),
                'lines': lines,
                'bases': [get_name(base) for base in node.bases],
                'methods': methods,
                'docstring': ast.get_docstring(node)
            })
            
        elif isinstance(node, (ast.Import, ast.ImportFrom)):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.append({
                        'type': 'import',
                        'module': alias.name,
                        'alias': alias.asname
                    })
            else:
                if node.module:
                    for alias in node.names:
                        imports.append({
                            'type': 'from',
                            'module': node.module,
                            'name': alias.name,
                            'alias': alias.asname
                        })
                        
    return {
        'filepath': filepath,
        'total_lines': len(content.splitlines()),
        'functions': functions,
        'classes': classes,
        'imports': imports,
        'stats': {
            'total_functions': len(functions),
            'total_classes': len(classes),
            'total_imports': len(imports),
            'avg_function_lines': sum(f['lines'] for f in functions) / len(functions) if functions else 0,
            'avg_class_lines': sum(c['lines'] for c in classes) / len(classes) if classes else 0
        }
    }


def get_name(node):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø§Ø³Ù… Ù…Ù† Ø¹Ù‚Ø¯Ø© AST"""
    if isinstance(node, ast.Name):
        return node.id
    elif isinstance(node, ast.Attribute):
        value = get_name(node.value)
        return f"{value}.{node.attr}" if value else node.attr
    return ""


def get_decorator_name(node):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø³Ù… decorator"""
    if isinstance(node, ast.Name):
        return node.id
    elif isinstance(node, ast.Call):
        return get_name(node.func)
    elif isinstance(node, ast.Attribute):
        return get_name(node)
    return ""


def check_usage_in_codebase(entity_name: str, search_dirs: list[str]) -> dict:
    """Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… entity ÙÙŠ Ø§Ù„ÙƒÙˆØ¯"""
    import subprocess
    
    # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª
    try:
        result = subprocess.run(
            ['grep', '-r', entity_name, '--include=*.py'] + search_dirs,
            capture_output=True,
            text=True,
            timeout=30
        )
        
        lines = result.stdout.strip().split('\n') if result.stdout else []
        # ØªØµÙÙŠØ© Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        filtered_lines = [
            line for line in lines 
            if line and not line.strip().startswith('#')
        ]
        
        return {
            'found': len(filtered_lines) > 0,
            'occurrences': len(filtered_lines),
            'files': list(set(line.split(':')[0] for line in filtered_lines if ':' in line))
        }
    except Exception as e:
        return {
            'found': False,
            'occurrences': 0,
            'files': [],
            'error': str(e)
        }


def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    print("ğŸ” ØªØ­Ù„ÙŠÙ„ ØªÙØµÙŠÙ„ÙŠ Ù„Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø© Ø§Ù„Ù…Ø´ØªØ¨Ù‡ Ø¨Ù‡Ø§")
    print("=" * 80)
    
    # Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø³ØªÙ‡Ø¯ÙØ©
    target_files = [
        'app/boundaries/service_boundaries.py',
        'app/telemetry/unified_observability.py',
        'app/core/gateway/mesh.py',
        'app/core/error_handling.py'
    ]
    
    search_dirs = ['app/', 'tests/']
    
    all_results = {}
    
    for filepath in target_files:
        print(f"\n{'=' * 80}")
        print(f"ğŸ“„ ØªØ­Ù„ÙŠÙ„: {filepath}")
        print('=' * 80)
        
        if not Path(filepath).exists():
            print(f"   âŒ Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
            continue
            
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù„Ù
        analysis = analyze_specific_file(filepath)
        
        print(f"\nğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª:")
        print(f"   Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£Ø³Ø·Ø±: {analysis['total_lines']}")
        print(f"   Ø¹Ø¯Ø¯ Ø§Ù„Ø¯ÙˆØ§Ù„: {analysis['stats']['total_functions']}")
        print(f"   Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ø§Ø³Ø§Øª: {analysis['stats']['total_classes']}")
        print(f"   Ø¹Ø¯Ø¯ Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯Ø§Øª: {analysis['stats']['total_imports']}")
        print(f"   Ù…ØªÙˆØ³Ø· Ø£Ø³Ø·Ø± Ø§Ù„Ø¯Ø§Ù„Ø©: {analysis['stats']['avg_function_lines']:.1f}")
        print(f"   Ù…ØªÙˆØ³Ø· Ø£Ø³Ø·Ø± Ø§Ù„ÙƒÙ„Ø§Ø³: {analysis['stats']['avg_class_lines']:.1f}")
        
        # ÙØ­Øµ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙƒÙ„Ø§Ø³Ø§Øª
        print(f"\nğŸ—ï¸  ÙØ­Øµ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙƒÙ„Ø§Ø³Ø§Øª:")
        unused_classes = []
        for cls in analysis['classes']:
            usage = check_usage_in_codebase(cls['name'], search_dirs)
            # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙÙ‚Ø· ÙÙŠ Ù†ÙØ³ Ø§Ù„Ù…Ù„Ù
            if usage['occurrences'] <= 2:  # Ø§Ù„ØªØ¹Ø±ÙŠÙ + Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ§Ø­Ø¯ Ù…Ø­ØªÙ…Ù„
                unused_classes.append(cls)
                print(f"   âš ï¸  {cls['name']} (Ø§Ù„Ø³Ø·Ø± {cls['lineno']}, {cls['lines']} Ø³Ø·Ø±)")
                print(f"      Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Øª: {usage['occurrences']}")
                
        # ÙØ­Øµ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¯ÙˆØ§Ù„
        print(f"\nğŸ”§ ÙØ­Øµ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¯ÙˆØ§Ù„:")
        unused_functions = []
        for func in analysis['functions'][:20]:  # Ø£ÙˆÙ„ 20 Ø¯Ø§Ù„Ø©
            # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ø®Ø§ØµØ© ÙˆØ§Ù„Ø³Ø­Ø±ÙŠØ©
            if func['name'].startswith('__') and func['name'].endswith('__'):
                continue
            if func['name'].startswith('_'):
                continue
                
            usage = check_usage_in_codebase(func['name'], search_dirs)
            if usage['occurrences'] <= 2:
                unused_functions.append(func)
                print(f"   âš ï¸  {func['name']} (Ø§Ù„Ø³Ø·Ø± {func['lineno']}, {func['lines']} Ø³Ø·Ø±)")
                print(f"      Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Øª: {usage['occurrences']}")
                
        # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        all_results[filepath] = {
            'analysis': analysis,
            'unused_classes': unused_classes,
            'unused_functions': unused_functions
        }
        
    # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ÙƒØ§Ù…Ù„Ø©
    with open('specific_files_analysis.json', 'w', encoding='utf-8') as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False)
        
    print("\n" + "=" * 80)
    print("âœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙØµÙŠÙ„ÙŠ!")
    print("ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ: specific_files_analysis.json")
    
    # Ù…Ù„Ø®Øµ Ù†Ù‡Ø§Ø¦ÙŠ
    print("\nğŸ“‹ Ù…Ù„Ø®Øµ Ù†Ù‡Ø§Ø¦ÙŠ:")
    for filepath, results in all_results.items():
        print(f"\n   {filepath}:")
        print(f"      ÙƒÙ„Ø§Ø³Ø§Øª ØºÙŠØ± Ù…Ø³ØªØ®Ø¯Ù…Ø©: {len(results['unused_classes'])}")
        print(f"      Ø¯ÙˆØ§Ù„ ØºÙŠØ± Ù…Ø³ØªØ®Ø¯Ù…Ø©: {len(results['unused_functions'])}")
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø£Ø³Ø·Ø± Ø§Ù„Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„Ø­Ø°Ù
        deletable_lines = (
            sum(c['lines'] for c in results['unused_classes']) +
            sum(f['lines'] for f in results['unused_functions'])
        )
        total_lines = results['analysis']['total_lines']
        percentage = (deletable_lines / total_lines * 100) if total_lines > 0 else 0
        
        print(f"      Ø£Ø³Ø·Ø± Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„Ø­Ø°Ù: {deletable_lines} ({percentage:.1f}%)")


if __name__ == '__main__':
    main()
