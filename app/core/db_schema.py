"""Ù…Ø¯Ù‚Ù‚ Ù…Ø®Ø·Ø· Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.

ÙŠØªÙˆÙ„Ù‰ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…Ø®Ø·Ø· ÙˆØ¥ØµÙ„Ø§Ø­Ù‡ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¹Ù†Ø¯ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ´ØºÙŠÙ„ Ù…Ø¹ Ø§Ø­ØªØ±Ø§Ù…
Ù…Ø¨Ø¯Ø£ Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„ÙŠØ© Ø§Ù„ÙˆØ§Ø­Ø¯Ø© ÙˆØ¯Ø¹Ù… ØªØ¹Ø¯Ø¯ Ù…Ø­Ø±ÙƒØ§Øª Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.
"""

import logging
import re

from sqlalchemy import text
from sqlalchemy.ext.asyncio import AsyncConnection

from app.core.agents.system_principles import format_architecture_system_principles
from app.core.database import engine
from app.core.db_schema_config import _ALLOWED_TABLES, REQUIRED_SCHEMA, SchemaValidationResult

logger = logging.getLogger(__name__)

_SQLITE_SKIP_INDEX_PATTERNS: tuple[re.Pattern[str], ...] = (
    re.compile(r"\bUSING\s+hnsw\b", flags=re.IGNORECASE),
    re.compile(r"\bUSING\s+GIN\b", flags=re.IGNORECASE),
)

__all__ = ["validate_schema_on_startup"]


def _format_table_column(table_name: str, column_name: str) -> str:
    """ÙŠØ¹ÙŠØ¯ ØªÙ…Ø«ÙŠÙ„Ø§Ù‹ Ù…ÙˆØ­Ø¯Ø§Ù‹ Ù„Ø§Ø³Ù… Ø§Ù„Ø¬Ø¯ÙˆÙ„ ÙˆØ§Ù„Ø¹Ù…ÙˆØ¯ Ù„Ø³Ù‡ÙˆÙ„Ø© Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©."""
    return f"{table_name}.{column_name}"


def _log_schema_action(message: str, table_name: str, column_name: str) -> None:
    """ÙŠØ³Ø¬Ù„ Ø­Ø¯Ø«Ø§Ù‹ Ù…ØªØ¹Ù„Ù‚Ø§Ù‹ Ø¨Ø§Ù„Ù…Ø®Ø·Ø· Ù…Ø¹ ØªÙ†Ø³ÙŠÙ‚ Ù…ÙˆØ­Ø¯ Ù„Ù„Ø£Ø³Ù…Ø§Ø¡."""
    logger.info("%s %s", message, _format_table_column(table_name, column_name))


def _format_table_index(table_name: str, index_name: str) -> str:
    """ÙŠØ¹ÙŠØ¯ ØªÙ…Ø«ÙŠÙ„Ø§Ù‹ Ù…ÙˆØ­Ø¯Ø§Ù‹ Ù„Ø§Ø³Ù… Ø§Ù„Ø¬Ø¯ÙˆÙ„ ÙˆØ§Ù„ÙÙ‡Ø±Ø³ Ù„Ø³Ù‡ÙˆÙ„Ø© Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©."""
    return f"{table_name}.{index_name}"


def _format_index_from_query(table_name: str, index_query: str) -> str | None:
    """ÙŠØ¹ÙŠØ¯ Ø§Ø³Ù… Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ù†Ø³Ù‚ Ø¥Ù† Ø£Ù…ÙƒÙ† Ø§Ø³ØªØ®Ù„Ø§ØµÙ‡ Ù…Ù† Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ø¥Ù†Ø´Ø§Ø¡."""
    index_name = _infer_index_name(index_query)
    if not index_name:
        return None
    return _format_table_index(table_name, index_name)


def _to_sqlite_ddl(sql: str) -> str:
    """ÙŠØ­ÙˆÙ‘Ù„ Ø£ÙˆØ§Ù…Ø± PostgreSQL Ø¥Ù„Ù‰ ØµÙŠØºØ© Ù…ØªÙˆØ§ÙÙ‚Ø© Ù…Ø¹ SQLite."""
    sql = re.sub(
        r"SERIAL PRIMARY KEY", "INTEGER PRIMARY KEY AUTOINCREMENT", sql, flags=re.IGNORECASE
    )
    sql = re.sub(r"TIMESTAMPTZ", "TIMESTAMP", sql, flags=re.IGNORECASE)
    sql = re.sub(r"\bJSON\b", "TEXT", sql, flags=re.IGNORECASE)
    sql = re.sub(r"\bJSONB\b", "TEXT", sql, flags=re.IGNORECASE)
    sql = re.sub(r"vector\(\d+\)", "TEXT", sql, flags=re.IGNORECASE)
    sql = re.sub(r"\bUUID\b", "TEXT", sql, flags=re.IGNORECASE)

    if _should_skip_sqlite_index(sql):
        return ""

    sql = re.sub(r"tsvector GENERATED ALWAYS AS .*? STORED", "TEXT", sql, flags=re.IGNORECASE)
    sql = re.sub(r"\btsvector\b", "TEXT", sql, flags=re.IGNORECASE)
    sql = re.sub(r"DEFAULT TRUE", "DEFAULT 1", sql, flags=re.IGNORECASE)
    sql = re.sub(r"DEFAULT FALSE", "DEFAULT 0", sql, flags=re.IGNORECASE)
    sql = re.sub(r"NOW\(\)", "CURRENT_TIMESTAMP", sql, flags=re.IGNORECASE)
    return re.sub(r"ADD COLUMN IF NOT EXISTS", "ADD COLUMN", sql, flags=re.IGNORECASE)


def _apply_dialect_ddl(conn: AsyncConnection, sql: str) -> str:
    """ÙŠÙˆØ§Ø¦Ù… Ø£ÙˆØ§Ù…Ø± DDL Ø­Ø³Ø¨ Ù…Ø­Ø±Ùƒ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Ø´Ø·."""
    return _to_sqlite_ddl(sql) if conn.dialect.name == "sqlite" else sql


def _should_skip_sqlite_index(sql: str) -> bool:
    """ÙŠØªØ­Ù‚Ù‚ Ù…Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø£Ù…Ø± ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙÙ‡Ø±Ø³ ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ… ÙÙŠ SQLite."""
    return any(pattern.search(sql) for pattern in _SQLITE_SKIP_INDEX_PATTERNS)


def _assert_schema_whitelist_alignment() -> None:
    """ÙŠØªØ£ÙƒØ¯ Ù…Ù† ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ù…Ø®Ø·Ø· Ù…Ø¹ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„Ù…Ø³Ù…ÙˆØ­ Ø¨Ù‡Ø§."""
    defined_tables = set(REQUIRED_SCHEMA.keys())
    undefined_tables = defined_tables - _ALLOWED_TABLES
    missing_definitions = _ALLOWED_TABLES - defined_tables

    if undefined_tables:
        raise ValueError(f"Unauthorized table in schema: {', '.join(sorted(undefined_tables))}")

    if missing_definitions:
        raise ValueError(
            f"Missing schema definition for allowed tables: {', '.join(sorted(missing_definitions))}"
        )


_assert_schema_whitelist_alignment()


async def _get_existing_columns(conn: AsyncConnection, table_name: str) -> set[str]:
    """ÙŠØ¬Ù„Ø¨ Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø­Ø§Ù„ÙŠØ© Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª."""
    dialect_name = conn.dialect.name

    if dialect_name == "sqlite":
        result = await conn.execute(
            text("SELECT * FROM pragma_table_info(:table_name)"),
            {"table_name": table_name},
        )
        return {row[1] for row in result.fetchall()}

    result = await conn.execute(
        text("SELECT column_name FROM information_schema.columns WHERE table_name = :table_name"),
        {"table_name": table_name},
    )
    return {row[0] for row in result.fetchall()}


async def _table_exists(conn: AsyncConnection, table_name: str) -> bool:
    """ÙŠØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø¬Ø¯ÙˆÙ„ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª."""
    dialect_name = conn.dialect.name

    if dialect_name == "sqlite":
        result = await conn.execute(
            text("SELECT name FROM sqlite_master WHERE type='table' AND name=:table_name"),
            {"table_name": table_name},
        )
        return result.fetchone() is not None

    result = await conn.execute(
        text(
            "SELECT EXISTS ("
            "SELECT FROM information_schema.tables "
            "WHERE table_schema = 'public' AND table_name = :table_name"
            ")"
        ),
        {"table_name": table_name},
    )
    return bool(result.scalar())


async def _fix_missing_column(
    conn: AsyncConnection,
    table_name: str,
    col: str,
    auto_fix_queries: dict[str, str],
    index_queries: dict[str, str],
) -> bool:
    """ÙŠØ­Ø§ÙˆÙ„ Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ù†Ø§Ù‚Øµ ÙˆØ¥Ø¶Ø§ÙØ© Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø±ØªØ¨Ø· Ø¨Ù‡ Ø¥Ù† Ù„Ø²Ù…."""
    if col not in auto_fix_queries:
        return False

    query = _apply_dialect_ddl(conn, auto_fix_queries[col])

    try:
        await conn.execute(text(query))
        _log_schema_action("âœ… Added missing column:", table_name, col)

        if col in index_queries:
            index_query = index_queries[col]
            idx_query = _apply_dialect_ddl(conn, index_query)
            await conn.execute(text(idx_query))
            formatted_index = _format_index_from_query(table_name, index_query)
            if formatted_index:
                logger.info("âœ… Created missing index: %s", formatted_index)
            else:
                _log_schema_action("âœ… Created index for:", table_name, col)

        return True
    except Exception as exc:
        logger.error("âŒ Failed to fix %s: %s", _format_table_column(table_name, col), exc)
        return False


def _infer_index_name(index_query: str) -> str | None:
    """ÙŠØ³ØªÙ†ØªØ¬ Ø§Ø³Ù… Ø§Ù„ÙÙ‡Ø±Ø³ Ù…Ù† Ø¹Ø¨Ø§Ø±Ø© SQL."""
    pattern = re.compile(
        r"INDEX(?: IF NOT EXISTS)?\s+(?:\"([^\"]+)\"|([^\s\"]+))",
        flags=re.IGNORECASE,
    )
    match = pattern.search(index_query)
    if match:
        raw_name = match.group(1) or match.group(2)
        if raw_name and "." in raw_name:
            return raw_name.split(".")[-1]
        return raw_name
    return None


async def _get_existing_indexes(conn: AsyncConnection, table_name: str) -> set[str]:
    """ÙŠØ¬Ù„Ø¨ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„ÙÙ‡Ø§Ø±Ø³ Ø§Ù„Ø­Ø§Ù„ÙŠØ© Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª."""
    dialect_name = conn.dialect.name

    if dialect_name == "sqlite":
        result = await conn.execute(
            text("SELECT name FROM pragma_index_list(:table_name)"),
            {"table_name": table_name},
        )
        return {row[0] for row in result.fetchall()}

    result = await conn.execute(
        text("SELECT indexname FROM pg_indexes WHERE tablename = :table_name"),
        {"table_name": table_name},
    )
    return {row[0] for row in result.fetchall()}


async def _ensure_missing_indexes(
    conn: AsyncConnection,
    table_name: str,
    index_queries: dict[str, str],
    index_names: dict[str, str],
    existing_columns: set[str],
    auto_fix: bool,
) -> tuple[list[str], list[str], list[str]]:
    """ÙŠØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„ÙÙ‡Ø§Ø±Ø³ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù…Ø¹ ØªØ³Ø¬ÙŠÙ„ Ù…Ø§ ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡ Ø£Ùˆ ÙÙ‚Ø¯Ù‡."""
    missing_indexes: list[str] = []
    fixed_indexes: list[str] = []
    errors: list[str] = []

    try:
        existing_indexes = await _get_existing_indexes(conn, table_name)
    except Exception as exc:
        return missing_indexes, fixed_indexes, [
            f"Error reading indexes for {table_name}: {exc}"
        ]

    for key, index_query in index_queries.items():
        index_name = index_names.get(key) or _infer_index_name(index_query)

        if not index_name:
            errors.append(
                "Unable to infer index name for "
                f"{_format_table_column(table_name, key)}"
            )
            continue

        if index_name in existing_indexes:
            continue

        if not auto_fix:
            missing_indexes.append(_format_table_index(table_name, index_name))
            continue

        if key not in existing_columns:
            errors.append(
                "Cannot create index "
                f"{_format_table_index(table_name, index_name)} "
                f"because column {_format_table_column(table_name, key)} is missing"
            )
            missing_indexes.append(_format_table_index(table_name, index_name))
            continue

        try:
            await conn.execute(text(_apply_dialect_ddl(conn, index_query)))
            fixed_indexes.append(_format_table_index(table_name, index_name))
            logger.info(
                "âœ… Created missing index: %s", _format_table_index(table_name, index_name)
            )
        except Exception as exc:
            errors.append(
                f"Failed to create index {_format_table_index(table_name, index_name)}: {exc}"
            )
            missing_indexes.append(_format_table_index(table_name, index_name))

    return missing_indexes, fixed_indexes, errors


async def validate_and_fix_schema(auto_fix: bool = True) -> SchemaValidationResult:
    """ÙŠØªØ­Ù‚Ù‚ Ù…Ù† Ù…Ø®Ø·Ø· Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙŠØµÙ„Ø­ Ø§Ù„Ù†ÙˆØ§Ù‚Øµ Ø­Ø³Ø¨ Ø§Ù„Ø³ÙŠØ§Ø³Ø© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©."""
    results: SchemaValidationResult = {
        "status": "ok",
        "checked_tables": [],
        "missing_columns": [],
        "fixed_columns": [],
        "missing_indexes": [],
        "fixed_indexes": [],
        "errors": [],
    }

    try:
        async with engine.connect() as conn:
            for table_name, schema_info in REQUIRED_SCHEMA.items():
                if table_name not in _ALLOWED_TABLES:
                    continue

                results["checked_tables"].append(table_name)

                table_exists = await _table_exists(conn, table_name)

                if not table_exists:
                    create_query = schema_info.get("create_table")

                    if create_query and auto_fix:
                        try:
                            await conn.execute(text(_apply_dialect_ddl(conn, create_query)))
                            logger.info(f"âœ… Created missing table: {table_name}")
                            existing_columns = set(schema_info.get("columns", []))
                            results["fixed_columns"].extend(
                                [f"{table_name}.{col}" for col in existing_columns]
                            )
                            table_exists = True
                        except Exception as exc:
                            results["errors"].append(f"Failed to create table {table_name}: {exc}")
                            continue
                    else:
                        results["errors"].append(
                            f"Table {table_name} is missing and cannot be auto-created"
                        )
                        continue

                try:
                    existing_columns = await _get_existing_columns(conn, table_name)
                except Exception as exc:
                    results["errors"].append(f"Error checking table {table_name}: {exc}")
                    continue

                required_columns = set(schema_info.get("columns", []))
                missing = required_columns - existing_columns

                if missing:
                    results["missing_columns"].extend([f"{table_name}.{col}" for col in missing])

                    if auto_fix:
                        auto_fix_queries = schema_info.get("auto_fix", {})
                        index_queries = schema_info.get("indexes", {})

                        for col in missing:
                            if await _fix_missing_column(
                                conn, table_name, col, auto_fix_queries, index_queries
                            ):
                                results["fixed_columns"].append(f"{table_name}.{col}")

                index_queries = schema_info.get("indexes", {})
                index_names = schema_info.get("index_names", {})

                if index_queries:
                    missing_idx, fixed_idx, index_errors = await _ensure_missing_indexes(
                        conn=conn,
                        table_name=table_name,
                        index_queries=index_queries,
                        index_names=index_names,
                        existing_columns=existing_columns,
                        auto_fix=auto_fix,
                    )

                    results["missing_indexes"].extend(missing_idx)
                    results["fixed_indexes"].extend(fixed_idx)
                    results["errors"].extend(index_errors)

            if results["fixed_columns"] or results["fixed_indexes"]:
                await conn.commit()

    except Exception as exc:
        results["status"] = "error"
        results["errors"].append(f"Schema validation failed: {exc}")
        logger.error(f"âŒ Schema validation error: {exc}")

    unresolved_columns = set(results["missing_columns"]) - set(results["fixed_columns"])
    unresolved_indexes = set(results["missing_indexes"]) - set(results["fixed_indexes"])

    if results["errors"]:
        results["status"] = "error"
    elif unresolved_columns or unresolved_indexes:
        results["status"] = "warning"

    return results


async def validate_schema_on_startup() -> None:
    """ÙŠÙ†ÙØ° ØªØ­Ù‚Ù‚ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ´ØºÙŠÙ„ ÙˆÙŠØ¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ø§Ù„Ø³Ø¬Ù„."""
    logger.info("ğŸ” Validating database schema... (Ø¬Ø§Ø±ÙŠ ÙØ­Øµ Ù…Ø®Ø·Ø· Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª)")
    logger.info(
        format_architecture_system_principles(
            header="Ù…Ø¨Ø§Ø¯Ø¦ Ø§Ù„Ù…Ø¹Ù…Ø§Ø±ÙŠØ© ÙˆØ­ÙˆÙƒÙ…Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Validation Context):",
            bullet="-",
            include_header=True,
        )
    )

    results = await validate_and_fix_schema(auto_fix=True)

    if results["status"] == "ok":
        logger.info("âœ… Schema validation passed (Ø§Ù„Ù…Ø®Ø·Ø· Ø³Ù„ÙŠÙ…)")
    elif results["fixed_columns"] or results["fixed_indexes"]:
        logger.warning(
            "âš ï¸ Schema auto-fixed: "
            f"columns={results['fixed_columns']}, indexes={results['fixed_indexes']}"
        )
    elif results["missing_columns"] or results["missing_indexes"]:
        logger.error(
            "âŒ CRITICAL: Missing schema elements: "
            f"columns={results['missing_columns']}, indexes={results['missing_indexes']}"
        )

    if results["errors"]:
        for error in results["errors"]:
            logger.error(f"   Error: {error}")
