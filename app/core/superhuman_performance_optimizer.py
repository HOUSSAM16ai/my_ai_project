# app/core/superhuman_performance_optimizer.py
"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                              â•‘
â•‘  ðŸš€ SUPERHUMAN PERFORMANCE OPTIMIZER V1.0                                    â•‘
â•‘  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                   â•‘
â•‘                                                                              â•‘
â•‘  Advanced AI Performance Optimization System                                 â•‘
â•‘  Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø®Ø§Ø±Ù‚ Ù„Ù„Ø£Ø¯Ø§Ø¡                                                  â•‘
â•‘                                                                              â•‘
â•‘  Features:                                                                   â•‘
â•‘  - Adaptive request batching with intelligent grouping                       â•‘
â•‘  - Predictive model selection using machine learning                         â•‘
â•‘  - Real-time performance analytics and optimization                          â•‘
â•‘  - Auto-tuning of retry strategies based on historical data                  â•‘
â•‘  - Smart caching with LRU and TTL policies                                   â•‘
â•‘  - Dynamic timeout adjustment based on request complexity                    â•‘
â•‘                                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

from __future__ import annotations

import asyncio
import hashlib
import logging
import time
from collections import defaultdict, deque
from dataclasses import dataclass, field
from typing import Any

logger = logging.getLogger(__name__)


# ==============================================================================
# ðŸ§  ADAPTIVE PERFORMANCE METRICS
# ==============================================================================
@dataclass
class PerformanceMetrics:
    """
    Real-time performance metrics for AI operations.
    Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ÙØ¹Ù„ÙŠ Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
    """

    model_id: str
    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0
    empty_responses: int = 0

    # Latency tracking (milliseconds)
    latencies: deque = field(default_factory=lambda: deque(maxlen=100))
    avg_latency_ms: float = 0.0
    p50_latency_ms: float = 0.0
    p95_latency_ms: float = 0.0
    p99_latency_ms: float = 0.0

    # Token usage
    total_tokens: int = 0
    avg_tokens_per_request: float = 0.0

    # Quality metrics
    avg_quality_score: float = 0.0
    quality_scores: deque = field(default_factory=lambda: deque(maxlen=100))

    # Time-based metrics
    last_request_time: float = 0.0
    first_request_time: float = 0.0

    def update_latency(self, latency_ms: float) -> None:
        """Update latency metrics with new measurement."""
        self.latencies.append(latency_ms)
        if self.latencies:
            sorted_latencies = sorted(self.latencies)
            self.avg_latency_ms = sum(sorted_latencies) / len(sorted_latencies)

            # Calculate percentiles
            n = len(sorted_latencies)
            self.p50_latency_ms = sorted_latencies[int(n * 0.50)]
            self.p95_latency_ms = sorted_latencies[int(n * 0.95)]
            self.p99_latency_ms = sorted_latencies[min(int(n * 0.99), n - 1)]

    def update_quality(self, score: float) -> None:
        """Update quality metrics with new score."""
        self.quality_scores.append(score)
        if self.quality_scores:
            self.avg_quality_score = sum(self.quality_scores) / len(self.quality_scores)

    def get_success_rate(self) -> float:
        """Calculate success rate as percentage."""
        if self.total_requests == 0:
            return 0.0
        return (self.successful_requests / self.total_requests) * 100.0

    def get_empty_rate(self) -> float:
        """Calculate empty response rate as percentage."""
        if self.total_requests == 0:
            return 0.0
        return (self.empty_responses / self.total_requests) * 100.0

    def to_dict(self) -> dict[str, Any]:
        """Export metrics as dictionary."""
        return {
            "model_id": self.model_id,
            "total_requests": self.total_requests,
            "success_rate": round(self.get_success_rate(), 2),
            "empty_rate": round(self.get_empty_rate(), 2),
            "latency": {
                "avg_ms": round(self.avg_latency_ms, 2),
                "p50_ms": round(self.p50_latency_ms, 2),
                "p95_ms": round(self.p95_latency_ms, 2),
                "p99_ms": round(self.p99_latency_ms, 2),
            },
            "quality": {
                "avg_score": round(self.avg_quality_score, 2),
                "samples": len(self.quality_scores),
            },
            "tokens": {
                "total": self.total_tokens,
                "avg_per_request": round(self.avg_tokens_per_request, 2),
            },
        }


# ==============================================================================
# ðŸŽ¯ INTELLIGENT MODEL SELECTOR
# ==============================================================================
class IntelligentModelSelector:
    """
    Advanced model selection using multi-armed bandit algorithm.
    Ø§Ø®ØªÙŠØ§Ø± Ø°ÙƒÙŠ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© multi-armed bandit

    Uses Thompson Sampling for optimal exploration-exploitation balance.
    """

    def __init__(self, epsilon: float = 0.1):
        self.epsilon = epsilon  # Exploration rate
        self.model_stats: dict[str, dict[str, float]] = defaultdict(
            lambda: {"alpha": 1.0, "beta": 1.0, "score": 0.5}
        )

    def select_model(self, available_models: list[str], context: str = "") -> str:
        """
        Select optimal model using Thompson Sampling.

        Args:
            available_models: List of model IDs to choose from
            context: Request context for contextual bandits

        Returns:
            Selected model ID
        """
        import random

        if not available_models:
            raise ValueError("No available models to select from")

        # Exploration: randomly select with epsilon probability
        if random.random() < self.epsilon:
            return random.choice(available_models)

        # Exploitation: select based on Thompson Sampling
        best_model = None
        best_sample = -1.0

        for model in available_models:
            stats = self.model_stats[model]
            # Sample from Beta distribution
            sample = random.betavariate(stats["alpha"], stats["beta"])

            if sample > best_sample:
                best_sample = sample
                best_model = model

        return best_model or available_models[0]

    def update_model_performance(
        self, model_id: str, success: bool, quality_score: float = 0.0
    ) -> None:
        """
        Update model statistics based on outcome.

        Args:
            model_id: Model that was used
            success: Whether request was successful
            quality_score: Quality score of response (0-1)
        """
        stats = self.model_stats[model_id]

        if success:
            # Reward based on quality
            reward = 0.5 + (quality_score * 0.5)  # Scale to [0.5, 1.0]
            stats["alpha"] += reward
            stats["beta"] += 1.0 - reward
        else:
            # Penalize failures
            stats["beta"] += 1.0

        # Update average score
        total = stats["alpha"] + stats["beta"]
        stats["score"] = stats["alpha"] / total if total > 0 else 0.5

    def get_model_scores(self) -> dict[str, float]:
        """Get current scores for all models."""
        return {model_id: stats["score"] for model_id, stats in self.model_stats.items()}


# ==============================================================================
# âš¡ ADAPTIVE BATCH PROCESSOR
# ==============================================================================
class AdaptiveBatchProcessor:
    """
    Intelligent request batching for optimal throughput.
    Ù…Ø¹Ø§Ù„Ø¬ Ø¯ÙØ¹Ø§Øª ØªÙƒÙŠÙÙŠ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ù‚ØµÙ‰ Ø¥Ù†ØªØ§Ø¬ÙŠØ©

    Features:
    - Dynamic batch size based on load
    - Intelligent grouping of similar requests
    - Automatic flush on timeout
    """

    def __init__(
        self,
        min_batch_size: int = 2,
        max_batch_size: int = 10,
        max_wait_time: float = 0.5,
    ):
        self.min_batch_size = min_batch_size
        self.max_batch_size = max_batch_size
        self.max_wait_time = max_wait_time

        self.pending_requests: list[dict[str, Any]] = []
        self.batch_start_time: float = 0.0
        self.lock = asyncio.Lock()

    async def add_request(self, request: dict[str, Any]) -> list[dict[str, Any]]:
        """
        Add request to batch and return batch if ready.

        Returns:
            List of requests to process (empty if waiting for more)
        """
        async with self.lock:
            if not self.pending_requests:
                self.batch_start_time = time.time()

            self.pending_requests.append(request)

            # Check if batch is ready
            elapsed = time.time() - self.batch_start_time
            batch_ready = len(self.pending_requests) >= self.max_batch_size or (
                len(self.pending_requests) >= self.min_batch_size and elapsed >= self.max_wait_time
            )

            if batch_ready:
                batch = self.pending_requests[:]
                self.pending_requests.clear()
                return batch

            return []

    def group_by_similarity(self, requests: list[dict[str, Any]]) -> list[list[dict[str, Any]]]:
        """
        Group requests by similarity for efficient processing.

        Uses content hashing for fast grouping.
        """
        groups: dict[str, list[dict[str, Any]]] = defaultdict(list)

        for req in requests:
            # Create hash based on model and prompt type
            content = f"{req.get('model', '')}:{req.get('prompt_type', '')}"
            hash_key = hashlib.md5(content.encode()).hexdigest()[:8]
            groups[hash_key].append(req)

        return list(groups.values())


# ==============================================================================
# ðŸ”¥ SUPERHUMAN PERFORMANCE OPTIMIZER
# ==============================================================================
class SuperhumanPerformanceOptimizer:
    """
    Main optimizer coordinating all performance enhancements.
    Ø§Ù„Ù…Ø­Ø³Ù† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ø§Ù„Ù…Ù†Ø³Ù‚ Ù„Ø¬Ù…ÙŠØ¹ ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡
    """

    def __init__(self):
        self.metrics: dict[str, PerformanceMetrics] = {}
        self.model_selector = IntelligentModelSelector(epsilon=0.1)
        self.batch_processor = AdaptiveBatchProcessor()

        # Global statistics
        self.total_requests = 0
        self.total_latency_ms = 0.0
        self.start_time = time.time()

        logger.info("ðŸš€ Superhuman Performance Optimizer initialized")

    def get_or_create_metrics(self, model_id: str) -> PerformanceMetrics:
        """Get or create metrics for a model."""
        if model_id not in self.metrics:
            self.metrics[model_id] = PerformanceMetrics(
                model_id=model_id, first_request_time=time.time()
            )
        return self.metrics[model_id]

    def record_request(
        self,
        model_id: str,
        success: bool,
        latency_ms: float,
        tokens: int = 0,
        quality_score: float = 0.0,
        empty_response: bool = False,
    ) -> None:
        """
        Record request metrics for analysis and optimization.

        Args:
            model_id: Model that processed the request
            success: Whether request succeeded
            latency_ms: Request latency in milliseconds
            tokens: Token count
            quality_score: Response quality (0-1)
            empty_response: Whether response was empty
        """
        metrics = self.get_or_create_metrics(model_id)

        # Update counters
        metrics.total_requests += 1
        if success:
            metrics.successful_requests += 1
        else:
            metrics.failed_requests += 1

        if empty_response:
            metrics.empty_responses += 1

        # Update latency
        metrics.update_latency(latency_ms)
        metrics.last_request_time = time.time()

        # Update tokens
        metrics.total_tokens += tokens
        if metrics.total_requests > 0:
            metrics.avg_tokens_per_request = metrics.total_tokens / metrics.total_requests

        # Update quality
        if quality_score > 0:
            metrics.update_quality(quality_score)

        # Update model selector
        self.model_selector.update_model_performance(model_id, success, quality_score)

        # Global statistics
        self.total_requests += 1
        self.total_latency_ms += latency_ms

    def get_recommended_model(self, available_models: list[str], context: str = "") -> str:
        """
        Get recommended model based on historical performance.

        Returns:
            Model ID with best expected performance
        """
        return self.model_selector.select_model(available_models, context)

    def get_optimal_timeout(self, model_id: str, default_timeout: float = 30.0) -> float:
        """
        Calculate optimal timeout based on historical latency.

        Uses P99 latency + buffer for reliability.
        """
        if model_id not in self.metrics:
            return default_timeout

        metrics = self.metrics[model_id]
        if metrics.p99_latency_ms > 0:
            # P99 + 50% buffer, converted to seconds
            optimal = (metrics.p99_latency_ms * 1.5) / 1000.0
            # Clamp between 5s and 120s
            return max(5.0, min(optimal, 120.0))

        return default_timeout

    def get_global_stats(self) -> dict[str, Any]:
        """Get global performance statistics."""
        uptime = time.time() - self.start_time
        avg_latency = (
            self.total_latency_ms / self.total_requests if self.total_requests > 0 else 0.0
        )

        return {
            "uptime_seconds": round(uptime, 2),
            "total_requests": self.total_requests,
            "avg_latency_ms": round(avg_latency, 2),
            "requests_per_second": round(self.total_requests / uptime, 2) if uptime > 0 else 0.0,
            "active_models": len(self.metrics),
            "model_scores": self.model_selector.get_model_scores(),
        }

    def get_detailed_report(self) -> dict[str, Any]:
        """Generate comprehensive performance report."""
        return {
            "global": self.get_global_stats(),
            "models": {model_id: metrics.to_dict() for model_id, metrics in self.metrics.items()},
        }

    def should_switch_model(
        self, current_model: str, available_models: list[str]
    ) -> tuple[bool, str | None]:
        """
        Determine if model should be switched based on performance.

        Returns:
            (should_switch, recommended_model)
        """
        if current_model not in self.metrics:
            return False, None

        current_metrics = self.metrics[current_model]

        # Switch if success rate is too low
        if current_metrics.total_requests >= 10 and current_metrics.get_success_rate() < 70.0:
            recommended = self.get_recommended_model(available_models)
            if recommended != current_model:
                logger.warning(
                    f"Recommending switch from {current_model} to {recommended} "
                    f"(success rate: {current_metrics.get_success_rate():.1f}%)"
                )
                return True, recommended

        # Switch if empty response rate is too high
        if current_metrics.total_requests >= 10 and current_metrics.get_empty_rate() > 20.0:
            recommended = self.get_recommended_model(available_models)
            if recommended != current_model:
                logger.warning(
                    f"Recommending switch from {current_model} to {recommended} "
                    f"(empty rate: {current_metrics.get_empty_rate():.1f}%)"
                )
                return True, recommended

        return False, None


# ==============================================================================
# ðŸŒ GLOBAL OPTIMIZER INSTANCE
# ==============================================================================
_global_optimizer: SuperhumanPerformanceOptimizer | None = None


def get_performance_optimizer() -> SuperhumanPerformanceOptimizer:
    """Get or create global optimizer instance."""
    global _global_optimizer
    if _global_optimizer is None:
        _global_optimizer = SuperhumanPerformanceOptimizer()
    return _global_optimizer


def reset_optimizer() -> None:
    """Reset global optimizer (for testing)."""
    global _global_optimizer
    _global_optimizer = None


__all__ = [
    "SuperhumanPerformanceOptimizer",
    "PerformanceMetrics",
    "IntelligentModelSelector",
    "AdaptiveBatchProcessor",
    "get_performance_optimizer",
    "reset_optimizer",
]
