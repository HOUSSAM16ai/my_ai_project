"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                              â•‘
â•‘  ðŸš€ SUPERHUMAN PERFORMANCE OPTIMIZER V1.0                                    â•‘
â•‘  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                   â•‘
â•‘                                                                              â•‘
â•‘  Advanced AI Performance Optimization System                                 â•‘
â•‘  Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø®Ø§Ø±Ù‚ Ù„Ù„Ø£Ø¯Ø§Ø¡                                                  â•‘
â•‘                                                                              â•‘
â•‘  Features:                                                                   â•‘
â•‘  - Adaptive request batching with intelligent grouping                       â•‘
â•‘  - Predictive model selection using machine learning                         â•‘
â•‘  - Real-time performance analytics and optimization                          â•‘
â•‘  - Auto-tuning of retry strategies based on historical data                  â•‘
â•‘  - Smart caching with LRU and TTL policies                                   â•‘
â•‘  - Dynamic timeout adjustment based on request complexity                    â•‘
â•‘                                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
from __future__ import annotations

import asyncio
import logging
import time
from collections import defaultdict, deque
from dataclasses import dataclass, field
from typing import Any

logger = logging.getLogger(__name__)


@dataclass
class PerformanceMetrics:
    """
    Real-time performance metrics for AI operations.
    Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ÙØ¹Ù„ÙŠ Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
    """
    model_id: str
    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0
    empty_responses: int = 0
    latencies: deque = field(default_factory=lambda : deque(maxlen=100))
    avg_latency_ms: float = 0.0
    p50_latency_ms: float = 0.0
    p95_latency_ms: float = 0.0
    p99_latency_ms: float = 0.0
    total_tokens: int = 0
    avg_tokens_per_request: float = 0.0
    avg_quality_score: float = 0.0
    quality_scores: deque = field(default_factory=lambda : deque(maxlen=100))
    last_request_time: float = 0.0
    first_request_time: float = 0.0

    def update_latency(self, latency_ms: float) ->None:
        """Update latency metrics with new measurement."""
        self.latencies.append(latency_ms)
        if self.latencies:
            sorted_latencies = sorted(self.latencies)
            self.avg_latency_ms = sum(sorted_latencies) / len(sorted_latencies)
            n = len(sorted_latencies)
            self.p50_latency_ms = sorted_latencies[int(n * 0.5)]
            self.p95_latency_ms = sorted_latencies[int(n * 0.95)]
            self.p99_latency_ms = sorted_latencies[min(int(n * 0.99), n - 1)]

    def update_quality(self, score: float) ->None:
        """Update quality metrics with new score."""
        self.quality_scores.append(score)
        if self.quality_scores:
            self.avg_quality_score = sum(self.quality_scores) / len(self.
                quality_scores)

    def get_success_rate(self) ->float:
        """Calculate success rate as percentage."""
        if self.total_requests == 0:
            return 0.0
        return self.successful_requests / self.total_requests * 100.0

    def get_empty_rate(self) ->float:
        """Calculate empty response rate as percentage."""
        if self.total_requests == 0:
            return 0.0
        return self.empty_responses / self.total_requests * 100.0

    def to_dict(self) ->dict[str, Any]:
        """Export metrics as dictionary."""
        return {'model_id': self.model_id, 'total_requests': self.
            total_requests, 'success_rate': round(self.get_success_rate(),
            2), 'empty_rate': round(self.get_empty_rate(), 2), 'latency': {
            'avg_ms': round(self.avg_latency_ms, 2), 'p50_ms': round(self.
            p50_latency_ms, 2), 'p95_ms': round(self.p95_latency_ms, 2),
            'p99_ms': round(self.p99_latency_ms, 2)}, 'quality': {
            'avg_score': round(self.avg_quality_score, 2), 'samples': len(
            self.quality_scores)}, 'tokens': {'total': self.total_tokens,
            'avg_per_request': round(self.avg_tokens_per_request, 2)}}


class IntelligentModelSelector:
    """
    Advanced model selection using multi-armed bandit algorithm.
    Ø§Ø®ØªÙŠØ§Ø± Ø°ÙƒÙŠ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© multi-armed bandit

    Uses Thompson Sampling for optimal exploration-exploitation balance.
    """

    def __init__(self, epsilon: float=0.1):
        self.epsilon = epsilon
        self.model_stats: dict[str, dict[str, float]] = defaultdict(lambda :
            {'alpha': 1.0, 'beta': 1.0, 'score': 0.5})

    def select_model(self, available_models: list[str], context: str='') ->str:
        """
        Select optimal model using Thompson Sampling.

        Args:
            available_models: List of model IDs to choose from
            context: Request context for contextual bandits

        Returns:
            Selected model ID
        """
        import random
        if not available_models:
            raise ValueError('No available models to select from')
        if random.random() < self.epsilon:
            return random.choice(available_models)
        best_model = None
        best_sample = -1.0
        for model in available_models:
            stats = self.model_stats[model]
            sample = random.betavariate(stats['alpha'], stats['beta'])
            if sample > best_sample:
                best_sample = sample
                best_model = model
        return best_model or available_models[0]

    def update_model_performance(self, model_id: str, success: bool,
        quality_score: float=0.0) ->None:
        """
        Update model statistics based on outcome.

        Args:
            model_id: Model that was used
            success: Whether request was successful
            quality_score: Quality score of response (0-1)
        """
        stats = self.model_stats[model_id]
        if success:
            reward = 0.5 + quality_score * 0.5
            stats['alpha'] += reward
            stats['beta'] += 1.0 - reward
        else:
            stats['beta'] += 1.0
        total = stats['alpha'] + stats['beta']
        stats['score'] = stats['alpha'] / total if total > 0 else 0.5

    def get_model_scores(self) ->dict[str, float]:
        """Get current scores for all models."""
        return {model_id: stats['score'] for model_id, stats in self.
            model_stats.items()}


class AdaptiveBatchProcessor:
    """
    Intelligent request batching for optimal throughput.
    Ù…Ø¹Ø§Ù„Ø¬ Ø¯ÙØ¹Ø§Øª ØªÙƒÙŠÙÙŠ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ù‚ØµÙ‰ Ø¥Ù†ØªØ§Ø¬ÙŠØ©

    Features:
    - Dynamic batch size based on load
    - Intelligent grouping of similar requests
    - Automatic flush on timeout
    """

    def __init__(self, min_batch_size: int=2, max_batch_size: int=10,
        max_wait_time: float=0.5):
        self.min_batch_size = min_batch_size
        self.max_batch_size = max_batch_size
        self.max_wait_time = max_wait_time
        self.pending_requests: list[dict[str, Any]] = []
        self.batch_start_time: float = 0.0
        self.lock = asyncio.Lock()


class SuperhumanPerformanceOptimizer:
    """
    Main optimizer coordinating all performance enhancements.
    Ø§Ù„Ù…Ø­Ø³Ù† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ø§Ù„Ù…Ù†Ø³Ù‚ Ù„Ø¬Ù…ÙŠØ¹ ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡
    """

    def __init__(self):
        self.metrics: dict[str, PerformanceMetrics] = {}
        self.model_selector = IntelligentModelSelector(epsilon=0.1)
        self.batch_processor = AdaptiveBatchProcessor()
        self.total_requests = 0
        self.total_latency_ms = 0.0
        self.start_time = time.time()
        logger.info('ðŸš€ Superhuman Performance Optimizer initialized')

    def get_or_create_metrics(self, model_id: str) ->PerformanceMetrics:
        """Get or create metrics for a model."""
        if model_id not in self.metrics:
            self.metrics[model_id] = PerformanceMetrics(model_id=model_id,
                first_request_time=time.time())
        return self.metrics[model_id]

    def record_request(self, model_id: str, success: bool, latency_ms:
        float, tokens: int=0, quality_score: float=0.0, empty_response:
        bool=False) ->None:
        """
        Record request metrics for analysis and optimization.

        Args:
            model_id: Model that processed the request
            success: Whether request succeeded
            latency_ms: Request latency in milliseconds
            tokens: Token count
            quality_score: Response quality (0-1)
            empty_response: Whether response was empty
        """
        metrics = self.get_or_create_metrics(model_id)
        metrics.total_requests += 1
        if success:
            metrics.successful_requests += 1
        else:
            metrics.failed_requests += 1
        if empty_response:
            metrics.empty_responses += 1
        metrics.update_latency(latency_ms)
        metrics.last_request_time = time.time()
        metrics.total_tokens += tokens
        if metrics.total_requests > 0:
            metrics.avg_tokens_per_request = (metrics.total_tokens /
                metrics.total_requests)
        if quality_score > 0:
            metrics.update_quality(quality_score)
        self.model_selector.update_model_performance(model_id, success,
            quality_score)
        self.total_requests += 1
        self.total_latency_ms += latency_ms

    def get_recommended_model(self, available_models: list[str], context:
        str='') ->str:
        """
        Get recommended model based on historical performance.

        Returns:
            Model ID with best expected performance
        """
        return self.model_selector.select_model(available_models, context)

    def get_global_stats(self) ->dict[str, Any]:
        """Get global performance statistics."""
        uptime = time.time() - self.start_time
        avg_latency = (self.total_latency_ms / self.total_requests if self.
            total_requests > 0 else 0.0)
        return {'uptime_seconds': round(uptime, 2), 'total_requests': self.
            total_requests, 'avg_latency_ms': round(avg_latency, 2),
            'requests_per_second': round(self.total_requests / uptime, 2) if
            uptime > 0 else 0.0, 'active_models': len(self.metrics),
            'model_scores': self.model_selector.get_model_scores()}

    def get_detailed_report(self) ->dict[str, Any]:
        """Generate comprehensive performance report."""
        return {'global': self.get_global_stats(), 'models': {model_id:
            metrics.to_dict() for model_id, metrics in self.metrics.items()}}


_global_optimizer: SuperhumanPerformanceOptimizer | None = None


def get_performance_optimizer() ->SuperhumanPerformanceOptimizer:
    """Get or create global optimizer instance."""
    global _global_optimizer
    if _global_optimizer is None:
        _global_optimizer = SuperhumanPerformanceOptimizer()
    return _global_optimizer


def reset_optimizer() ->None:
    """Reset global optimizer (for testing)."""
    global _global_optimizer
    _global_optimizer = None


__all__ = ['AdaptiveBatchProcessor', 'IntelligentModelSelector',
    'PerformanceMetrics', 'SuperhumanPerformanceOptimizer',
    'get_performance_optimizer', 'reset_optimizer']
