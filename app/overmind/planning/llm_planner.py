# -*- coding: utf-8 -*-
# app/overmind/planning/llm_planner.py
# -*- coding: utf-8 -*-
"""
LLM GROUNDED PLANNER (LEVEL‑4 DEEP SCAN EDITION)
===============================================
Version: 4.2.0-l4
Status : Production Hardened / Deterministic Pattern First / Safe Fallback
Author : Overmind Orchestrator Core

Purpose
-------
تحويل الهدف النصي (Mission Objective) إلى مخطط مهام قابل للتنفيذ (MissionPlanSchema)
مع دعم "المستوى 4" الذي يشمل:
- Deep Project Scan (list_dir + selective read_file with ignore_missing)
- Multi‑phase analytical reasoning (generic_think layers)
- Final synthesis & artifact creation (ensure_file + write_file)
- Zero-stall friendly (READ tasks لا تفشل عند غياب الملف)
- Deterministic pattern specialization للمهام المعمارية / التقارير

High-Level Level‑4 Pipeline (Architecture / Summary Pattern)
-----------------------------------------------------------
Layer 0: list_dir (configurable target roots)
Layer 1: read_file (core manifest + sampled source)
Layer 2: analytical generic_think tasks (components / data-flow / dependencies / risks)
Layer 3: synthesis generic_think (consolidated structured report)
Layer 4: ensure_file (optional) + write_file final artifact
(Extra) Optional executive summary / refined variants (future extension)

Key Design Guarantees
---------------------
1. Never returns zero tasks (analytic minimal fallback always available).
2. Always yields final write_file (or ensure_file + write_file) if objective mentions "summary" / "report" / "file".
3. read_file tasks tolerant (ignore_missing=True) preventing pipeline stall.
4. Tool names canonical (list_dir, read_file, generic_think, ensure_file, write_file).
5. Interpolation markers stable ({{tXX.content}} / {{tYY.answer}}).
6. Risk scoring heuristic stored in tool_args["_meta_risk"] (non-breaking meta).
7. Deterministic ordering & IDs (t01..tNN). No duplicates / no cycles generated by pattern path.
8. Arabic detection: إذا احتوى الهدف على حروف عربية أو كلمة 'arabic' → المخرجات الأساسية بالعربية.
9. Configurable scanning breadth & depth via environment flags.
10. Strict allowlist enforcement (non-allowed tools coerced to generic_think if normalization path used).

Environment Flags (Level‑4 Focus)
---------------------------------
PLANNER_L4_ENABLED=1|0
PLANNER_L4_SCAN_DIRS="app,."                 Comma separated roots to list_dir
PLANNER_L4_MAX_LIST_DIRS=4                  Cap list_dir tasks
PLANNER_L4_CORE_FILES="README.md,requirements.txt,docker-compose.yml,config.py,Dockerfile"
PLANNER_L4_INCLUDE_CODE_EXTS=".py,.md,.yml,.yaml,.toml,.json,.txt,.ini,.cfg"
PLANNER_L4_MAX_CODE_FILES=12                Max sampled code/extra files (besides core)
PLANNER_L4_MAX_TOTAL_READS=24               Safety ceiling for read_file tasks
PLANNER_L4_MULTI_ANALYSIS=1                 If 1 produce multi analytic think tasks
PLANNER_L4_ENABLE_ENSURE=1                  Insert ensure_file before write_file if tool available
PLANNER_L4_FINAL_FILENAME_HINT="ARCHITECTURE_PRINCIPLES.md"  Preferred fallback file name
PLANNER_L4_MIN_SYNTH_SECTIONS=6             Soft guidance (# sections for synthesis)
PLANNER_L4_SAMPLING_STRATEGY="size|alpha"   Future extension (currently heuristic)
PLANNER_L4_PRIORITY_KEYWORDS="service,api,model,config,core,main,router,controller"

General Planner Flags (legacy compatibility)
--------------------------------------------
PLANNER_ALLOWED_TOOLS="list_dir,read_file,ensure_file,generic_think,write_file"
PLANNER_ENFORCE_ALLOWED_TOOLS=1
PLANNER_AUTO_FIX_FILE_TASKS=1
PLANNER_FORCE_FILE_TOOLS=1
PLANNER_FILE_DEFAULT_EXT=.md
PLANNER_FILE_DEFAULT_CONTENT="Placeholder content (auto-generated)."
PLANNER_FORCE_REPORT_NAME=...
PLANNER_MAX_WRITES=2
LLM_PLANNER_MAX_TASKS=40    (Ceiling guard)
FALLBACK_ALLOW=1             (If structured LLM path is used; pattern path bypasses)
LLM_PLANNER_STRICT_JSON=0

LLM Mode (Structured) Usage
---------------------------
In Level‑4 architecture summary goals we rely on deterministic pattern.
LLM structured planning only used for non-matching objectives or future complex flows.

Arabic Detection
----------------
- If objective contains any Arabic Unicode (U+0600–U+06FF) OR token 'arabic'
  -> All descriptive prompts shift to Arabic where clarity preserved,
     else English default.

Fallback Strategy
-----------------
- If pattern engine fails & no LLM structured plan → analytic minimal fallback:
    t01 generic_think
    t02 write_file (filename forced or derived)
- Guarantees plan viability in all error states.

Extensibility
-------------
Add new deterministic patterns inside PatternEngine (e.g., dependency graph, test coverage summary).
Add advanced parsing tasks (future parse_python_graph tool) easily.

"""

from __future__ import annotations

import json
import logging
import os
import re
import time
from typing import Any, Dict, List, Optional, Tuple

# --------------------------------------------------------------------------------------
# Logging
# --------------------------------------------------------------------------------------
_LOG = logging.getLogger("llm_planner")
_env_level = os.getenv("LLM_PLANNER_LOG_LEVEL", "").upper()
if _env_level:
    _LOG.setLevel(getattr(logging, _env_level, logging.INFO))
else:
    _LOG.setLevel(logging.INFO)
if not _LOG.handlers:
    _h = logging.StreamHandler()
    _h.setFormatter(logging.Formatter("[%(asctime)s][%(levelname)s][%(name)s] %(message)s"))
    _LOG.addHandler(_h)

# --------------------------------------------------------------------------------------
# Base / Schemas Imports (fallback stub option)
# --------------------------------------------------------------------------------------
_ALLOW_STUB = os.getenv("LLM_PLANNER_ALLOW_STUB", "0") == "1"
try:
    from .base_planner import (
        BasePlanner,
        PlannerError,
        PlanValidationError,
    )
except Exception as _e:  # pragma: no cover
    if not _ALLOW_STUB:
        raise RuntimeError(
            "Failed to import base_planner (set LLM_PLANNER_ALLOW_STUB=1 for dev stub)."
        ) from _e

    class PlannerError(Exception):  # type: ignore
        def __init__(self, msg: str, planner: str = "stub", objective: str = "", **extra):
            super().__init__(msg)
            self.planner = planner
            self.objective = objective
            self.extra = extra or {}

    class PlanValidationError(PlannerError):  # type: ignore
        pass

    class BasePlanner:  # type: ignore
        name = "base_planner_stub"

        @classmethod
        def live_planner_classes(cls):
            return {}

        @classmethod
        def planner_metadata(cls):
            return {}

    _LOG.error("USING STUB BasePlanner (development mode).")

from .schemas import MissionPlanSchema, PlannedTask, PlanningContext  # type: ignore

# --------------------------------------------------------------------------------------
# Optional Services
# --------------------------------------------------------------------------------------
try:
    from app.services import maestro  # type: ignore
except Exception:
    maestro = None  # type: ignore

try:
    from app.services import agent_tools  # type: ignore
except Exception:
    agent_tools = None  # type: ignore

CANON_FUNC = getattr(agent_tools, "canonicalize_tool_name", None) if agent_tools else None

# --------------------------------------------------------------------------------------
# Environment / Config
# --------------------------------------------------------------------------------------
STRICT_JSON_ONLY = os.getenv("LLM_PLANNER_STRICT_JSON", "0") == "1"
FALLBACK_ALLOW  = os.getenv("FALLBACK_ALLOW", "1") == "1"
MAX_TASKS_GLOBAL = int(os.getenv("LLM_PLANNER_MAX_TASKS", "40"))

AUTO_FIX_FILE_TASKS = os.getenv("PLANNER_AUTO_FIX_FILE_TASKS", "1") == "1"
FORCE_FILE_TOOLS   = os.getenv("PLANNER_FORCE_FILE_TOOLS", "1") == "1"
FILE_DEFAULT_EXT   = os.getenv("PLANNER_FILE_DEFAULT_EXT", ".md")
FILE_DEFAULT_CONTENT = os.getenv(
    "PLANNER_FILE_DEFAULT_CONTENT",
    "Placeholder content (auto-generated)."
)
ALLOWED_TOOLS_RAW = os.getenv(
    "PLANNER_ALLOWED_TOOLS",
    "list_dir,read_file,ensure_file,generic_think,write_file"
)
ALLOWED_TOOLS = {t.strip() for t in ALLOWED_TOOLS_RAW.split(",") if t.strip()}
ENFORCE_ALLOWED = os.getenv("PLANNER_ENFORCE_ALLOWED_TOOLS", "1") == "1"
FORCE_REPORT_NAME = os.getenv("PLANNER_FORCE_REPORT_NAME", "").strip() or None
MAX_WRITES = int(os.getenv("PLANNER_MAX_WRITES", "2"))

# Level 4 Specific
L4_ENABLED        = os.getenv("PLANNER_L4_ENABLED", "1") == "1"
L4_SCAN_DIRS      = [d.strip() for d in os.getenv("PLANNER_L4_SCAN_DIRS", "app,.").split(",") if d.strip()]
L4_MAX_LIST_DIRS  = int(os.getenv("PLANNER_L4_MAX_LIST_DIRS", "4"))
L4_CORE_FILES     = [f.strip() for f in os.getenv(
    "PLANNER_L4_CORE_FILES",
    "README.md,requirements.txt,docker-compose.yml,config.py,Dockerfile"
).split(",") if f.strip()]
L4_INCLUDE_CODE_EXTS = [e.strip().lower() for e in os.getenv(
    "PLANNER_L4_INCLUDE_CODE_EXTS",
    ".py,.md,.yml,.yaml,.toml,.json,.txt,.ini,.cfg"
).split(",") if e.strip()]
L4_MAX_CODE_FILES      = int(os.getenv("PLANNER_L4_MAX_CODE_FILES", "12"))
L4_MAX_TOTAL_READS     = int(os.getenv("PLANNER_L4_MAX_TOTAL_READS", "24"))
L4_MULTI_ANALYSIS      = os.getenv("PLANNER_L4_MULTI_ANALYSIS", "1") == "1"
L4_ENABLE_ENSURE       = os.getenv("PLANNER_L4_ENABLE_ENSURE", "1") == "1"
L4_FINAL_FILENAME_HINT = os.getenv("PLANNER_L4_FINAL_FILENAME_HINT", "ARCHITECTURE_PRINCIPLES.md").strip()
L4_MIN_SYNTH_SECTIONS  = int(os.getenv("PLANNER_L4_MIN_SYNTH_SECTIONS", "6"))
L4_PRIORITY_KEYWORDS   = [k.strip().lower() for k in os.getenv(
    "PLANNER_L4_PRIORITY_KEYWORDS",
    "service,api,model,config,core,main,router,controller"
).split(",") if k.strip()]

# Regex / Aliases
TOOL_ID_REGEX = re.compile(r"^tool:\d{1,5}$")

CANON_WRITE = "write_file"
CANON_READ  = "read_file"
CANON_THINK = "generic_think"
TOOL_ENSURE = "ensure_file"
CANON_LIST  = "list_dir"

WRITE_INTENT = {"write", "create", "generate", "append", "produce", "save", "output", "file"}
READ_INTENT  = {"read", "inspect", "load", "open", "view"}
THINK_INTENT = {"analyze", "analysis", "think", "reason", "summarize", "synthesize"}

MANDATORY_ARGS = {
    CANON_WRITE: ["path", "content"],
    CANON_READ:  ["path"],
    TOOL_ENSURE: ["path"]
}

# --------------------------------------------------------------------------------------
# Helpers
# --------------------------------------------------------------------------------------
def _clip(s: str, n: int = 140) -> str:
    if not s:
        return ""
    return s if len(s) <= n else s[: n - 3] + "..."

def _lower(x: Any) -> str:
    return str(x or "").strip().lower()

def _objective_has_arabic(obj: str) -> bool:
    if "arabic" in obj.lower():
        return True
    return any('\u0600' <= ch <= '\u06FF' for ch in obj)

def _looks_like_write(desc: str) -> bool:
    d = desc.lower()
    return any(k in d for k in WRITE_INTENT)

def _looks_like_read(desc: str) -> bool:
    d = desc.lower()
    return any(k in d for k in READ_INTENT)

def _looks_like_think(desc: str) -> bool:
    d = desc.lower()
    return any(k in d for k in THINK_INTENT)

def _canonical_task_id(idx: int) -> str:
    return f"t{idx:02d}"

def _risk_score(tool: str, desc: str, args: Dict[str, Any], deps: List[str]) -> float:
    score = 0.0
    if tool == CANON_WRITE:
        score += 2.5
        if isinstance(args.get("content"), str):
            score += min(len(args["content"]) / 1000.0, 3.5)
    if tool == TOOL_ENSURE:
        score += 1.0
    if tool == CANON_READ:
        score += 0.8
    if tool == CANON_THINK:
        score += 1.2
    score += len(deps) * 0.3
    return round(min(score, 10.0), 2)

def _extract_target_filename(objective: str) -> Optional[str]:
    low = objective.lower()
    # Pattern: summary file named X  OR file named X
    m = re.search(r"(?:summary\s+file\s+named|file\s+named)\s+([A-Za-z0-9_.\-]+)", low)
    if m:
        return m.group(1)
    # Look for explicit tokens
    for token in ("architecture_principles.md", "summary_report.md", "architecture.md"):
        if token in low:
            return token
    return None

def _autofill_file_args(tool_name: str, tool_args: Dict[str, Any], task_index: int, notes: List[str]):
    if tool_name not in MANDATORY_ARGS:
        return
    req = MANDATORY_ARGS[tool_name]
    changed = False
    if "path" in req and not tool_args.get("path"):
        filename = f"auto_generated_{task_index:02d}{FILE_DEFAULT_EXT}"
        tool_args["path"] = filename
        notes.append(f"autofill_path:{filename}")
        changed = True
    if "content" in req and tool_name == CANON_WRITE:
        if not isinstance(tool_args.get("content"), str) or not tool_args["content"].strip():
            tool_args["content"] = FILE_DEFAULT_CONTENT
            notes.append("autofill_content:default")
            changed = True
    if changed:
        notes.append("mandatory_args_filled")

def _canonicalize_tool_local(raw: str, description: str) -> Tuple[str, List[str]]:
    # Local fallback canonicalization
    name = _lower(raw)
    notes: List[str] = []
    if TOOL_ID_REGEX.match(name):
        notes.append("tool_id_unmapped")
        return name, notes
    if name in {"file_writer", "writer", "create_file"} or _looks_like_write(description) or "write" in name:
        notes.append("intent_write")
        return CANON_WRITE, notes
    if name in {"file_reader", "reader"} or _looks_like_read(description) or "read" in name:
        notes.append("intent_read")
        return CANON_READ, notes
    if _looks_like_think(description) or any(k in name for k in ("think", "analy", "reason")):
        notes.append("intent_think")
        return CANON_THINK, notes
    if name == "ensure_file":
        notes.append("ensure_pass")
        return TOOL_ENSURE, notes
    return raw, notes

def _canonicalize_tool(raw: str, description: str) -> Tuple[str, List[str]]:
    if CANON_FUNC:
        try:
            c, notes = CANON_FUNC(raw, description)  # type: ignore
            return c, notes
        except Exception:
            return _canonicalize_tool_local(raw, description)
    return _canonicalize_tool_local(raw, description)

# --------------------------------------------------------------------------------------
# Pattern Engine (Deterministic)
# --------------------------------------------------------------------------------------
class PatternResult:
    def __init__(self, tasks: List[PlannedTask], notes: List[str]):
        self.tasks = tasks
        self.notes = notes

class PatternEngine:
    def __init__(self, objective: str, max_tasks: int):
        self.objective = objective
        self.low = objective.lower()
        self.max_tasks = max_tasks

    def detect(self) -> Optional[PatternResult]:
        if not L4_ENABLED:
            return None
        # Architecture / summary style
        if ("analyze" in self.low or "analysis" in self.low or "architecture" in self.low) and \
           ("summary" in self.low or "report" in self.low or "file" in self.low or "principles" in self.low):
            return self._architecture_deep_scan()
        # Could add more pattern types here later
        return None

    def _architecture_deep_scan(self) -> PatternResult:
        """
        Build Level‑4 deep scan plan deterministically.
        """
        notes: List[str] = ["pattern:l4_architecture_deep_scan"]
        want_ar = _objective_has_arabic(self.objective)

        target_file = FORCE_REPORT_NAME or _extract_target_filename(self.low) or L4_FINAL_FILENAME_HINT
        # Ensure .md extension
        if not target_file.lower().endswith(".md"):
            target_file += ".md"

        tasks: List[PlannedTask] = []
        idx = 1

        # 1) list_dir tasks
        list_roots = L4_SCAN_DIRS[:L4_MAX_LIST_DIRS]
        list_ids: List[str] = []
        for root in list_roots:
            tid = _canonical_task_id(idx); idx += 1
            tasks.append(
                PlannedTask(
                    task_id=tid,
                    description=f"List directory '{root}' to discover candidate files.",
                    tool_name=CANON_LIST,
                    tool_args={"path": root, "max_entries": 400},
                    dependencies=[]
                )
            )
            list_ids.append(tid)
            if idx > self.max_tasks:
                notes.append("cap_hit_after_list_dir")
                return PatternResult(tasks, notes)

        # 2) Core file read tasks (ignore_missing gracefully)
        read_ids: List[str] = []
        for cf in L4_CORE_FILES:
            if len(read_ids) >= L4_MAX_TOTAL_READS:
                break
            tid = _canonical_task_id(idx); idx += 1
            tasks.append(
                PlannedTask(
                    task_id=tid,
                    description=f"Read core file {cf} if it exists.",
                    tool_name=CANON_READ,
                    tool_args={"path": cf, "ignore_missing": True, "max_bytes": 30000},
                    dependencies=[]
                )
            )
            read_ids.append(tid)
            if idx > self.max_tasks:
                notes.append("cap_hit_after_core_reads")
                return PatternResult(tasks, notes)

        # 3) Heuristic code sample file names (deterministic suggestion)
        # We cannot list dynamically here (list_dir content unknown yet), but we seed common paths
        heuristic_candidates = [
            "app/__init__.py",
            "app/config.py",
            "app/routes.py",
            "app/services/agent_tools.py",
            "app/services/master_agent_service.py",
            "app/overmind/planning/llm_planner.py",
            "app/models.py",
            "migrations/env.py",
        ]
        # Filter by extension allowlist
        filtered_candidates = []
        for c in heuristic_candidates:
            ext = "." + c.split(".")[-1].lower() if "." in c else ""
            if not ext or ext in L4_INCLUDE_CODE_EXTS:
                filtered_candidates.append(c)

        for c in filtered_candidates:
            if len(read_ids) >= L4_MAX_TOTAL_READS or len(read_ids) - len(L4_CORE_FILES) >= L4_MAX_CODE_FILES:
                break
            tid = _canonical_task_id(idx); idx += 1
            tasks.append(
                PlannedTask(
                    task_id=tid,
                    description=f"Read sample code file {c} (ignore if missing).",
                    tool_name=CANON_READ,
                    tool_args={"path": c, "ignore_missing": True, "max_bytes": 24000},
                    dependencies=[]
                )
            )
            read_ids.append(tid)
            if idx > self.max_tasks:
                notes.append("cap_hit_after_code_reads")
                return PatternResult(tasks, notes)

        # 4) Analytical generic_think tasks
        analysis_ids: List[str] = []
        if L4_MULTI_ANALYSIS:
            analyses_spec = [
                ("Identify major components, layers and their responsibilities.",
                 "components_and_layers"),
                ("Map data flow, persistence stores, and external integrations (APIs, DB, services).",
                 "data_flow"),
                ("Extract key dependencies (internal modules + external libraries) and coupling hotspots.",
                 "dependencies"),
                ("Assess risks, technical debt, scalability constraints, and security considerations.",
                 "risks"),
                ("List improvement opportunities and modernization recommendations.",
                 "recommendations")
            ]
            for text, tag in analyses_spec:
                tid = _canonical_task_id(idx); idx += 1
                prompt = self._build_analysis_prompt(text, read_ids, want_ar)
                tasks.append(
                    PlannedTask(
                        task_id=tid,
                        description=f"Analytical step: {tag}",
                        tool_name=CANON_THINK,
                        tool_args={"prompt": prompt},
                        dependencies=read_ids
                    )
                )
                analysis_ids.append(tid)
                if idx > self.max_tasks:
                    notes.append("cap_hit_during_analyses")
                    return PatternResult(tasks, notes)
        else:
            tid = _canonical_task_id(idx); idx += 1
            prompt = self._build_analysis_prompt(
                "Perform holistic architecture analysis (components, data flow, dependencies, risks, improvements).",
                read_ids,
                want_ar
            )
            tasks.append(
                PlannedTask(
                    task_id=tid,
                    description="Holistic architecture analysis.",
                    tool_name=CANON_THINK,
                    tool_args={"prompt": prompt},
                    dependencies=read_ids
                )
            )
            analysis_ids.append(tid)

        # 5) Synthesis generic_think
        synth_id = _canonical_task_id(idx); idx += 1
        synth_prompt = self._build_synthesis_prompt(analysis_ids, read_ids, target_file, want_ar)
        tasks.append(
            PlannedTask(
                task_id=synth_id,
                description="Synthesize final architecture report.",
                tool_name=CANON_THINK,
                tool_args={"prompt": synth_prompt},
                dependencies=analysis_ids
            )
        )

        # 6) ensure_file (optional)
        ensure_id: Optional[str] = None
        if L4_ENABLE_ENSURE and "ensure_file" in ALLOWED_TOOLS:
            ensure_id = _canonical_task_id(idx); idx += 1
            tasks.append(
                PlannedTask(
                    task_id=ensure_id,
                    description=f"Ensure target output file {target_file} exists or create placeholder.",
                    tool_name=TOOL_ENSURE,
                    tool_args={
                        "path": target_file,
                        "initial_content": "Initializing architecture report placeholder...",
                        "enforce_ext": ".md"
                    },
                    dependencies=[]
                )
            )

        # 7) write_file final
        write_id = _canonical_task_id(idx); idx += 1
        tasks.append(
            PlannedTask(
                task_id=write_id,
                description=f"Write final architecture report to {target_file}.",
                tool_name=CANON_WRITE,
                tool_args={
                    "path": target_file,
                    "content": f"{{{{{synth_id}.answer}}}}"
                },
                dependencies=[synth_id] if ensure_id is None else [synth_id]
            )
        )

        # Inject risk scores
        for t in tasks:
            if isinstance(t.tool_args, dict):
                t.tool_args["_meta_risk"] = _risk_score(t.tool_name, t.description, t.tool_args, t.dependencies)

        return PatternResult(tasks=tasks, notes=notes)

    def _build_analysis_prompt(self, focus: str, read_ids: List[str], want_ar: bool) -> str:
        lang_header = "حلّل بدقة" if want_ar else "Deeply analyze"
        markers = "\n".join(f"- {{${tid}.content}}" for tid in read_ids[:8])  # description markers (not actual)
        # We rely on interpolation engine; actual injection uses {{tNN.content}}
        ref_lines = "\n".join(f"{{{{{tid}.content}}}}" for tid in read_ids)
        if want_ar:
            return (
                f"{lang_header} جانب: {focus}\n"
                "استخدم المقتطفات التالية (قد تكون بعض الملفات ناقصة أو فارغة):\n"
                f"{ref_lines}\n"
                "أجب بنقاط منظمة واضحة.\n"
            )
        return (
            f"{lang_header} the following architectural aspect: {focus}\n"
            "Use the collected file excerpts (some may be empty / missing):\n"
            f"{ref_lines}\n"
            "Respond with structured bullet points.\n"
        )

    def _build_synthesis_prompt(self, analysis_ids: List[str], read_ids: List[str],
                                target_file: str, want_ar: bool) -> str:
        # Inject answers of analysis-level tasks
        analysis_refs = "\n".join(f"<<<ANALYSIS:{aid}>>>\n{{{{{aid}.answer}}}}" for aid in analysis_ids)
        raw_refs      = "\n".join(f"{{{{{rid}.content}}}}" for rid in read_ids[:6])  # subset for safety
        sec_count = L4_MIN_SYNTH_SECTIONS
        if want_ar:
            return (
                "اكتب تقريراً نهائياً احترافياً عن معمارية النظام بالمواصفات التالية:\n"
                f"- ملف الهدف: {target_file}\n"
                f"- أقسام (حوالي {sec_count} أو أكثر): مقدمة، نظرة عامة على الطبقات، المكونات والخدمات، "
                "تدفق البيانات، الاعتمادات الداخلية والخارجية، التوسع والأداء، المخاطر والأمن، التحسينات الموصى بها، "
                "خاتمة موجزة.\n"
                "- وضّح الافتراضات صراحة إذا كانت بعض المعلومات مفقودة.\n"
                "- استخدم لغة عربية فصيحة موجزة بلا حشو.\n"
                "- لا تكرر النص الحرفي بالكامل من الملفات؛ استخلص وفكك.\n"
                f"\n[تحليلات وسيطة]\n{analysis_refs}\n"
                f"\n[مقتطفات أولية]\n{raw_refs}\n"
            )
        return (
            "Produce a professional final ARCHITECTURE REPORT with these characteristics:\n"
            f"- Target file: {target_file}\n"
            f"- ~{sec_count}+ sections: Introduction, Layered Overview, Components & Services, "
            "Data Flow & Persistence, Internal & External Dependencies, Scalability & Performance, "
            "Risks & Security, Recommended Improvements, Executive Summary / Conclusion.\n"
            "- Explicitly state assumptions when source context missing.\n"
            "- Concise, factual, no marketing fluff, avoid verbatim copy – summarize & structure.\n"
            "- Use Markdown headings and tables if helpful.\n"
            f"\n[Intermediate Analyses]\n{analysis_refs}\n"
            f"\n[Raw Excerpts Subset]\n{raw_refs}\n"
        )

# --------------------------------------------------------------------------------------
# Planner Class
# --------------------------------------------------------------------------------------
class LLMGroundedPlanner(BasePlanner):
    name = "llm_grounded_planner_l4"
    version = "4.2.0-l4"
    tier = "core"
    production_ready = True
    capabilities = {"planning", "llm", "tool-grounding", "hybrid", "deep-scan"}
    tags = {"mission", "tasks", "level4"}

    @classmethod
    def self_test(cls) -> Tuple[bool, str]:
        # Light self-test
        if L4_ENABLED:
            return True, "l4_enabled"
        return True, "l4_disabled"

    # Public API
    def generate_plan(
        self,
        objective: str,
        context: Optional[PlanningContext] = None,
        max_tasks: Optional[int] = None
    ) -> MissionPlanSchema:
        start = time.perf_counter()
        if not self._objective_valid(objective):
            raise PlannerError("objective_invalid_or_short", self.name, objective)

        cap = min(max_tasks or MAX_TASKS_GLOBAL, MAX_TASKS_GLOBAL)
        _LOG.info("[%s] plan_start objective='%s' cap=%d",
                  self.name, _clip(objective, 140), cap)

        # 1. Deterministic Pattern (preferred path for architecture)
        pattern = PatternEngine(objective, cap).detect()
        if pattern:
            _LOG.info("[%s] pattern_match notes=%s", self.name, pattern.notes)
            tasks = self._finalize_pattern_tasks(pattern.tasks, objective)
            plan = MissionPlanSchema(objective=objective, tasks=tasks)
            self._post_validate(plan)
            self._log_success(plan, start, degraded=False, notes=pattern.notes)
            return plan

        # 2. If no pattern -> try structured LLM (optional / can be extended further)
        structured_plan = None
        errors: List[str] = []
        if maestro and hasattr(maestro, "generation_service"):
            try:
                structured_plan = self._call_structured(objective, context, cap)
            except Exception as e:
                errors.append(f"struct_fail:{type(e).__name__}")
                _LOG.warning("[%s] structured_failed %s", self.name, e)
        else:
            errors.append("maestro_unavailable")

        if STRICT_JSON_ONLY and structured_plan is None and not FALLBACK_ALLOW:
            _LOG.warning("[%s] strict_json_only->analytic_fallback", self.name)
            fallback_tasks = self._analytic_minimal(objective)
            plan = MissionPlanSchema(objective=objective, tasks=fallback_tasks)
            self._post_validate(plan)
            self._log_success(plan, start, degraded=True, notes=errors+["analytic_min"])
            return plan

        if structured_plan:
            norm_errs: List[str] = []
            try:
                tasks = self._normalize_tasks(structured_plan.get("tasks"), cap, norm_errs)
            except Exception as e:
                errors.append(f"normalize_fail:{type(e).__name__}")
                tasks = self._analytic_minimal(objective)
                degraded = True
            else:
                tasks = self._post_process_normalized(tasks, objective)
                degraded = False
            plan = MissionPlanSchema(objective=objective, tasks=tasks)
            self._post_validate(plan)
            self._log_success(plan, start, degraded=degraded, notes=errors+norm_errs)
            return plan

        # 3. (Optional) textual fallback (not focusing here) -> direct analytic minimal
        fallback = self._analytic_minimal(objective)
        plan = MissionPlanSchema(objective=objective, tasks=fallback)
        self._post_validate(plan)
        self._log_success(plan, start, degraded=True, notes=errors+["analytic_min_final"])
        return plan

    # ----------------------------------------------------------------------------------
    # Structured LLM path (skeleton – not central to Level 4 deterministic pattern)
    def _call_structured(self, objective: str, context: Optional[PlanningContext], max_tasks: int) -> Dict[str, Any]:
        if maestro is None or not hasattr(maestro, "generation_service"):
            raise RuntimeError("maestro_generation_service_unavailable")
        svc = maestro.generation_service  # type: ignore
        schema = {
            "type": "object",
            "properties": {
                "objective": {"type": "string"},
                "tasks": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "description": {"type": "string"},
                            "tool_name": {"type": "string"},
                            "tool_args": {"type": "object"},
                            "dependencies": {"type": "array", "items": {"type": "string"}},
                        },
                        "required": ["description", "tool_name"],
                    },
                },
            },
            "required": ["tasks"],
        }
        prompt = self._render_structured_prompt(objective, context, max_tasks)
        resp = svc.structured_json(
            system_prompt=(
                "You are a strict mission planner. Output ONLY JSON that conforms to the schema. "
                "Use ONLY allowed tool names. Provide required arguments."
            ),
            user_prompt=prompt,
            format_schema=schema,
            temperature=0.15,
            max_retries=1,
            fail_hard=False,
        )
        if not isinstance(resp, dict):
            raise PlannerError("structured_not_dict", self.name, objective)
        return resp

    def _render_structured_prompt(self, objective: str, context: Optional[PlanningContext], max_tasks: int) -> str:
        lines = [
            "OBJECTIVE:",
            objective,
            "",
            "ALLOWED_TOOLS:",
        ]
        for t in sorted(ALLOWED_TOOLS):
            lines.append(f"- {t}")
        lines += [
            "",
            "RULES:",
            "1. If objective implies final file deliverable produce generic_think then write_file.",
            "2. read_file only if needed; else skip.",
            "3. Provide concise descriptions.",
            "4. Use dependencies only when needed (t02 depends on t01).",
            "",
            f"Return <= {max_tasks} tasks."
        ]
        return "\n".join(lines)

    # ----------------------------------------------------------------------------------
    # Normalization (LLM path only)
    def _normalize_tasks(self, tasks_raw: Any, cap: int, errors_out: List[str]) -> List[PlannedTask]:
        if not isinstance(tasks_raw, list):
            raise PlanValidationError("tasks_not_list", self.name)

        cleaned: List[PlannedTask] = []
        for idx, raw in enumerate(tasks_raw):
            if len(cleaned) >= cap:
                errors_out.append("task_cap_reached")
                break
            if not isinstance(raw, dict):
                errors_out.append(f"task_{idx}_not_dict")
                continue
            desc = str(raw.get("description") or "").strip()
            if not desc:
                errors_out.append(f"task_{idx}_missing_desc")
                continue
            raw_tool = str(raw.get("tool_name") or "unknown").strip()
            tool_args = raw.get("tool_args")
            if not isinstance(tool_args, dict):
                tool_args = {}
                errors_out.append(f"task_{idx}_args_not_object")
            deps_raw = raw.get("dependencies") or []
            deps = [d for d in deps_raw if isinstance(d, str) and re.match(r"^t\d{2}$", d)]

            tool, cnotes = _canonicalize_tool(raw_tool, desc)
            # Force classification if requested
            if FORCE_FILE_TOOLS and tool not in ALLOWED_TOOLS:
                if _looks_like_write(desc):
                    tool = CANON_WRITE
                elif _looks_like_read(desc):
                    tool = CANON_READ
                elif _looks_like_think(desc):
                    tool = CANON_THINK

            if ENFORCE_ALLOWED and tool not in ALLOWED_TOOLS:
                tool = CANON_THINK

            if AUTO_FIX_FILE_TASKS:
                _autofill_file_args(tool, tool_args, idx + 1, cnotes)

            tid = _canonical_task_id(len(cleaned) + 1)
            tool_args["_meta_risk"] = _risk_score(tool, desc, tool_args, deps)
            cleaned.append(
                PlannedTask(
                    task_id=tid,
                    description=desc,
                    tool_name=tool,
                    tool_args=tool_args,
                    dependencies=deps
                )
            )

        if not cleaned:
            raise PlanValidationError("no_valid_tasks", self.name)

        return cleaned

    def _post_process_normalized(self, tasks: List[PlannedTask], objective: str) -> List[PlannedTask]:
        # Collapse multiple writes -> keep last meaningful
        write_indices = [i for i, t in enumerate(tasks) if t.tool_name == CANON_WRITE]
        if write_indices:
            if len(write_indices) > 1:
                last_idx = write_indices[-1]
                pruned: List[PlannedTask] = []
                kept_writes = 0
                for i, t in enumerate(tasks):
                    if t.tool_name == CANON_WRITE and i != last_idx:
                        content_val = (t.tool_args or {}).get("content")
                        if isinstance(content_val, str) and content_val.strip() == FILE_DEFAULT_CONTENT.strip():
                            continue
                        if kept_writes >= (MAX_WRITES - 1):
                            continue
                        kept_writes += 1
                        pruned.append(t)
                    else:
                        pruned.append(t)
                tasks = pruned
            # Normalize final write filename
            final_write = None
            for t in reversed(tasks):
                if t.tool_name == CANON_WRITE:
                    final_write = t
                    break
            if final_write:
                target = FORCE_REPORT_NAME or _extract_target_filename(objective) or L4_FINAL_FILENAME_HINT
                if not target.lower().endswith(FILE_DEFAULT_EXT):
                    target += FILE_DEFAULT_EXT
                final_write.tool_args.setdefault("path", target)
        return tasks

    # ----------------------------------------------------------------------------------
    # Pattern post-finalization
    def _finalize_pattern_tasks(self, tasks: List[PlannedTask], objective: str) -> List[PlannedTask]:
        # Any final patching (currently risk already injected)
        # Ensure task count limit
        if len(tasks) > MAX_TASKS_GLOBAL:
            tasks = tasks[:MAX_TASKS_GLOBAL]
        return tasks

    # ----------------------------------------------------------------------------------
    # Minimal Analytical Fallback
    def _analytic_minimal(self, objective: str) -> List[PlannedTask]:
        want_ar = _objective_has_arabic(objective)
        filename = FORCE_REPORT_NAME or _extract_target_filename(objective) or L4_FINAL_FILENAME_HINT
        if not filename.lower().endswith(".md"):
            filename += ".md"
        lang_line = "اكتب ملخصاً منظماً موجزاً." if want_ar else "Write a concise structured summary."
        prompt = (
            f"{lang_line}\nObjective:\n{objective}\n"
            "State assumptions where info is missing. Sections: Overview, Key Points, Recommendations."
        )
        return [
            PlannedTask(
                task_id="t01",
                description="Analytical summary draft.",
                tool_name=CANON_THINK,
                tool_args={"prompt": prompt, "_meta_risk": 1.0},
                dependencies=[]
            ),
            PlannedTask(
                task_id="t02",
                description="Write final summary file.",
                tool_name=CANON_WRITE,
                tool_args={"path": filename, "content": "{{t01.answer}}", "_meta_risk": 2.0},
                dependencies=["t01"]
            )
        ]

    # ----------------------------------------------------------------------------------
    # Validation
    def _post_validate(self, plan: MissionPlanSchema):
        # Simple cycle & size checks
        if len(plan.tasks) > MAX_TASKS_GLOBAL:
            raise PlanValidationError("exceed_global_max_tasks", self.name)
        ids = {t.task_id for t in plan.tasks}
        for t in plan.tasks:
            for d in t.dependencies:
                if d not in ids:
                    raise PlanValidationError(f"dangling_dependency:{t.task_id}->{d}", self.name)
        # Objective demands file but no write?
        low = plan.objective.lower()
        if any(k in low for k in ("summary", "report", "file")) and not any(t.tool_name == CANON_WRITE for t in plan.tasks):
            _LOG.warning("[%s] validation: objective implies file but no write_file produced", self.name)

    # ----------------------------------------------------------------------------------
    # Logging
    def _log_success(self, plan: MissionPlanSchema, start: float, degraded: bool, notes: Optional[List[str]] = None):
        elapsed_ms = (time.perf_counter() - start) * 1000.0
        _LOG.info("[%s] plan_success%s tasks=%d elapsed_ms=%.1f objective='%s'",
                  self.name,
                  "_degraded" if degraded else "",
                  len(plan.tasks),
                  elapsed_ms,
                  _clip(plan.objective, 100))
        if notes:
            _LOG.debug("[%s] notes_tail=%s", self.name, notes[-12:])

    # ----------------------------------------------------------------------------------
    # Objective Validator
    def _objective_valid(self, objective: str) -> bool:
        if not objective:
            return False
        x = objective.strip()
        if len(x) < 5:
            return False
        if x.isdigit():
            return False
        return True


# --------------------------------------------------------------------------------------
# Exports
# --------------------------------------------------------------------------------------
__all__ = [
    "LLMGroundedPlanner",
    "MissionPlanSchema",
    "PlannedTask",
    "PlanningContext",
    "PlannerError",
    "PlanValidationError",
]

# --------------------------------------------------------------------------------------
# Dev Quick Test
# --------------------------------------------------------------------------------------
if __name__ == "__main__":
    planner = LLMGroundedPlanner()
    tests = [
        "Analyze the current system architecture and create a summary file named ARCHITECTURE_PRINCIPLES.md",
        "Produce an architecture report (Arabic) analyzing components and data flow",
        "Simple objective summary",
    ]
    for obj in tests:
        print("\n=== OBJECTIVE:", obj)
        plan = planner.generate_plan(obj)
        print(f"Produced {len(plan.tasks)} tasks:")
        for t in plan.tasks:
            print(f"  {t.task_id} | {t.tool_name} | deps={t.dependencies} | args_keys={list((t.tool_args or {}).keys())}")