from __future__ import annotations

from typing import Any


import asyncio
import json
import logging
from collections.abc import AsyncGenerator, Callable

from sqlalchemy.ext.asyncio import AsyncSession

from app.core.ai_gateway import AIClient
from app.core.domain.models import AdminConversation, MessageRole
from app.services.admin.chat_persistence import AdminChatPersistence
from app.services.chat import get_chat_orchestrator

logger = logging.getLogger(__name__)

# Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¹Ø§Ù„Ù…ÙŠØ© Ù„Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ø®Ù„ÙÙŠØ© ÙˆÙ…Ù†Ø¹ Ø¬Ù…Ø¹ Ø§Ù„Ù‚Ù…Ø§Ù…Ø© (Garbage Collection)
# Ù‡Ø°Ø§ ÙŠØ¶Ù…Ù† Ø§Ø³ØªÙ…Ø±Ø§Ø± Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø­ÙØ¸ Ø­ØªÙ‰ Ø¨Ø¹Ø¯ Ø§Ù†ØªÙ‡Ø§Ø¡ Ø¯Ø§Ù„Ø© Ø§Ù„Ø¨Ø«
_background_tasks: set[asyncio.Task] = set()

class AdminChatStreamer:
    """
    Ø¨Ø« Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„ (Admin Chat Streamer).
    ---------------------------------------------------------
    Ù…Ø³Ø¤ÙˆÙ„ Ø¹Ù† Ø¥Ø¯Ø§Ø±Ø© ØªØ¯ÙÙ‚ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­ÙŠØ© (SSE) Ø¨ÙŠÙ† Ø§Ù„Ù†ÙˆØ§Ø© Ø§Ù„Ù…Ø±ÙƒØ²ÙŠØ© (Overmind)
    ÙˆÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„ØŒ Ù…Ø¹ Ø¶Ù…Ø§Ù† Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø§Ù„Ø© (Persistence) Ø¨Ø´ÙƒÙ„ ØºÙŠØ± Ù…ØªØ²Ø§Ù…Ù†.

    Ø§Ù„Ù…Ø¨Ø§Ø¯Ø¦ Ø§Ù„Ù…Ø¹Ù…Ø§Ø±ÙŠØ©:
    - **ÙØµÙ„ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…Ø§Øª (Separation of Concerns)**: Ù…Ù†Ø·Ù‚ Ø§Ù„Ø¨Ø« Ù…ÙØµÙˆÙ„ ØªÙ…Ø§Ù…Ø§Ù‹ Ø¹Ù† Ù…Ù†Ø·Ù‚ Ø§Ù„ØªØ®Ø²ÙŠÙ†.
    - **Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ© (Reliability)**: Ø§Ù†ØªØ¸Ø§Ø± Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„Ø­ÙØ¸ Ù„Ø¶Ù…Ø§Ù† Ø¹Ø¯Ù… Ø¶ÙŠØ§Ø¹ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„.
    - **Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„ÙÙˆØ±ÙŠØ© (Low Latency)**: Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡ (Chunks) ÙÙˆØ± ÙˆØµÙˆÙ„Ù‡Ø§.
    """

    def __init__(self, persistence: AdminChatPersistence) -> None:
        """
        ØªÙ‡ÙŠØ¦Ø© Ø¨Ø§Ø« Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©.

        Args:
            persistence: Ø®Ø¯Ù…Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø¯Ø§Ø¦Ù… Ù„Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª.
        """
        self.persistence = persistence

    async def stream_response(
        self,
        user_id: int,
        conversation: AdminConversation,
        question: str,
        history: list[dict[str, Any]],
        ai_client: AIClient,
        session_factory_func: Callable[[], AsyncSession],
    ) -> AsyncGenerator[str, None]:
        """
        ØªÙ†ÙÙŠØ° Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø¨Ø« Ø§Ù„Ø­ÙŠ Ù„Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©.
        Execute live streaming response operation.

        Yields:
            str: Ø£Ø­Ø¯Ø§Ø« SSE Ø¨ØªÙ†Ø³ÙŠÙ‚ `event: type\ndata: json\n\n`.
        """
        # 1. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø³ÙŠØ§Ù‚ ÙˆØ§Ù„ØªØ§Ø±ÙŠØ® | Prepare context and history
        self._inject_system_context_if_missing(history)
        self._update_history_with_question(history, question)

        # 2. Ø¥Ø±Ø³Ø§Ù„ Ø­Ø¯Ø« Ø§Ù„ØªÙ‡ÙŠØ¦Ø© | Send initialization event
        yield self._create_init_event(conversation)

        # 3. ØªÙ†ÙÙŠØ° Ø§Ù„Ø¨Ø« Ù…Ø¹ Ø§Ù„Ø­ÙØ¸ | Execute streaming with persistence
        try:
            orchestrator = get_chat_orchestrator()
            full_response: list[str] = []
            
            async for chunk in self._stream_with_safety_checks(
                orchestrator, question, user_id, conversation.id,
                ai_client, history, session_factory_func, full_response
            ):
                yield chunk

            # 4. Ø­ÙØ¸ ÙˆØ¥Ù†Ù‡Ø§Ø¡ | Persist and complete
            await self._persist_response(
                conversation.id, full_response, session_factory_func
            )
            yield "data: [DONE]\n\n"

        except Exception as e:
            logger.error(f"ğŸ”¥ Streaming error: {e}")
            yield self._create_error_event(str(e))

    def _inject_system_context_if_missing(self, history: list[dict[str, Any]]) -> None:
        """
        Ø­Ù‚Ù† Ø³ÙŠØ§Ù‚ Ø§Ù„Ù†Ø¸Ø§Ù… Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ÙÙ‚ÙˆØ¯Ø§Ù‹.
        Inject system context if missing from history.
        """
        has_system = any(msg.get("role") == "system" for msg in history)
        if not has_system:
            try:
                from app.services.chat.context_service import get_context_service
                ctx_service = get_context_service()
                system_prompt = ctx_service.get_context_system_prompt()
                history.insert(0, {"role": "system", "content": system_prompt})
            except Exception as e:
                logger.error(f"âš ï¸ Failed to inject Overmind context: {e}")

    def _update_history_with_question(
        self, history: list[dict[str, Any]], question: str
    ) -> None:
        """
        ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªØ§Ø±ÙŠØ® Ø¨Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø¬Ø¯ÙŠØ¯.
        Update history with new question.
        """
        if not history or history[-1]["content"] != question:
            history.append({"role": "user", "content": question})

    def _create_init_event(self, conversation: AdminConversation) -> str:
        """
        Ø¥Ù†Ø´Ø§Ø¡ Ø­Ø¯Ø« Ø§Ù„ØªÙ‡ÙŠØ¦Ø©.
        Create initialization event for frontend.
        """
        init_payload = {
            "conversation_id": conversation.id,
            "title": conversation.title,
        }
        return f"event: conversation_init\ndata: {json.dumps(init_payload)}\n\n"

    async def _stream_with_safety_checks(
        self,
        orchestrator,
        question: str,
        user_id: int,
        conversation_id: int,
        ai_client: AIClient,
        history: list[dict[str, Any]],
        session_factory_func,
        full_response: list[str],
    ) -> AsyncGenerator[str, None]:
        """
        Ø¨Ø« Ù…Ø¹ ÙØ­ÙˆØµØ§Øª Ø§Ù„Ø³Ù„Ø§Ù…Ø©.
        Stream with safety checks for size limits.
        """
        async for content_part in orchestrator.process(
            question=question,
            user_id=user_id,
            conversation_id=conversation_id,
            ai_client=ai_client,
            history_messages=history,
            session_factory=session_factory_func,
        ):
            if not content_part:
                continue

            full_response.append(content_part)

            # ÙØ­Øµ Ø§Ù„Ø­Ø¬Ù… Ø§Ù„Ø£Ù‚ØµÙ‰ | Check maximum size
            if self._exceeds_safety_limit(full_response):
                yield self._create_size_limit_error()
                break

            yield self._create_chunk_event(content_part)

    def _exceeds_safety_limit(self, response_parts: list[str]) -> bool:
        """
        Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØªØ¬Ø§ÙˆØ² Ø­Ø¯ Ø§Ù„Ø£Ù…Ø§Ù†.
        Check if response exceeds safety limit (100k chars).
        """
        current_size = sum(len(x) for x in response_parts)
        return current_size > 100000

    def _create_chunk_event(self, content: str) -> str:
        """
        Ø¥Ù†Ø´Ø§Ø¡ Ø­Ø¯Ø« Ø¬Ø²Ø¡ Ù…Ø­ØªÙˆÙ‰.
        Create content chunk event (OpenAI style).
        """
        chunk_data = {"choices": [{"delta": {"content": content}}]}
        return f"data: {json.dumps(chunk_data)}\n\n"

    def _create_size_limit_error(self) -> str:
        """
        Ø¥Ù†Ø´Ø§Ø¡ Ø­Ø¯Ø« Ø®Ø·Ø£ ØªØ¬Ø§ÙˆØ² Ø§Ù„Ø­Ø¬Ù….
        Create size limit exceeded error event.
        """
        error_payload = {
            "type": "error",
            "payload": {"error": "Response exceeded safety limit (100k chars). Aborting stream."}
        }
        return f"event: error\ndata: {json.dumps(error_payload)}\n\n"

    def _create_error_event(self, error_details: str) -> str:
        """
        Ø¥Ù†Ø´Ø§Ø¡ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø¹Ø§Ù….
        Create general error event.
        """
        error_payload = {"type": "error", "payload": {"details": error_details}}
        return f"event: error\ndata: {json.dumps(error_payload)}\n\n"

    async def _persist_response(
        self,
        conversation_id: int,
        response_parts: list[str],
        session_factory_func: Callable[[], AsyncSession],
    ) -> None:
        """
        Ø­ÙØ¸ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.
        Persist assistant response to database.
        """
        assistant_content = "".join(response_parts)
        if not assistant_content:
            assistant_content = "Error: No response received from AI service."

        try:
            async with session_factory_func() as session:
                p = AdminChatPersistence(session)
                await p.save_message(conversation_id, MessageRole.ASSISTANT, assistant_content)
            logger.info(f"âœ… Conversation {conversation_id} saved successfully.")
        except Exception as e:
            logger.error(f"âŒ Failed to save assistant message: {e}")
