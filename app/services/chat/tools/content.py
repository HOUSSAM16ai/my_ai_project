"""
Ø£Ø¯ÙˆØ§Øª Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ (Content Tools).

ØªØªÙŠØ­ Ù„Ù„ÙˆÙƒÙ„Ø§Ø¡:
1. Ø§Ø³ØªÙƒØ´Ø§Ù Ù‡ÙŠÙƒÙ„Ø© Ø§Ù„Ù…Ù†Ù‡Ø¬ (Subject -> Branch -> Topic).
2. Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ØªÙ…Ø§Ø±ÙŠÙ† ÙˆÙ…Ø­ØªÙˆÙ‰ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙÙ„Ø§ØªØ±.
3. Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø®Ø§Ù… (Raw Content) ÙˆØ§Ù„Ø­Ù„ÙˆÙ„ Ø§Ù„Ø±Ø³Ù…ÙŠØ©.
"""

import asyncio
import os

from app.core.logging import get_logger
from app.services.content.service import content_service
from app.services.search_engine import get_retriever
from app.services.search_engine.query_refiner import get_refined_query
from app.services.search_engine.fallback_expander import FallbackQueryExpander

logger = get_logger("content-tools")


async def get_curriculum_structure(
    level: str | None = None,
    lang: str = "ar",
) -> dict[str, object]:
    """Ø¬Ù„Ø¨ Ø´Ø¬Ø±Ø© Ø§Ù„Ù…Ù†Ù‡Ø¬ Ø§Ù„Ø¯Ø±Ø§Ø³ÙŠ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ Ø£Ùˆ Ù„Ù…Ø³ØªÙˆÙ‰ Ù…Ø­Ø¯Ø¯."""
    try:
        return await content_service.get_curriculum_structure(level)
    except Exception as e:
        logger.error(f"Failed to fetch curriculum structure: {e}")
        return {}


async def search_content(
    q: str | None = None,
    level: str | None = None,
    subject: str | None = None,
    branch: str | None = None,
    set_name: str | None = None,
    year: int | None = None,
    type: str | None = None,
    lang: str | None = None,
    limit: int = 10,
) -> list[dict[str, object]]:
    """Ø¨Ø­Ø« Ù…ØªÙ‚Ø¯Ù… Ø¹Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ Ù…Ø¹ ÙÙ„Ø§ØªØ± Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªÙˆØ³Ø¹."""
    try:
        refined_q = q
        api_key = os.environ.get("OPENROUTER_API_KEY")

        # 1. Try Smart Refinement (DSPy)
        if q and api_key:
            try:
                logger.info(f"ðŸ” DSPy Active: Refining query '{q}'...")
                refined_q = await asyncio.to_thread(get_refined_query, q, api_key)
                logger.info(f"âœ… DSPy Result: '{refined_q}'")
            except Exception as dspy_error:
                logger.warning(f"âš ï¸ DSPy refinement failed: {dspy_error}")

        # 2. Build Candidate List
        # Priority:
        # 1. Refined Query (usually English/Normalized)
        # 2. Expanded Variations of Original Query (Arabic Typos/Plurals)
        # 3. Original Query itself

        query_candidates = []
        if refined_q and refined_q != q:
            query_candidates.append(refined_q)

        # Always add fallback variations of the original query
        if q:
            variations = FallbackQueryExpander.generate_variations(q)
            for var in variations:
                if var not in query_candidates:
                    query_candidates.append(var)

        content_ids: list[str] = []

        # We try to find content using the candidates.
        # First, check if LlamaIndex works for the first candidate (usually the most specific/refined one)
        # If LlamaIndex is active, it uses vector search which is robust.

        primary_q = query_candidates[0] if query_candidates else q

        if primary_q:
            db_url = os.environ.get("DATABASE_URL")
            if db_url:
                try:
                    logger.info("ðŸ” LlamaIndex Active: Executing Vector Search...")
                    retriever = get_retriever(db_url)
                    # Try searching with the refined query first
                    nodes = retriever.search(primary_q)

                    # If no results and we have multiple candidates, maybe try the second one?
                    # For now, let's stick to primary for vector search to save time.

                    for node in nodes:
                        metadata = getattr(node, "node", node)
                        meta = getattr(metadata, "metadata", {})
                        if isinstance(meta, dict):
                            content_id = meta.get("content_id")
                            if isinstance(content_id, str):
                                content_ids.append(content_id)
                    logger.info(f"âœ… LlamaIndex Retrieval: Found {len(content_ids)} relevant vector IDs.")
                except Exception as e:
                    logger.warning(f"âš ï¸ LlamaIndex search failed, falling back to basic search: {e}")

        # Delegate to ContentService with Fallback Loop
        # We iterate through query candidates until we find results or run out.

        results = []
        unique_results = set()

        for candidate_q in query_candidates:
            logger.info(f"Searching content with query: '{candidate_q}'")

            # Attempt 1: Semantic Search (Vector IDs only)
            # If we have vector matches, we trust the semantic relevance over exact keyword matching.
            # We pass q=None so ContentService doesn't apply a strict 'LIKE' text filter,
            # allowing us to retrieve items that match meaning but not necessarily phrasing.
            if content_ids:
                logger.info(f"Applying vector filter with {len(content_ids)} IDs.")
                batch_results = await content_service.search_content(
                    q=None,
                    level=level,
                    subject=subject,
                    branch=branch,
                    set_name=set_name,
                    year=year,
                    type=type,
                    lang=lang,
                    content_ids=content_ids,
                    limit=limit,
                )
                if batch_results:
                    logger.info(f"Found {len(batch_results)} results via Semantic Search (Vector IDs).")
                    return batch_results
                else:
                    logger.info("Vector results filtered out by metadata. Falling back to Keyword Search.")
                    # If vector results didn't match the metadata (e.g. wrong year),
                    # we shouldn't retry them with other candidates.
                    content_ids = []

            # Attempt 2: Pure Keyword Search (Fallback)
            # This is where 'Ø¹Ù„ÙˆÙ… ØªØ¬Ø±Ø¨Ø©' -> 'Ø¹Ù„ÙˆÙ… ØªØ¬Ø±ÙŠØ¨ÙŠØ©' expansion shines.
            batch_results = await content_service.search_content(
                q=candidate_q,
                level=level,
                subject=subject,
                branch=branch,
                set_name=set_name,
                year=year,
                type=type,
                lang=lang,
                content_ids=None,
                limit=limit,
            )

            if batch_results:
                logger.info(f"Found {len(batch_results)} results with query '{candidate_q}' (Pure Keyword).")
                # We could return immediately or accumulate?
                # Returning immediately is safer to avoid duplicates and mixing unrelated stuff.
                return batch_results

        return results

    except Exception as e:
        logger.error(f"Search content failed: {e}")
        return []


async def get_content_raw(content_id: str) -> dict[str, str] | None:
    """Ø¬Ù„Ø¨ Ø§Ù„Ù†Øµ Ø§Ù„Ø®Ø§Ù… (Markdown) Ù„ØªÙ…Ø±ÙŠÙ† Ø£Ùˆ Ø¯Ø±Ø³ Ù…Ø¹ÙŠÙ†ØŒ Ù…Ø¹ Ø§Ù„Ø­Ù„ Ø¥Ø°Ø§ ØªÙˆÙØ±."""
    try:
        return await content_service.get_content_raw(content_id)
    except Exception as e:
        logger.error(f"Get content raw failed: {e}")
        return None


async def get_solution_raw(content_id: str) -> dict[str, object] | None:
    """Ø¬Ù„Ø¨ Ø§Ù„Ø­Ù„ Ø§Ù„Ø±Ø³Ù…ÙŠ (Official Solution) Ù„ØªÙ…Ø±ÙŠÙ†."""
    data = await content_service.get_content_raw(content_id)
    if data and "solution" in data:
        return {
            "solution_md": data["solution"],
        }
    return None
