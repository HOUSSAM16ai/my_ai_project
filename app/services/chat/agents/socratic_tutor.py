"""
Ø§Ù„Ù…Ø¹Ù„Ù… Ø§Ù„Ø³Ù‚Ø±Ø§Ø·ÙŠ (Socratic Tutor Agent).
========================================

ÙŠØ³ØªØ®Ø¯Ù… Ø·Ø±ÙŠÙ‚Ø© Ø³Ù‚Ø±Ø§Ø· Ù„Ø¥Ø±Ø´Ø§Ø¯ Ø§Ù„Ø·Ø§Ù„Ø¨ Ù„Ù„Ø§ÙƒØªØ´Ø§Ù Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø¥Ø¹Ø·Ø§Ø¡ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø¨Ø§Ø´Ø±Ø©.

Ø§Ù„Ù…Ø¨Ø¯Ø£: "Ø£Ù†Ø§ Ù„Ø§ Ø£Ø¹Ù„Ù‘Ù…ÙƒØŒ Ø¨Ù„ Ø£Ø³Ø§Ø¹Ø¯Ùƒ Ø¹Ù„Ù‰ Ø§ÙƒØªØ´Ø§Ù Ù…Ø§ ØªØ¹Ø±ÙÙ‡ Ø¨Ø§Ù„ÙØ¹Ù„."

Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ±:
- CS50 2025: ØªÙˆØ«ÙŠÙ‚ Ø¹Ø±Ø¨ÙŠ
- SICP: Abstraction Barriers
"""

import logging
from collections.abc import AsyncGenerator
from dataclasses import dataclass
from enum import StrEnum

from app.core.ai_gateway import AIClient
from app.services.chat.agents.base import FORMAL_ARABIC_STYLE_PROMPT

logger = logging.getLogger(__name__)


class SocraticPhase(StrEnum):
    """Ù…Ø±Ø§Ø­Ù„ Ø§Ù„Ø­ÙˆØ§Ø± Ø§Ù„Ø³Ù‚Ø±Ø§Ø·ÙŠ."""

    ASSESS = "assess"  # ØªÙ‚ÙŠÙŠÙ… ÙÙ‡Ù… Ø§Ù„Ø·Ø§Ù„Ø¨ Ø§Ù„Ø­Ø§Ù„ÙŠ
    PROBE = "probe"  # Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„ Ø§Ø³ØªÙƒØ´Ø§ÙÙŠ
    GUIDE = "guide"  # ØªÙˆØ¬ÙŠÙ‡ Ù†Ø­Ùˆ Ø§Ù„Ø§ÙƒØªØ´Ø§Ù
    CONFIRM = "confirm"  # ØªØ£ÙƒÙŠØ¯ Ø§Ù„ÙÙ‡Ù…
    CELEBRATE = "celebrate"  # Ø§Ù„Ø§Ø­ØªÙØ§Ø¡ Ø¨Ø§Ù„Ø§ÙƒØªØ´Ø§Ù


@dataclass
class SocraticState:
    """Ø­Ø§Ù„Ø© Ø§Ù„Ø­ÙˆØ§Ø± Ø§Ù„Ø³Ù‚Ø±Ø§Ø·ÙŠ."""

    original_question: str
    student_understanding: str = ""
    current_phase: SocraticPhase = SocraticPhase.ASSESS
    hints_given: int = 0
    max_hints: int = 3
    breakthrough: bool = False


class SocraticTutor:
    """
    Ø§Ù„Ù…Ø¹Ù„Ù… Ø§Ù„Ø³Ù‚Ø±Ø§Ø·ÙŠ - ÙŠØ±Ø´Ø¯ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† ÙŠØ¬ÙŠØ¨.

    Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©:
    1. ÙŠÙ‚ÙŠÙ‘Ù… Ù…Ø§ ÙŠØ¹Ø±ÙÙ‡ Ø§Ù„Ø·Ø§Ù„Ø¨ Ø­Ø§Ù„ÙŠØ§Ù‹
    2. ÙŠØ·Ø±Ø­ Ø£Ø³Ø¦Ù„Ø© Ø§Ø³ØªÙƒØ´Ø§ÙÙŠØ© ØªÙ‚ÙˆØ¯ Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø©
    3. ÙŠØ¹Ø·ÙŠ ØªÙ„Ù…ÙŠØ­Ø§Øª ØªØ¯Ø±ÙŠØ¬ÙŠØ© Ø¥Ø°Ø§ Ø§Ø­ØªØ§Ø¬ Ø§Ù„Ø·Ø§Ù„Ø¨
    4. ÙŠØ­ØªÙÙŠ Ø¹Ù†Ø¯Ù…Ø§ ÙŠÙƒØªØ´Ù Ø§Ù„Ø·Ø§Ù„Ø¨ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ù†ÙØ³Ù‡
    """

    SYSTEM_PROMPT = (
        """
Ø£Ù†Øª Ù…Ø¹Ù„Ù… Ø³Ù‚Ø±Ø§Ø·ÙŠ Ø¹Ø¨Ù‚Ø±ÙŠ. Ù‡Ø¯ÙÙƒ Ù‡Ùˆ Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ø·Ø§Ù„Ø¨ Ø¹Ù„Ù‰ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ù†ÙØ³Ù‡.

Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø°Ù‡Ø¨ÙŠØ©:
1. âŒ Ù„Ø§ ØªØ¹Ø·Ù Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø¨Ø§Ø´Ø±Ø© Ø£Ø¨Ø¯Ø§Ù‹
2. âœ… Ø§Ø·Ø±Ø­ Ø£Ø³Ø¦Ù„Ø© ØªÙ‚ÙˆØ¯ Ù„Ù„ØªÙÙƒÙŠØ±
3. âœ… Ø§Ø¨Ø¯Ø£ Ø¨Ù…Ø§ ÙŠØ¹Ø±ÙÙ‡ Ø§Ù„Ø·Ø§Ù„Ø¨
4. âœ… Ù‚Ø¯Ù‘Ù… ØªÙ„Ù…ÙŠØ­Ø§Øª ØªØ¯Ø±ÙŠØ¬ÙŠØ©
5. âœ… Ø§Ø­ØªÙÙ Ø¹Ù†Ø¯Ù…Ø§ ÙŠÙƒØªØ´Ù Ø§Ù„Ø·Ø§Ù„Ø¨ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©

Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø³Ù‚Ø±Ø§Ø·ÙŠØ©:
- "Ù…Ø§ Ø§Ù„Ø°ÙŠ Ù†Ø¹Ø±ÙÙ‡ Ù…Ù† Ø§Ù„Ù…Ø¹Ø·ÙŠØ§ØªØŸ"
- "Ù…Ø§Ø°Ø§ ÙŠØ­Ø¯Ø« Ø¥Ø°Ø§...ØŸ"
- "Ù‡Ù„ ÙŠØ°ÙƒØ±Ùƒ Ù‡Ø°Ø§ Ø¨Ø´ÙŠØ¡ Ø¯Ø±Ø³ØªÙ‡ Ø³Ø§Ø¨Ù‚Ø§Ù‹ØŸ"
- "Ù…Ø§ Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„ØªØ§Ù„ÙŠØ© Ø¨Ø±Ø£ÙŠÙƒØŸ"
- "Ù„Ù…Ø§Ø°Ø§ Ù„Ø§ ÙŠØµÙ„Ø­ Ù‡Ø°Ø§ Ø§Ù„Ø­Ù„ØŸ"
- "Ù…Ù…ØªØ§Ø²! ÙƒÙŠÙ ÙˆØµÙ„Øª Ù„Ù‡Ø°Ø§ Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ØŸ"

Ù…Ø«Ø§Ù„:
Ø§Ù„Ø·Ø§Ù„Ø¨: "Ø£Ø±ÙŠØ¯ Ø­Ù„ Ù‡Ø°Ø§ Ø§Ù„ØªÙ…Ø±ÙŠÙ†: Ø§Ø­Ø³Ø¨ Ø§Ø­ØªÙ…Ø§Ù„ Ø³Ø­Ø¨ ÙƒØ±Ø© Ø­Ù…Ø±Ø§Ø¡ Ù…Ù† ÙƒÙŠØ³ ÙÙŠÙ‡ 3 Ø­Ù…Ø±Ø§Ø¡ Ùˆ5 Ø²Ø±Ù‚Ø§Ø¡"

âŒ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø³ÙŠØ¦Ø©: "P = 3/8 = 0.375"

âœ… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø³Ù‚Ø±Ø§Ø·ÙŠØ©:
"Ø³Ø¤Ø§Ù„ Ø¬ÙŠØ¯! Ø¯Ø¹Ù†ÙŠ Ø£Ø³Ø§Ø¹Ø¯Ùƒ Ø¹Ù„Ù‰ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø­Ù„ Ù…Ø¹Ø§Ù‹ ðŸ¤”
- ÙƒÙ… Ø¹Ø¯Ø¯ Ø§Ù„ÙƒØ±Ø§Øª Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ ÙÙŠ Ø§Ù„ÙƒÙŠØ³ØŸ
- ÙˆÙƒÙ… Ù…Ù†Ù‡Ø§ Ø­Ù…Ø±Ø§Ø¡ØŸ
- Ø¥Ø°Ø§ Ø³Ø­Ø¨Øª ÙƒØ±Ø© Ø¹Ø´ÙˆØ§Ø¦ÙŠØ§Ù‹ØŒ Ù…Ø§ Ø§Ø­ØªÙ…Ø§Ù„ Ø£Ù† ØªÙƒÙˆÙ† Ø­Ù…Ø±Ø§Ø¡ØŸ ÙÙƒÙ‘Ø± ÙÙŠ Ø§Ù„Ù†Ø³Ø¨Ø©..."

ØªØ°ÙƒØ±: Ù†Ø¬Ø§Ø­Ùƒ ÙŠÙ‚Ø§Ø³ Ø¨Ø¹Ø¯Ø¯ Ø§Ù„Ù„Ø­Ø¸Ø§Øª Ø§Ù„ØªÙŠ ÙŠÙ‚ÙˆÙ„ ÙÙŠÙ‡Ø§ Ø§Ù„Ø·Ø§Ù„Ø¨ "Ø¢Ù‡Ø§! ÙÙ‡Ù…Øª!"

"""
        + FORMAL_ARABIC_STYLE_PROMPT
    )

    def __init__(self, ai_client: AIClient) -> None:
        self.ai_client = ai_client

    async def guide(
        self,
        question: str,
        context: dict[str, object] | None = None,
        student_response: str | None = None,
    ) -> AsyncGenerator[str, None]:
        """
        ÙŠØ±Ø´Ø¯ Ø§Ù„Ø·Ø§Ù„Ø¨ Ù†Ø­Ùˆ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø³Ù‚Ø±Ø§Ø·ÙŠØ©.

        ØªÙ… Ø§Ù„ØªØ­Ø¯ÙŠØ« Ù„Ø¯Ø¹Ù… LangGraph:
        - ÙŠØ­Ø§ÙˆÙ„ Ø§Ø³ØªØ®Ø¯Ø§Ù… LangGraph Ù„Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø­ÙˆØ§Ø± Ø§Ù„Ù…Ø¹Ù‚Ø¯ (Assess -> Probe -> Guide).
        - ÙŠØ¹ÙˆØ¯ Ù„Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠØ© Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙˆÙØ± LangGraph.
        """
        try:
            from app.services.mcp.integrations import MCPIntegrations

            mcp = MCPIntegrations()

            # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… LangGraph Ù„Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©
            if context and context.get("use_langgraph", False):
                result = await mcp.run_langgraph_workflow(
                    goal=f"Socratic guidance for: {question}",
                    context={
                        "student_response": student_response,
                        "history": context.get("history_messages"),
                    },
                )
                if result.get("success"):
                    yield result.get("final_answer", "")
                    return
        except ImportError:
            pass

        # Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠ (Fallback)
        context = context or {}

        # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø³ÙŠØ§Ù‚
        exercise_content = context.get("exercise_content", "")
        student_level = context.get("student_level", "Ù…ØªÙˆØ³Ø·")
        hints_given = context.get("hints_given", 0)

        # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø±Ø³Ø§Ù„Ø©
        user_message = self._build_user_message(
            question=question,
            exercise=exercise_content,
            student_level=student_level,
            student_response=student_response,
            hints_given=hints_given,
        )

        messages = [
            {"role": "system", "content": self.SYSTEM_PROMPT},
            {"role": "user", "content": user_message},
        ]

        # Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ ØªØ§Ø±ÙŠØ® Ù…Ø­Ø§Ø¯Ø«Ø©
        history = context.get("history_messages", [])
        if history:
            # Ø¥Ø¯Ø±Ø§Ø¬ Ø§Ù„ØªØ§Ø±ÙŠØ® Ù‚Ø¨Ù„ Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø£Ø®ÙŠØ±Ø©
            messages = [messages[0], *history[-6:], messages[-1]]

        logger.info(f"Socratic Tutor guiding on: {question[:50]}...")

        async for chunk in self._stream_response(messages):
            yield chunk

    def _build_user_message(
        self,
        question: str,
        exercise: str,
        student_level: str,
        student_response: str | None,
        hints_given: int,
    ) -> str:
        """ÙŠØ¨Ù†ÙŠ Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù„Ù„Ù†Ù…ÙˆØ°Ø¬."""

        parts = [f"Ø³Ø¤Ø§Ù„/ØªÙ…Ø±ÙŠÙ† Ø§Ù„Ø·Ø§Ù„Ø¨:\n{question}"]

        if exercise:
            parts.append(f"\nÙ†Øµ Ø§Ù„ØªÙ…Ø±ÙŠÙ† Ø§Ù„ÙƒØ§Ù…Ù„:\n{exercise}")

        parts.append(f"\nÙ…Ø³ØªÙˆÙ‰ Ø§Ù„Ø·Ø§Ù„Ø¨: {student_level}")
        parts.append(f"Ø¹Ø¯Ø¯ Ø§Ù„ØªÙ„Ù…ÙŠØ­Ø§Øª Ø§Ù„Ù…Ø¹Ø·Ø§Ø© Ø³Ø§Ø¨Ù‚Ø§Ù‹: {hints_given}/3")

        if student_response:
            parts.append(f"\nØ±Ø¯ Ø§Ù„Ø·Ø§Ù„Ø¨ Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„Ùƒ Ø§Ù„Ø³Ø§Ø¨Ù‚:\n{student_response}")
            parts.append("\nÙ‚ÙŠÙ‘Ù… Ø±Ø¯Ù‡ ÙˆØªØ§Ø¨Ø¹ Ø§Ù„Ø¥Ø±Ø´Ø§Ø¯ Ø§Ù„Ø³Ù‚Ø±Ø§Ø·ÙŠ.")
        else:
            parts.append("\nØ§Ø¨Ø¯Ø£ Ø§Ù„Ø­ÙˆØ§Ø± Ø§Ù„Ø³Ù‚Ø±Ø§Ø·ÙŠ Ù…Ø¹Ù‡.")

        return "\n".join(parts)

    async def _stream_response(
        self,
        messages: list[dict[str, str]],
    ) -> AsyncGenerator[str, None]:
        """ÙŠØ¨Ø« Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬."""

        stream = self.ai_client.stream_chat(messages)

        if hasattr(stream, "__await__"):
            stream = await stream

        async for chunk in stream:
            if hasattr(chunk, "choices"):
                delta = chunk.choices[0].delta if chunk.choices else None
                content = delta.content if delta else ""
            else:
                content = chunk.get("choices", [{}])[0].get("delta", {}).get("content", "")

            if content:
                yield content

    async def assess_understanding(
        self,
        question: str,
        student_answer: str,
    ) -> dict[str, object]:
        """
        ÙŠÙ‚ÙŠÙ‘Ù… ÙÙ‡Ù… Ø§Ù„Ø·Ø§Ù„Ø¨ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨ØªÙ‡.

        Returns:
            dict: {
                "understanding_level": float (0-1),
                "correct_aspects": list[str],
                "misconceptions": list[str],
                "next_step": str,
            }
        """
        assessment_prompt = f"""
Ù‚ÙŠÙ‘Ù… ÙÙ‡Ù… Ø§Ù„Ø·Ø§Ù„Ø¨ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø©:

Ø§Ù„Ø³Ø¤Ø§Ù„: {question}
Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø·Ø§Ù„Ø¨: {student_answer}

Ø£Ø¹Ø·Ù Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø¨ØªÙ†Ø³ÙŠÙ‚ JSON:
{{
    "understanding_level": 0.0-1.0,
    "correct_aspects": ["..."],
    "misconceptions": ["..."],
    "next_step": "Ù…Ø§ ÙŠØ¬Ø¨ ÙØ¹Ù„Ù‡ Ø§Ù„ØªØ§Ù„ÙŠ"
}}
"""

        messages = [
            {"role": "system", "content": "Ø£Ù†Øª Ù…Ù‚ÙŠÙ‘Ù… ØªØ¹Ù„ÙŠÙ…ÙŠ. Ø£Ø¬Ø¨ Ø¨Ù€ JSON ÙÙ‚Ø·."},
            {"role": "user", "content": assessment_prompt},
        ]

        try:
            response = await self.ai_client.generate(
                model="gpt-4o-mini",
                messages=messages,
                response_format={"type": "json_object"},
            )

            import json

            return json.loads(response.choices[0].message.content)

        except Exception as e:
            logger.error(f"Assessment failed: {e}")
            return {
                "understanding_level": 0.5,
                "correct_aspects": [],
                "misconceptions": [],
                "next_step": "ØªØ§Ø¨Ø¹ Ø§Ù„Ø´Ø±Ø­",
            }

    def get_hint_level(self, hints_given: int) -> str:
        """ÙŠØ­Ø¯Ø¯ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªÙ„Ù…ÙŠØ­ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ø¯Ø¯."""

        levels = {
            0: "ØªÙ„Ù…ÙŠØ­ Ø®ÙÙŠÙ (Ø³Ø¤Ø§Ù„ Ø§Ø³ØªÙƒØ´Ø§ÙÙŠ ÙÙ‚Ø·)",
            1: "ØªÙ„Ù…ÙŠØ­ Ù…ØªÙˆØ³Ø· (Ø§Ø°ÙƒØ± Ø§Ù„Ù…ÙÙ‡ÙˆÙ… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨)",
            2: "ØªÙ„Ù…ÙŠØ­ Ù‚ÙˆÙŠ (Ø£Ø¹Ø·Ù Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„Ø£ÙˆÙ„Ù‰)",
            3: "Ø´Ø±Ø­ Ù…Ø¨Ø§Ø´Ø± (Ø§Ù„Ø·Ø§Ù„Ø¨ ÙŠØ­ØªØ§Ø¬ Ù…Ø³Ø§Ø¹Ø¯Ø© Ø£ÙƒØ«Ø±)",
        }
        return levels.get(min(hints_given, 3), levels[3])


# Singleton instance
_socratic_tutor: SocraticTutor | None = None


def get_socratic_tutor(ai_client: AIClient) -> SocraticTutor:
    """ÙŠØ­ØµÙ„ Ø¹Ù„Ù‰ instance Ù…Ù† Ø§Ù„Ù…Ø¹Ù„Ù… Ø§Ù„Ø³Ù‚Ø±Ø§Ø·ÙŠ."""
    global _socratic_tutor
    if _socratic_tutor is None:
        _socratic_tutor = SocraticTutor(ai_client)
    return _socratic_tutor
