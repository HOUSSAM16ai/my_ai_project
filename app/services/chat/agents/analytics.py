"""
ูููู ุชุญููู ุงูุฃุฏุงุก (Analytics Agent) - ุงููุณุฎุฉ ุงูุฎุงุฑูุฉ (Superhuman Edition).

ูุณุชุฎุฏู ุงูุฐูุงุก ุงูุงุตุทูุงุนู ูุชุญููู ุณุฌูุงุช ุงูุฏุฑุฏุดุฉ ูุงูููุงู ุจุฏูุฉ ูุชูุงููุฉ.
"""

from collections.abc import AsyncGenerator

from app.core.ai_gateway import AIClient
from app.core.logging import get_logger
from app.services.chat.intent_detector import ChatIntent
from app.services.chat.tools import ToolRegistry

logger = get_logger("analytics-agent")


class AnalyticsAgent:
    """
    ูููู ูุชุฎุตุต ูู ุชุญููู ุจูุงูุงุช ุงูุทูุงุจ ุงูุชุนููููุฉ ูุชูุฏูู ุชูุงุฑูุฑ ุชุดุฎูุตูุฉ "ุนุจูุฑูุฉ".
    """

    def __init__(self, tools: ToolRegistry, ai_client: AIClient | None = None) -> None:
        self.tools = tools
        self.ai_client = ai_client

    async def process(self, context: dict[str, object]) -> AsyncGenerator[str, None]:
        """
        ุชูููุฐ ุนูููุฉ ุงูุชุญููู ุงูุนููู ุจุงุณุชุฎุฏุงู AI Client ูุจุงุดุฑุฉ.
        """
        logger.info("Analytics agent started processing (Superhuman Mode)")

        user_id = context.get("user_id")
        if not user_id:
            yield "ุนุฐุฑุงูุ ูู ุฃุชููู ูู ุชุญุฏูุฏ ูููุฉ ุงููุณุชุฎุฏู ูุชุญููู ุจูุงูุงุชู."
            return

        if not self.ai_client:
            yield "โ๏ธ ุฎุทุฃ ุฏุงุฎูู: ูู ูุชู ุชุฒููุฏ ุงููููู ุจูุญุฑู ุงูุฐูุงุก ุงูุงุตุทูุงุนู."
            return

        yield "๐ **ุฌุงุฑู ุงุณุชุฏุนุงุก ุณุฌูุงุชู ุงูุฏุฑุงุณูุฉ ูุชุญููู ูุญุงุฏุซุงุชู ุงูุณุงุจูุฉ ุจุงููุงูู...**\n"

        # 1. Fetch Comprehensive Data (Chat Logs + Missions)
        try:
            data = await self.tools.execute(
                "fetch_comprehensive_student_history", {"user_id": user_id}
            )
        except Exception as e:
            logger.error(f"Error fetching comprehensive history: {e}")
            yield "ุญุฏุซ ุฎุทุฃ ุฃุซูุงุก ุฌูุจ ุงูุจูุงูุงุช."
            return

        # 2. Construct the Superhuman Prompt
        chat_logs = str(data.get("chat_history_text", "No logs."))
        missions = data.get("missions_summary", {})
        stats = data.get("profile_stats", {})
        intent = context.get("intent")

        system_prompt, user_prompt = self._build_analysis_prompts(
            chat_logs=chat_logs,
            missions=missions,
            stats=stats,
            intent=intent,
        )

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        # 3. Stream the AI Analysis
        yield "\n"  # Spacing

        try:
            async for chunk in self.ai_client.stream_chat(messages):
                # Extract content depending on client wrapper structure
                content = ""
                if isinstance(chunk, dict):
                    # OpenAI-like format
                    choices = chunk.get("choices", [])
                    if choices:
                        delta = choices[0].get("delta", {})
                        content = delta.get("content", "")
                elif hasattr(chunk, "choices"):
                    # Object format
                    if chunk.choices:
                        content = chunk.choices[0].delta.content or ""

                if content:
                    yield content

        except Exception as exc:
            logger.error(f"AI Analysis Failed: {exc}")
            yield "\nโ๏ธ ุญุฏุซ ุฎุทุฃ ุฃุซูุงุก ุชูููุฏ ุงูุชุญููู ุงูุฐูู. ูุฑุฌู ุงููุญุงููุฉ ูุงุญูุงู."

    def _build_analysis_prompts(
        self,
        *,
        chat_logs: str,
        missions: dict[str, object],
        stats: dict[str, object],
        intent: object,
    ) -> tuple[str, str]:
        """
        ุจูุงุก ุชูุฌููุงุช ุชุญููููุฉ ูุชูุฏูุฉ ุญุณุจ ููุฉ ุงูุทุงูุจ.

        Args:
            chat_logs: ุณุฌูุงุช ุงููุญุงุฏุซุงุช ุงููุตูุฉ ุงููุฎุชุตุฑุฉ.
            missions: ููุฎุต ุงูููุงู ูุงูุฅูุฌุงุฒุงุช ุงูุชุนููููุฉ.
            stats: ุฅุญุตุงุกุงุช ุงูุฃุฏุงุก ุงูุนุงูุฉ.
            intent: ููุฉ ุงููุญุงุฏุซุฉ ุงูุญุงููุฉ (ุฅู ููุฌุฏุช).

        Returns:
            tuple: (system_prompt, user_prompt) ูุชุบุฐูุฉ ูููุฐุฌ ุงูุชุญููู.
        """
        base_data = (
            "ุจูุงูุงุช ุงูุทุงูุจ:\n"
            f"1) ุณุฌูุงุช ุงููุญุงุฏุซุฉ (ุขุฎุฑ ~60 ุฑุณุงูุฉ):\n{chat_logs}\n\n"
            f"2) ุณุฌู ุงูููุงู ุงูุชุนููููุฉ:\n{missions}\n\n"
            f"3) ุฅุญุตุงุกุงุช ุงูุฃุฏุงุก:\n{stats}\n"
        )

        if intent == ChatIntent.LEARNING_SUMMARY:
            system_prompt = (
                "ุฃูุช Overmindุ ูุญูู ุชุนูููู ูุงุฆู ุงูุชุทูุฑ.\n"
                "ูููุชู: ุชูุฏูู ููุฎุต ุนููู ููููู ููู ูุง ุชุนููู ุงูุทุงูุจ ุญุชู ุงูุขูุ "
                "ูุน ุชุญููู ูุนุฑูู ูุณูููู ููุถุญ ุชุทูุฑ ุงููููุ ุงูุฎุฑุงุฆุท ุงูููุงููููุฉุ "
                "ูููุงุท ุงูุชุญูู ูู ุทุฑููุฉ ุงูุชูููุฑ.\n\n"
                "ูุชุทูุจุงุช ุงูุฅุฎุฑุงุฌ:\n"
                "- ูุบุฉ ุนุฑุจูุฉ ุงุญุชุฑุงููุฉ ุฏูููุฉ.\n"
                "- ุชุญููู ูุชุนุฏุฏ ุงูุทุจูุงุช (ููุงูููุ ููุงุฑุงุชุ ุฃููุงุท ุชูููุฑุ ุชุทุจููุงุช).\n"
                "- ุฅุจุฑุงุฒ ูุง ุชู ุงูุชุณุงุจู ููุง ูุฒุงู ูุญุชุงุฌ ุชุซุจูุชุงู.\n"
                "- ุงูุชุฑุงุญ 3 ุฎุทูุงุช ูุชูุฏูุฉ ูุจููุฉ ุนูู ุงูุฏููู.\n"
                "- ุงุณุชุฎุฏู Markdown ุจุนูุงููู ูุงุถุญุฉ ูููุงุฆู ููุธูุฉ.\n\n"
                f"{base_data}"
            )
            user_prompt = "ูุฏูู ููุฎุตุงู ุชุญููููุงู ุนูููุงู ููู ูุง ุชุนููุชู ุญุชู ุงูุขู."
            return system_prompt, user_prompt

        system_prompt = (
            "ุฃูุช Overmindุ ุงููุฑุดุฏ ุงูุฃูุงุฏููู ุงูุนุจูุฑู.\n"
            "ูุฏูู ุชุญููู ุชุงุฑูุฎ ุชูุงุนู ุงูุทุงูุจ ุจุงููุงูู ูุชูุฏูู ุชุดุฎูุต ูุนุฑูู ูุฃูุงุฏููู ุนููู.\n"
            "ูุง ุชูุชูู ุจุนุฑุถ ุงูุฃุฑูุงูุ ุจู ุญููู ุงููุญุชูู ูุงูุฃุณุฆูุฉ ูุงูุณูุงู.\n\n"
            "ูุชุทูุจุงุช ุงูุฅุฎุฑุงุฌ:\n"
            "- ูุจุฑุฉ ุงุญุชุฑุงููุฉ ูุดุฌุนุฉ ูุจุตูุฑุฉ ุนุงููุฉ.\n"
            "- ุชุญููู ุทุฑููุฉ ุงูุชูููุฑ ูููุท ุงูุงุณุชูุนุงุจ.\n"
            "- ููุงุกูุฉ ุงูุชูุฏู ูุน ุฎุงุฑุทุฉ ูููุฌ ููุงุณูุฉ.\n"
            "- ุชุญุฏูุฏ ููุงุท ุงูุถุนู ุงูููุงููููุฉ ุงูุญููููุฉ.\n"
            "- ุฎุทุฉ ุนูู ุจุซูุงุซ ุฎุทูุงุช ูุญุฏุฏุฉ.\n"
            "- ุชูุณูู Markdown ุจุนูุงููู ูุฑููุฒ ุชุนุจูุฑูุฉ.\n\n"
            f"{base_data}"
        )
        user_prompt = "ุญูู ุฃุฏุงุฆู ุงูุฏุฑุงุณู ุจูุงุกู ุนูู ูู ูุง ุชุนุฑูู ุนูู."
        return system_prompt, user_prompt
