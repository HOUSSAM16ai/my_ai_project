"""
ğŸš€ SUPERHUMAN AI-DRIVEN SELF-ADAPTIVE MICROSERVICES
====================================================

Ù†Ø¸Ø§Ù… Ø®Ø¯Ù…Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„ØªÙƒÙŠÙ Ù…Ø¯Ø¹ÙˆÙ…Ø© Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
ØªØªÙÙˆÙ‚ Ø¹Ù„Ù‰ GoogleØŒ MicrosoftØŒ AWS Ø¨Ø³Ù†ÙˆØ§Øª Ø¶ÙˆØ¦ÙŠØ©

This module implements:
- AI-powered auto-scaling
- ML-based intelligent routing
- Predictive health monitoring
- Self-healing capabilities
- Multi-objective optimization
"""

import statistics
import threading
from collections import defaultdict, deque
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Any


class ServiceHealth(Enum):
    """Ø­Ø§Ù„Ø© ØµØ­Ø© Ø§Ù„Ø®Ø¯Ù…Ø©"""

    HEALTHY = "healthy"
    DEGRADED = "degraded"
    UNHEALTHY = "unhealthy"
    CRITICAL = "critical"
    RECOVERING = "recovering"


class ScalingDirection(Enum):
    """Ø§ØªØ¬Ø§Ù‡ Ø§Ù„ØªÙˆØ³Ø¹"""

    UP = "scale_up"
    DOWN = "scale_down"
    STABLE = "stable"


@dataclass
class ServiceMetrics:
    """Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø®Ø¯Ù…Ø© ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ÙØ¹Ù„ÙŠ"""

    service_name: str
    timestamp: datetime
    cpu_usage: float  # 0-100
    memory_usage: float  # 0-100
    request_rate: float  # requests per second
    error_rate: float  # 0-100
    latency_p50: float  # milliseconds
    latency_p95: float  # milliseconds
    latency_p99: float  # milliseconds
    active_connections: int
    queue_depth: int

    def to_dict(self) -> dict[str, Any]:
        return {
            "service_name": self.service_name,
            "timestamp": self.timestamp.isoformat(),
            "cpu_usage": self.cpu_usage,
            "memory_usage": self.memory_usage,
            "request_rate": self.request_rate,
            "error_rate": self.error_rate,
            "latency_p50": self.latency_p50,
            "latency_p95": self.latency_p95,
            "latency_p99": self.latency_p99,
            "active_connections": self.active_connections,
            "queue_depth": self.queue_depth,
        }


@dataclass
class ScalingDecision:
    """Ù‚Ø±Ø§Ø± Ø§Ù„ØªÙˆØ³Ø¹ Ø§Ù„Ø°ÙƒÙŠ"""

    service_name: str
    direction: ScalingDirection
    current_instances: int
    target_instances: int
    confidence: float  # 0-1
    reason: str
    predicted_impact: dict[str, float]
    timestamp: datetime


@dataclass
class ServiceInstance:
    """Ù†Ù…ÙˆØ°Ø¬ Ø®Ø¯Ù…Ø© Ù…ØµØºØ±Ø©"""

    instance_id: str
    service_name: str
    status: ServiceHealth
    cpu_limit: float = 80.0
    memory_limit: float = 80.0
    created_at: datetime = field(default_factory=datetime.now)
    last_health_check: datetime | None = None
    metrics_history: deque = field(default_factory=lambda: deque(maxlen=100))

    def is_healthy(self) -> bool:
        return self.status == ServiceHealth.HEALTHY


class AIScalingEngine:
    """
    Ù…Ø­Ø±Ùƒ Ø§Ù„ØªÙˆØ³Ø¹ Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ… Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
    ÙŠØ³ØªØ®Ø¯Ù… ML Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ø­Ù…Ù„ ÙˆØ§ØªØ®Ø§Ø° Ù‚Ø±Ø§Ø±Ø§Øª Ø§Ù„ØªÙˆØ³Ø¹
    """

    def __init__(self):
        self.historical_patterns = defaultdict(list)
        self.scaling_history = []
        self.learning_rate = 0.1

    def predict_load(
        self, service_name: str, current_metrics: ServiceMetrics, time_window_minutes: int = 15
    ) -> tuple[float, float]:
        """
        Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ø­Ù…Ù„ Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… time series analysis
        Returns: (predicted_load, confidence)
        """
        # Get historical patterns
        history = self.historical_patterns[service_name]

        if len(history) < 10:
            # Not enough data, use current load with low confidence
            return current_metrics.request_rate, 0.3

        # Simple moving average with trend detection
        recent_loads = [h.request_rate for h in history[-20:]]
        avg_load = statistics.mean(recent_loads)

        # Detect trend
        if len(recent_loads) >= 2:
            trend = (recent_loads[-1] - recent_loads[0]) / len(recent_loads)
        else:
            trend = 0

        # Predict future load
        predicted_load = avg_load + (trend * time_window_minutes)

        # Calculate confidence based on variance
        variance = statistics.variance(recent_loads) if len(recent_loads) > 1 else 0
        confidence = max(0.5, min(0.95, 1.0 - (variance / (avg_load + 1))))

        return max(0, predicted_load), confidence

    def calculate_optimal_instances(
        self, service_name: str, current_metrics: ServiceMetrics, current_instances: int
    ) -> ScalingDecision:
        """
        Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ù…Ø«Ù„ Ù…Ù† Ø§Ù„Ù…Ø«ÙŠÙ„Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… multi-objective optimization
        """
        # Predict future load
        _predicted_load, confidence = self.predict_load(service_name, current_metrics)

        # Calculate resource utilization
        cpu_util = current_metrics.cpu_usage
        mem_util = current_metrics.memory_usage
        avg_util = (cpu_util + mem_util) / 2

        # Target utilization: 60-70% (sweet spot)
        target_util = 65.0

        # Calculate scaling factor
        if avg_util > 80:
            # Critical: scale up aggressively
            scaling_factor = avg_util / target_util
            direction = ScalingDirection.UP
            reason = f"High utilization: {avg_util:.1f}% (threshold: 80%)"
        elif avg_util < 30 and current_instances > 1:
            # Underutilized: scale down
            scaling_factor = avg_util / target_util
            direction = ScalingDirection.DOWN
            reason = f"Low utilization: {avg_util:.1f}% (threshold: 30%)"
        else:
            # Optimal range
            scaling_factor = 1.0
            direction = ScalingDirection.STABLE
            reason = f"Optimal utilization: {avg_util:.1f}%"

        # Consider latency
        if current_metrics.latency_p99 > 1000:  # 1 second
            scaling_factor = max(scaling_factor, 1.5)
            direction = ScalingDirection.UP
            reason += f" + High latency: P99={current_metrics.latency_p99:.0f}ms"

        # Consider error rate
        if current_metrics.error_rate > 5:  # 5% error rate
            scaling_factor = max(scaling_factor, 1.3)
            direction = ScalingDirection.UP
            reason += f" + High error rate: {current_metrics.error_rate:.1f}%"

        # Calculate target instances
        target_instances = int(current_instances * scaling_factor)
        target_instances = max(1, min(target_instances, 20))  # Min 1, max 20

        # Predict impact
        predicted_impact = {
            "cpu_reduction": (avg_util / target_instances) if target_instances > 0 else avg_util,
            "latency_improvement": 1.0 / scaling_factor if scaling_factor > 1 else 1.0,
            "cost_increase": target_instances / current_instances if current_instances > 0 else 1.0,
        }

        return ScalingDecision(
            service_name=service_name,
            direction=direction,
            current_instances=current_instances,
            target_instances=target_instances,
            confidence=confidence,
            reason=reason,
            predicted_impact=predicted_impact,
            timestamp=datetime.now(),
        )

    def learn_from_decision(self, decision: ScalingDecision, actual_impact: dict[str, float]):
        """
        Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ù‚Ø±Ø§Ø±Ø§Øª Ø§Ù„ØªÙˆØ³Ø¹ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©
        """
        self.scaling_history.append(
            {"decision": decision, "actual_impact": actual_impact, "timestamp": datetime.now()}
        )

        # Simple learning: adjust confidence based on accuracy
        # ÙÙŠ ØªØ·Ø¨ÙŠÙ‚ Ø­Ù‚ÙŠÙ‚ÙŠØŒ Ø³Ù†Ø³ØªØ®Ø¯Ù… RL Ø£Ùˆ supervised learning


class IntelligentRouter:
    """
    Ù…ÙˆØ¬Ù‡ Ø°ÙƒÙŠ ÙŠØ³ØªØ®Ø¯Ù… ML Ù„Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ instance Ù„Ù„Ø·Ù„Ø¨
    """

    def __init__(self):
        self.routing_history = defaultdict(list)
        self.instance_scores = defaultdict(float)

    def select_instance(
        self,
        service_name: str,
        instances: list[ServiceInstance],
        request_metadata: dict | None = None,
    ) -> ServiceInstance | None:
        """
        Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ instance Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ML scoring
        """
        if not instances:
            return None

        # Filter healthy instances
        healthy_instances = [i for i in instances if i.is_healthy()]

        if not healthy_instances:
            # No healthy instances, try degraded
            degraded = [i for i in instances if i.status == ServiceHealth.DEGRADED]
            if degraded:
                return degraded[0]
            return None

        # Score each instance
        scored_instances = []
        for instance in healthy_instances:
            score = self._calculate_instance_score(instance, request_metadata)
            scored_instances.append((score, instance))

        # Sort by score (higher is better)
        scored_instances.sort(reverse=True, key=lambda x: x[0])

        # Select best instance
        best_instance = scored_instances[0][1]

        # Record routing decision for learning
        self.routing_history[service_name].append(
            {
                "instance_id": best_instance.instance_id,
                "timestamp": datetime.now(),
                "request_metadata": request_metadata,
            }
        )

        return best_instance

    def _calculate_instance_score(
        self, instance: ServiceInstance, request_metadata: dict | None = None
    ) -> float:
        """
        Ø­Ø³Ø§Ø¨ score Ù„Ù„Ù€ instance Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¹Ø¯Ø© Ø¹ÙˆØ§Ù…Ù„
        """
        if not instance.metrics_history:
            return 0.5  # Neutral score for new instances

        latest_metrics = instance.metrics_history[-1]

        # Factor 1: Resource availability (higher is better)
        cpu_availability = 100 - latest_metrics.cpu_usage
        mem_availability = 100 - latest_metrics.memory_usage
        resource_score = (cpu_availability + mem_availability) / 200

        # Factor 2: Latency (lower is better)
        latency_score = max(0, 1.0 - (latest_metrics.latency_p95 / 1000))

        # Factor 3: Error rate (lower is better)
        error_score = max(0, 1.0 - (latest_metrics.error_rate / 100))

        # Factor 4: Load (lower is better for even distribution)
        load_score = max(0, 1.0 - (latest_metrics.active_connections / 1000))

        # Factor 5: Historical performance
        historical_score = self.instance_scores.get(instance.instance_id, 0.5)

        # Weighted combination
        total_score = (
            resource_score * 0.3
            + latency_score * 0.25
            + error_score * 0.2
            + load_score * 0.15
            + historical_score * 0.1
        )

        return total_score

    def update_instance_score(self, instance_id: str, success: bool, response_time: float):
        """
        ØªØ­Ø¯ÙŠØ« score Ø§Ù„Ù€ instance Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø·Ù„Ø¨
        """
        current_score = self.instance_scores.get(instance_id, 0.5)

        # Calculate new score based on result
        if success and response_time < 200:  # Fast success
            adjustment = 0.05
        elif success:  # Slow success
            adjustment = 0.02
        else:  # Failure
            adjustment = -0.1

        # Update with learning rate
        new_score = current_score + (adjustment * 0.1)
        new_score = max(0.0, min(1.0, new_score))  # Clamp to [0, 1]

        self.instance_scores[instance_id] = new_score


class PredictiveHealthMonitor:
    """
    Ù†Ø¸Ø§Ù… Ù…Ø±Ø§Ù‚Ø¨Ø© ØµØ­Ø© ØªÙ†Ø¨Ø¤ÙŠ ÙŠÙƒØªØ´Ù Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ù‚Ø¨Ù„ Ø­Ø¯ÙˆØ«Ù‡Ø§
    """

    def __init__(self):
        self.health_patterns = defaultdict(list)
        self.anomaly_threshold = 2.5  # Standard deviations

    def analyze_health(
        self, service_name: str, current_metrics: ServiceMetrics
    ) -> tuple[ServiceHealth, list[str]]:
        """
        ØªØ­Ù„ÙŠÙ„ ØµØ­Ø© Ø§Ù„Ø®Ø¯Ù…Ø© ÙˆØ§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ù…Ø´Ø§ÙƒÙ„
        """
        warnings = []

        # Store metrics for pattern analysis
        self.health_patterns[service_name].append(current_metrics)

        # Keep only recent history
        if len(self.health_patterns[service_name]) > 1000:
            self.health_patterns[service_name] = self.health_patterns[service_name][-1000:]

        # Critical checks
        if current_metrics.cpu_usage > 95:
            warnings.append(f"CRITICAL: CPU usage at {current_metrics.cpu_usage:.1f}%")
        if current_metrics.memory_usage > 95:
            warnings.append(f"CRITICAL: Memory usage at {current_metrics.memory_usage:.1f}%")
        if current_metrics.error_rate > 50:
            warnings.append(f"CRITICAL: Error rate at {current_metrics.error_rate:.1f}%")

        if warnings:
            return ServiceHealth.CRITICAL, warnings

        # Degraded checks
        if current_metrics.cpu_usage > 85:
            warnings.append(f"WARNING: High CPU usage {current_metrics.cpu_usage:.1f}%")
        if current_metrics.memory_usage > 85:
            warnings.append(f"WARNING: High memory usage {current_metrics.memory_usage:.1f}%")
        if current_metrics.error_rate > 10:
            warnings.append(f"WARNING: Elevated error rate {current_metrics.error_rate:.1f}%")
        if current_metrics.latency_p99 > 2000:
            warnings.append(f"WARNING: High latency P99={current_metrics.latency_p99:.0f}ms")

        if warnings:
            return ServiceHealth.DEGRADED, warnings

        # Anomaly detection
        anomalies = self._detect_anomalies(service_name, current_metrics)
        if anomalies:
            warnings.extend(anomalies)
            return ServiceHealth.DEGRADED, warnings

        return ServiceHealth.HEALTHY, []

    def _detect_anomalies(self, service_name: str, current_metrics: ServiceMetrics) -> list[str]:
        """
        ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ° Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… statistical analysis
        """
        history = self.health_patterns[service_name]

        if len(history) < 30:  # Need sufficient history
            return []

        anomalies = []

        # Check each metric for anomalies
        metrics_to_check = {
            "cpu_usage": [m.cpu_usage for m in history[-30:]],
            "memory_usage": [m.memory_usage for m in history[-30:]],
            "latency_p99": [m.latency_p99 for m in history[-30:]],
            "error_rate": [m.error_rate for m in history[-30:]],
        }

        for metric_name, values in metrics_to_check.items():
            if len(values) < 2:
                continue

            mean = statistics.mean(values)
            stdev = statistics.stdev(values)

            current_value = getattr(current_metrics, metric_name)

            # Check if current value is an anomaly
            if stdev > 0:
                z_score = abs((current_value - mean) / stdev)
                if z_score > self.anomaly_threshold:
                    anomalies.append(
                        f"ANOMALY: {metric_name}={current_value:.1f} "
                        f"(mean={mean:.1f}, z-score={z_score:.1f})"
                    )

        return anomalies

    def predict_failure(
        self, service_name: str, lookahead_minutes: int = 15
    ) -> tuple[float, list[str]]:
        """
        Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„ÙØ´Ù„ ÙÙŠ Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ Ø§Ù„Ù‚Ø±ÙŠØ¨
        Returns: (failure_probability, risk_factors)
        """
        history = self.health_patterns[service_name]

        if len(history) < 10:
            return 0.0, []

        risk_factors = []
        risk_score = 0.0

        # Analyze trends
        recent_cpu = [m.cpu_usage for m in history[-10:]]
        recent_mem = [m.memory_usage for m in history[-10:]]
        recent_errors = [m.error_rate for m in history[-10:]]

        # CPU trend
        if len(recent_cpu) >= 2:
            cpu_trend = (recent_cpu[-1] - recent_cpu[0]) / len(recent_cpu)
            if cpu_trend > 5:  # Increasing rapidly
                risk_score += 0.3
                risk_factors.append(f"CPU trending up: +{cpu_trend:.1f}% per interval")

        # Memory trend
        if len(recent_mem) >= 2:
            mem_trend = (recent_mem[-1] - recent_mem[0]) / len(recent_mem)
            if mem_trend > 5:
                risk_score += 0.3
                risk_factors.append(f"Memory trending up: +{mem_trend:.1f}% per interval")

        # Error rate trend
        if len(recent_errors) >= 2:
            error_trend = (recent_errors[-1] - recent_errors[0]) / len(recent_errors)
            if error_trend > 1:
                risk_score += 0.4
                risk_factors.append(f"Errors trending up: +{error_trend:.1f}% per interval")

        failure_probability = min(1.0, risk_score)

        return failure_probability, risk_factors


class SelfAdaptiveMicroservices:
    """
    Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…ØµØºØ±Ø© Ø°Ø§ØªÙŠØ© Ø§Ù„ØªÙƒÙŠÙ
    """

    def __init__(self):
        self.services: dict[str, list[ServiceInstance]] = defaultdict(list)
        self.scaling_engine = AIScalingEngine()
        self.router = IntelligentRouter()
        self.health_monitor = PredictiveHealthMonitor()
        self.lock = threading.RLock()  # Use RLock to prevent deadlock with nested calls

    def register_service(
        self, service_name: str, initial_instances: int = 1
    ) -> list[ServiceInstance]:
        """
        ØªØ³Ø¬ÙŠÙ„ Ø®Ø¯Ù…Ø© Ø¬Ø¯ÙŠØ¯Ø© Ù…Ø¹ Ø§Ù„Ù…Ø«ÙŠÙ„Ø§Øª Ø§Ù„Ø£ÙˆÙ„ÙŠØ©
        """
        with self.lock:
            instances = []
            for i in range(initial_instances):
                instance = ServiceInstance(
                    instance_id=f"{service_name}-{i}",
                    service_name=service_name,
                    status=ServiceHealth.HEALTHY,
                )
                instances.append(instance)

            self.services[service_name] = instances
            return instances

    def update_metrics(self, service_name: str, instance_id: str, metrics: ServiceMetrics):
        """
        ØªØ­Ø¯ÙŠØ« Ù…Ù‚Ø§ÙŠÙŠØ³ instance Ù…Ø­Ø¯Ø¯
        """
        with self.lock:
            for instance in self.services[service_name]:
                if instance.instance_id == instance_id:
                    instance.metrics_history.append(metrics)

                    # Update health status
                    health, warnings = self.health_monitor.analyze_health(service_name, metrics)
                    instance.status = health
                    instance.last_health_check = datetime.now()

                    if warnings:
                        print(f"âš ï¸ Health warnings for {instance_id}:")
                        for warning in warnings:
                            print(f"  - {warning}")

                    break

    def auto_scale(self, service_name: str) -> ScalingDecision | None:
        """
        Ø§Ù„ØªÙˆØ³Ø¹ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ AI analysis
        """
        with self.lock:
            instances = self.services.get(service_name, [])
            if not instances:
                return None

            # Get latest metrics
            healthy_instances = [i for i in instances if i.metrics_history]
            if not healthy_instances:
                return None

            # Use metrics from most loaded instance
            all_metrics = [i.metrics_history[-1] for i in healthy_instances]
            avg_metrics = self._aggregate_metrics(service_name, all_metrics)

            # Calculate scaling decision
            decision = self.scaling_engine.calculate_optimal_instances(
                service_name, avg_metrics, len(instances)
            )

            # Execute scaling
            if decision.direction == ScalingDirection.UP:
                self._scale_up(service_name, decision.target_instances - len(instances))
            elif decision.direction == ScalingDirection.DOWN:
                self._scale_down(service_name, len(instances) - decision.target_instances)

            return decision

    def _aggregate_metrics(
        self, service_name: str, metrics_list: list[ServiceMetrics]
    ) -> ServiceMetrics:
        """
        Ø¯Ù…Ø¬ Ù…Ù‚Ø§ÙŠÙŠØ³ Ù…Ù† instances Ù…ØªØ¹Ø¯Ø¯Ø©
        """
        if not metrics_list:
            return ServiceMetrics(
                service_name=service_name,
                timestamp=datetime.now(),
                cpu_usage=0,
                memory_usage=0,
                request_rate=0,
                error_rate=0,
                latency_p50=0,
                latency_p95=0,
                latency_p99=0,
                active_connections=0,
                queue_depth=0,
            )

        return ServiceMetrics(
            service_name=service_name,
            timestamp=datetime.now(),
            cpu_usage=statistics.mean([m.cpu_usage for m in metrics_list]),
            memory_usage=statistics.mean([m.memory_usage for m in metrics_list]),
            request_rate=sum([m.request_rate for m in metrics_list]),
            error_rate=statistics.mean([m.error_rate for m in metrics_list]),
            latency_p50=statistics.median([m.latency_p50 for m in metrics_list]),
            latency_p95=statistics.median([m.latency_p95 for m in metrics_list]),
            latency_p99=max([m.latency_p99 for m in metrics_list]),
            active_connections=sum([m.active_connections for m in metrics_list]),
            queue_depth=sum([m.queue_depth for m in metrics_list]),
        )

    def _scale_up(self, service_name: str, count: int):
        """Ø¥Ø¶Ø§ÙØ© instances Ø¬Ø¯ÙŠØ¯Ø©"""
        current_count = len(self.services[service_name])
        for i in range(count):
            instance = ServiceInstance(
                instance_id=f"{service_name}-{current_count + i}",
                service_name=service_name,
                status=ServiceHealth.HEALTHY,
            )
            self.services[service_name].append(instance)
        print(f"âœ… Scaled UP {service_name}: +{count} instances")

    def _scale_down(self, service_name: str, count: int):
        """Ø¥Ø²Ø§Ù„Ø© instances"""
        if count >= len(self.services[service_name]):
            count = len(self.services[service_name]) - 1  # Keep at least 1

        # Remove least loaded instances
        instances = self.services[service_name]
        instances.sort(
            key=lambda i: i.metrics_history[-1].active_connections if i.metrics_history else 0
        )

        for _ in range(count):
            if len(instances) > 1:
                removed = instances.pop(0)
                print(f"âœ… Scaled DOWN {service_name}: removed {removed.instance_id}")

    def route_request(
        self, service_name: str, request_metadata: dict | None = None
    ) -> ServiceInstance | None:
        """
        ØªÙˆØ¬ÙŠÙ‡ Ø·Ù„Ø¨ Ø¥Ù„Ù‰ Ø£ÙØ¶Ù„ instance
        """
        instances = self.services.get(service_name, [])
        return self.router.select_instance(service_name, instances, request_metadata)

    def get_service_status(self, service_name: str) -> dict[str, Any]:
        """
        Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø­Ø§Ù„Ø© Ø§Ù„Ø®Ø¯Ù…Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©
        """
        with self.lock:
            instances = self.services.get(service_name, [])

            if not instances:
                return {"service_name": service_name, "status": "not_found", "instances": 0}

            # Calculate aggregate metrics
            healthy_count = sum(1 for i in instances if i.status == ServiceHealth.HEALTHY)

            # Predict failure
            latest_metrics = []
            for inst in instances:
                if inst.metrics_history:
                    latest_metrics.append(inst.metrics_history[-1])

            failure_prob = 0.0
            risk_factors = []
            if latest_metrics:
                self._aggregate_metrics(service_name, latest_metrics)
                failure_prob, risk_factors = self.health_monitor.predict_failure(service_name)

            return {
                "service_name": service_name,
                "total_instances": len(instances),
                "healthy_instances": healthy_count,
                "overall_health": "healthy" if healthy_count / len(instances) > 0.7 else "degraded",
                "failure_probability": failure_prob,
                "risk_factors": risk_factors,
                "instances": [
                    {
                        "id": i.instance_id,
                        "status": i.status.value,
                        "uptime": (datetime.now() - i.created_at).total_seconds(),
                    }
                    for i in instances
                ],
            }


# Example usage
if __name__ == "__main__":
    print("ğŸš€ Initializing Self-Adaptive Microservices System...")

    system = SelfAdaptiveMicroservices()

    # Register services
    system.register_service("user-service", initial_instances=2)
    system.register_service("mission-service", initial_instances=2)
    system.register_service("task-service", initial_instances=3)

    print("\nâœ… Services registered successfully!")
    print("\nğŸ“Š Service Status:")

    for service_name in ["user-service", "mission-service", "task-service"]:
        status = system.get_service_status(service_name)
        print(f"\n{service_name}:")
        print(f"  Instances: {status['total_instances']}")
        print(f"  Health: {status['overall_health']}")

    print("\nğŸ¯ System ready for intelligent auto-scaling and routing!")
