# app/services/api_chaos_monkey_service.py
# ======================================================================================
# ==    SUPERHUMAN CHAOS MONKEY SERVICE (v1.0 - AUTOMATED RESILIENCE)             ==
# ======================================================================================
# PRIME DIRECTIVE:
#   Ù†Ø¸Ø§Ù… Chaos Monkey Ø®Ø§Ø±Ù‚ ÙŠØªÙÙˆÙ‚ Ø¹Ù„Ù‰ Netflix OSS
#   âœ¨ Ø§Ù„Ù…Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø®Ø§Ø±Ù‚Ø©:
#   - Automated chaos experiments
#   - Scheduled resilience testing
#   - Intelligent failure injection
#   - Self-healing validation
#   - Resilience scoring
#   - Disaster simulation
#   - Recovery time tracking
#   - Blast radius control

import logging
import random
import secrets
import threading
from collections import deque
from dataclasses import dataclass, field
from datetime import UTC, datetime, timedelta
from enum import Enum
from typing import Any

# ======================================================================================
# ENUMERATIONS
# ======================================================================================


class ChaosMonkeyMode(Enum):
    """Chaos Monkey operating modes"""

    PASSIVE = "passive"  # Observation only
    SCHEDULED = "scheduled"  # Run on schedule
    CONTINUOUS = "continuous"  # Always running
    PRODUCTION = "production"  # Production-safe mode


class FailureScenario(Enum):
    """Types of failure scenarios"""

    SERVICE_CRASH = "service_crash"
    SLOW_RESPONSE = "slow_response"
    NETWORK_FAILURE = "network_failure"
    DATABASE_UNAVAILABLE = "database_unavailable"
    MEMORY_LEAK = "memory_leak"
    CPU_SPIKE = "cpu_spike"
    DISK_FULL = "disk_full"
    DNS_FAILURE = "dns_failure"


class ResilienceLevel(Enum):
    """System resilience levels"""

    EXCELLENT = "excellent"  # 95-100%
    GOOD = "good"  # 80-95%
    FAIR = "fair"  # 60-80%
    POOR = "poor"  # 40-60%
    CRITICAL = "critical"  # 0-40%


# ======================================================================================
# DATA STRUCTURES
# ======================================================================================


@dataclass
class ChaosSchedule:
    """Scheduled chaos experiment"""

    schedule_id: str
    name: str
    scenario: FailureScenario

    # Scheduling
    cron_expression: str  # e.g., "0 */6 * * *" for every 6 hours
    next_run: datetime
    last_run: datetime | None = None

    # Configuration
    target_services: list[str] = field(default_factory=list)
    max_duration_minutes: int = 30
    blast_radius_limit: float = 0.1  # Affect max 10% of traffic

    # Safety
    production_safe: bool = False
    require_approval: bool = True

    # Status
    enabled: bool = True
    total_runs: int = 0


@dataclass
class ChaosExecution:
    """Chaos experiment execution record"""

    execution_id: str
    schedule_id: str | None
    scenario: FailureScenario

    started_at: datetime
    ended_at: datetime | None = None

    # Targets
    affected_services: list[str] = field(default_factory=list)
    affected_requests: int = 0

    # Results
    system_recovered: bool = False
    recovery_time_seconds: float = 0.0
    alerts_triggered: int = 0

    # Metrics
    error_rate_before: float = 0.0
    error_rate_during: float = 0.0
    error_rate_after: float = 0.0

    latency_p99_before: float = 0.0
    latency_p99_during: float = 0.0
    latency_p99_after: float = 0.0

    # Outcome
    passed: bool = False
    lessons_learned: list[str] = field(default_factory=list)


@dataclass
class ResilienceScore:
    """System resilience scoring"""

    score: float  # 0-100
    level: ResilienceLevel

    # Components
    availability_score: float = 0.0
    recovery_score: float = 0.0
    fault_tolerance_score: float = 0.0

    # Details
    total_tests: int = 0
    passed_tests: int = 0
    failed_tests: int = 0

    avg_recovery_time: float = 0.0
    max_recovery_time: float = 0.0

    calculated_at: datetime = field(default_factory=lambda: datetime.now(UTC))


# ======================================================================================
# CHAOS MONKEY SERVICE
# ======================================================================================


class ChaosMonkeyService:
    """
    Ø®Ø¯Ù…Ø© Chaos Monkey Ø§Ù„Ø®Ø§Ø±Ù‚Ø© - Superhuman Chaos Monkey Service

    Features:
    - Automated chaos experiments
    - Scheduled resilience testing
    - Intelligent failure injection
    - Self-healing validation
    - Resilience scoring
    - Production-safe chaos engineering
    """

    def __init__(self):
        self.mode: ChaosMonkeyMode = ChaosMonkeyMode.SCHEDULED
        self.schedules: dict[str, ChaosSchedule] = {}
        self.executions: deque = deque(maxlen=1000)
        self.resilience_history: deque = deque(maxlen=100)

        self.lock = threading.RLock()

        # Safety controls
        self.enabled = False
        self.blast_radius_limit = 0.1  # Max 10% of traffic
        self.max_concurrent_experiments = 1
        self.running_experiments: set[str] = set()

        # Metrics
        self.total_experiments_run = 0
        self.total_recoveries_validated = 0

        self._initialize_default_schedules()

    def _initialize_default_schedules(self):
        """Initialize default chaos schedules"""

        # Database failure simulation
        self.schedules["db_failure"] = ChaosSchedule(
            schedule_id="sch_db_001",
            name="Database Connection Failure",
            scenario=FailureScenario.DATABASE_UNAVAILABLE,
            cron_expression="0 */12 * * *",  # Every 12 hours
            next_run=datetime.now(UTC) + timedelta(hours=12),
            target_services=["database"],
            max_duration_minutes=5,
            blast_radius_limit=0.05,
            production_safe=True,
            require_approval=False,
        )

        # Slow response simulation
        self.schedules["slow_api"] = ChaosSchedule(
            schedule_id="sch_slow_001",
            name="API Slow Response",
            scenario=FailureScenario.SLOW_RESPONSE,
            cron_expression="0 */6 * * *",  # Every 6 hours
            next_run=datetime.now(UTC) + timedelta(hours=6),
            target_services=["api"],
            max_duration_minutes=10,
            blast_radius_limit=0.1,
            production_safe=True,
            require_approval=False,
        )

        # Network failure simulation
        self.schedules["network_fail"] = ChaosSchedule(
            schedule_id="sch_net_001",
            name="Network Partition",
            scenario=FailureScenario.NETWORK_FAILURE,
            cron_expression="0 0 * * 0",  # Weekly on Sunday
            next_run=datetime.now(UTC) + timedelta(days=7),
            target_services=["external_api"],
            max_duration_minutes=15,
            blast_radius_limit=0.05,
            production_safe=False,
            require_approval=True,
        )

    def enable_chaos_monkey(self, mode: ChaosMonkeyMode = ChaosMonkeyMode.SCHEDULED):
        """Enable Chaos Monkey"""
        with self.lock:
            self.enabled = True
            self.mode = mode

            logging.getLogger(__name__).info(f"ðŸ’ Chaos Monkey enabled in {mode.value} mode")

    def disable_chaos_monkey(self):
        """Disable Chaos Monkey"""
        with self.lock:
            self.enabled = False
            logging.getLogger(__name__).info("ðŸ’ Chaos Monkey disabled")

    def execute_chaos_experiment(
        self,
        scenario: FailureScenario,
        target_services: list[str],
        duration_minutes: int = 10,
        schedule_id: str | None = None,
    ) -> ChaosExecution:
        """Execute a chaos experiment"""
        with self.lock:
            if not self.enabled:
                raise RuntimeError("Chaos Monkey is disabled")

            if len(self.running_experiments) >= self.max_concurrent_experiments:
                raise RuntimeError("Maximum concurrent experiments reached")

            execution_id = f"exec_{secrets.token_hex(8)}"

            now = datetime.now(UTC)

            execution = ChaosExecution(
                execution_id=execution_id,
                schedule_id=schedule_id,
                scenario=scenario,
                started_at=now,
                affected_services=target_services,
            )

            self.running_experiments.add(execution_id)

            try:
                # Capture pre-experiment metrics
                execution.error_rate_before = self._get_current_error_rate()
                execution.latency_p99_before = self._get_current_latency_p99()

                # Inject failure
                self._inject_failure(scenario, target_services, duration_minutes)

                # Monitor during experiment
                execution.error_rate_during = self._get_current_error_rate()
                execution.latency_p99_during = self._get_current_latency_p99()
                execution.alerts_triggered = self._count_alerts_triggered()

                # Wait for recovery
                recovery_start = datetime.now(UTC)
                execution.system_recovered = self._wait_for_recovery(timeout_minutes=30)
                recovery_end = datetime.now(UTC)
                execution.recovery_time_seconds = (recovery_end - recovery_start).total_seconds()

                # Capture post-experiment metrics
                execution.error_rate_after = self._get_current_error_rate()
                execution.latency_p99_after = self._get_current_latency_p99()

                # Determine if test passed
                execution.passed = (
                    execution.system_recovered
                    and execution.recovery_time_seconds < 300  # 5 minutes
                    and execution.error_rate_after < execution.error_rate_before * 1.1
                )

                # Generate lessons learned
                execution.lessons_learned = self._generate_lessons(execution)

            finally:
                execution.ended_at = datetime.now(UTC)
                self.running_experiments.discard(execution_id)
                self.executions.append(execution)
                self.total_experiments_run += 1

                if execution.system_recovered:
                    self.total_recoveries_validated += 1

            logging.getLogger(__name__).info(
                f"ðŸ’ Chaos experiment {execution_id} completed: "
                f"{'PASSED' if execution.passed else 'FAILED'}"
            )

            return execution

    def _inject_failure(
        self, scenario: FailureScenario, target_services: list[str], duration_minutes: int
    ):
        """Inject specific failure scenario"""
        logging.getLogger(__name__).info(
            f"ðŸ’¥ Injecting {scenario.value} into {', '.join(target_services)} "
            f"for {duration_minutes} minutes"
        )

        # Simulate failure injection
        # In production, this would actually inject faults
        pass

    def _wait_for_recovery(self, timeout_minutes: int = 30) -> bool:
        """Wait for system to recover"""
        # Simplified recovery check
        # In production, this would monitor actual system health
        return True

    def _get_current_error_rate(self) -> float:
        """Get current error rate"""
        # Simplified - in production, get from observability service
        return random.uniform(0.0, 5.0)

    def _get_current_latency_p99(self) -> float:
        """Get current P99 latency"""
        # Simplified - in production, get from observability service
        return random.uniform(100.0, 500.0)

    def _count_alerts_triggered(self) -> int:
        """Count alerts triggered during experiment"""
        # Simplified - in production, check alerting system
        return random.randint(0, 5)

    def _generate_lessons(self, execution: ChaosExecution) -> list[str]:
        """Generate lessons learned from execution"""
        lessons = []

        if not execution.system_recovered:
            lessons.append("System did not recover automatically - manual intervention required")

        if execution.recovery_time_seconds > 300:
            lessons.append(
                f"Recovery took {execution.recovery_time_seconds:.0f}s - exceeds 5-minute SLA"
            )

        if execution.error_rate_during > 20:
            lessons.append(f"Error rate spiked to {execution.error_rate_during:.1f}% during chaos")

        if execution.alerts_triggered == 0:
            lessons.append("No alerts triggered - alerting system may need tuning")

        if execution.passed:
            lessons.append("System demonstrated good resilience and recovery")

        return lessons

    def calculate_resilience_score(self) -> ResilienceScore:
        """Calculate overall system resilience score"""
        with self.lock:
            if not self.executions:
                return ResilienceScore(score=0.0, level=ResilienceLevel.CRITICAL, total_tests=0)

            # Get recent executions
            recent = list(self.executions)[-20:]  # Last 20 tests

            total_tests = len(recent)
            passed_tests = sum(1 for e in recent if e.passed)
            failed_tests = total_tests - passed_tests

            # Calculate component scores
            pass_rate = (passed_tests / total_tests) * 100 if total_tests > 0 else 0

            recovery_times = [e.recovery_time_seconds for e in recent if e.system_recovered]
            avg_recovery = sum(recovery_times) / len(recovery_times) if recovery_times else 0
            max_recovery = max(recovery_times) if recovery_times else 0

            # Recovery score (lower recovery time = higher score)
            recovery_score = max(0, 100 - (avg_recovery / 3.0))  # 300s = 0 points

            # Fault tolerance score
            fault_tolerance = (sum(1 for e in recent if e.system_recovered) / total_tests) * 100

            # Overall score
            overall_score = (pass_rate * 0.4) + (recovery_score * 0.3) + (fault_tolerance * 0.3)

            # Determine level
            if overall_score >= 95:
                level = ResilienceLevel.EXCELLENT
            elif overall_score >= 80:
                level = ResilienceLevel.GOOD
            elif overall_score >= 60:
                level = ResilienceLevel.FAIR
            elif overall_score >= 40:
                level = ResilienceLevel.POOR
            else:
                level = ResilienceLevel.CRITICAL

            score = ResilienceScore(
                score=round(overall_score, 2),
                level=level,
                availability_score=round(pass_rate, 2),
                recovery_score=round(recovery_score, 2),
                fault_tolerance_score=round(fault_tolerance, 2),
                total_tests=total_tests,
                passed_tests=passed_tests,
                failed_tests=failed_tests,
                avg_recovery_time=round(avg_recovery, 2),
                max_recovery_time=round(max_recovery, 2),
            )

            self.resilience_history.append(score)

            return score

    def get_chaos_status(self) -> dict[str, Any]:
        """Get current Chaos Monkey status"""
        with self.lock:
            resilience = self.calculate_resilience_score()

            return {
                "enabled": self.enabled,
                "mode": self.mode.value,
                "running_experiments": len(self.running_experiments),
                "total_experiments_run": self.total_experiments_run,
                "total_recoveries_validated": self.total_recoveries_validated,
                "blast_radius_limit": self.blast_radius_limit,
                "resilience_score": {
                    "score": resilience.score,
                    "level": resilience.level.value,
                    "components": {
                        "availability": resilience.availability_score,
                        "recovery": resilience.recovery_score,
                        "fault_tolerance": resilience.fault_tolerance_score,
                    },
                    "tests": {
                        "total": resilience.total_tests,
                        "passed": resilience.passed_tests,
                        "failed": resilience.failed_tests,
                    },
                },
                "scheduled_experiments": len(self.schedules),
            }

    def get_experiment_history(self, limit: int = 10) -> list[dict[str, Any]]:
        """Get chaos experiment history"""
        with self.lock:
            recent = list(self.executions)[-limit:]

            return [
                {
                    "execution_id": exec.execution_id,
                    "scenario": exec.scenario.value,
                    "started_at": exec.started_at.isoformat(),
                    "ended_at": exec.ended_at.isoformat() if exec.ended_at else None,
                    "affected_services": exec.affected_services,
                    "system_recovered": exec.system_recovered,
                    "recovery_time_seconds": exec.recovery_time_seconds,
                    "passed": exec.passed,
                    "lessons_learned": exec.lessons_learned,
                }
                for exec in reversed(recent)
            ]


# ======================================================================================
# SINGLETON INSTANCE
# ======================================================================================

_chaos_monkey_instance: ChaosMonkeyService | None = None
_service_lock = threading.Lock()


def get_chaos_monkey_service() -> ChaosMonkeyService:
    """Get singleton chaos monkey service"""
    global _chaos_monkey_instance

    if _chaos_monkey_instance is None:
        with _service_lock:
            if _chaos_monkey_instance is None:
                _chaos_monkey_instance = ChaosMonkeyService()

    return _chaos_monkey_instance
