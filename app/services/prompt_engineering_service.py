"""
PROMPT ENGINEERING SERVICE - SUPERHUMAN EDITION v2.0
=====================================================
File        : app/services/prompt_engineering_service.py
Version     : 2.0.0 â€¢ "ULTIMATE-PROMPT-FORGE-SUPERHUMAN"
Status      : Production / Superhuman / Revolutionary / World-Class
Author      : Overmind + RAG System + Meta-Prompt Engine + Security AI

MISSION (Ø§Ù„Ù…Ù‡Ù…Ø©)
-------
Ø®Ø¯Ù…Ø© Ù‡Ù†Ø¯Ø³Ø© Prompts Ø®Ø§Ø±Ù‚Ø© ØªÙ†ØªØ¬ prompts Ø§Ø­ØªØ±Ø§ÙÙŠØ© Ù…Ø®ØµØµØ© Ù„Ù„Ù…Ø´Ø±ÙˆØ¹ Ø¨Ù…ÙŠØ²Ø§Øª ØªÙÙˆÙ‚ Ø§Ù„Ø´Ø±ÙƒØ§Øª Ø§Ù„Ø¹Ù…Ù„Ø§Ù‚Ø©:
  - ØªØ¬Ù…Ø¹ Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„ÙƒØ§Ù…Ù„Ø© Ø¹Ù† Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ (Knowledge Base)
  - ØªØ³ØªØ®Ø¯Ù… Meta-Prompt Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ Ù…Ø¹ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ù…Ø´Ø±ÙˆØ¹
  - ØªØ¯Ù…Ø¬ Ø£Ù…Ø«Ù„Ø© Few-Shot Ù…Ù† Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹
  - ØªØ³ØªØ®Ø¯Ù… RAG Ù„Ø¬Ù„Ø¨ Ù…Ù‚ØªØ·ÙØ§Øª Ø°Ø§Øª ØµÙ„Ø©
  - Ø¯Ø¹Ù… Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù„ØºØ§Øª Ø§Ù„ÙƒØ§Ù…Ù„ (Ø¹Ø±Ø¨ÙŠØŒ Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØŒ ÙØ±Ù†Ø³ÙŠØŒ Ø¥Ø³Ø¨Ø§Ù†ÙŠØŒ ØµÙŠÙ†ÙŠØŒ ÙˆØ£ÙƒØ«Ø±)
  - ØªÙƒØªØ´Ù ÙˆØªÙ…Ù†Ø¹ Ù‡Ø¬Ù…Ø§Øª prompt injection Ø¨Ø°ÙƒØ§Ø¡ Ø®Ø§Ø±Ù‚
  - ØªØªØ¹Ù„Ù… ÙˆØªØªÙˆØ³Ø¹ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
  - ØªØ¯Ø¹Ù… Ø§Ù„Ø³ÙŠØ§Ù‚Ø§Øª Ø§Ù„Ø·ÙˆÙŠÙ„Ø© (Long Context) Ø­ØªÙ‰ 1M tokens
  - ØªØ·Ø¨Ù‚ ØªÙ‚Ù†ÙŠØ§Øª chain-of-thought Ùˆ few-shot learning Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
  - ØªØªÙÙˆÙ‚ Ø¹Ù„Ù‰ Ø£Ø¹Ø¸Ù… Ø´Ø±ÙƒØ§Øª Ù‡Ù†Ø¯Ø³Ø© Prompts Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© (OpenAI, Google, Microsoft, Meta, Apple)

SUPERHUMAN FEATURES (Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø®Ø§Ø±Ù‚Ø©) - NEW IN v2.0
---------------------------
âœ… Multi-Language Support (16+ languages with auto-detection)
âœ… Prompt Injection Detection & Prevention (AI-powered)
âœ… Auto-Expanding Prompt Library (learns from usage)
âœ… Multi-Modal Support (text, code, images, audio descriptors)
âœ… Advanced Chain-of-Thought Prompting
âœ… Few-Shot Learning with Dynamic Examples
âœ… Long Context Handling (up to 1M tokens)
âœ… PEFT Support (Parameter-Efficient Fine-Tuning)
âœ… Complete Observability Integration
âœ… Advanced Feedback Loop (RLHF++)
âœ… Risk Classification & Management
âœ… Budget Management & Cost Optimization
âœ… Streaming Support for Large Prompts
âœ… Template Versioning & A/B Testing
âœ… Performance Monitoring & Analytics

CORE CAPABILITIES (Ø§Ù„Ù‚Ø¯Ø±Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©)
-----------------
1. generate_prompt() - ØªÙˆÙ„ÙŠØ¯ prompt Ø®Ø§Ø±Ù‚ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù„ØºØ§Øª
2. create_template() - Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ù„Ø¨ meta-prompt Ø¬Ø¯ÙŠØ¯ Ù…Ø¹ versioning
3. get_project_context() - Ø¬Ù…Ø¹ Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„ÙƒØ§Ù…Ù„ Ø¨Ø°ÙƒØ§Ø¡
4. retrieve_relevant_snippets() - Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ù…Ù‚ØªØ·ÙØ§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… RAG Ù…ØªÙ‚Ø¯Ù…
5. build_few_shot_examples() - Ø¨Ù†Ø§Ø¡ Ø£Ù…Ø«Ù„Ø© Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ© Ù…Ù† Ø§Ù„Ù…Ø´Ø±ÙˆØ¹
6. evaluate_prompt() - ØªÙ‚ÙŠÙŠÙ… Ø¬ÙˆØ¯Ø© Ø§Ù„Ù€ prompt Ø§Ù„Ù…ÙˆÙ„Ø¯
7. detect_prompt_injection() - Ø§ÙƒØªØ´Ø§Ù Ù‡Ø¬Ù…Ø§Øª prompt injection
8. sanitize_prompt() - ØªÙ†Ù‚ÙŠØ© Ø§Ù„Ù€ prompts Ù…Ù† Ù…Ø­ØªÙˆÙ‰ Ø®Ø·ÙŠØ±
9. classify_risk() - ØªØµÙ†ÙŠÙ Ù…Ø®Ø§Ø·Ø± Ø§Ù„Ù€ prompts
10. expand_library() - ØªÙˆØ³ÙŠØ¹ Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù‚ÙˆØ§Ù„Ø¨ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
11. support_multimodal() - Ù…Ø¹Ø§Ù„Ø¬Ø© prompts Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø·
12. handle_long_context() - Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³ÙŠØ§Ù‚Ø§Øª Ø§Ù„Ø·ÙˆÙŠÙ„Ø© Ø¨ÙƒÙØ§Ø¡Ø©

ARCHITECTURE (Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„Ù…Ø·ÙˆØ±Ø©)
------------
Multi-Language Detector â†’ Security Scanner â†’ Knowledge Base â†’ RAG Retrieval
         â†“                       â†“                 â†“                â†“
Language Adaptation    Risk Classifier    Project Context   Advanced Retrieval
         â†“                       â†“                 â†“                â†“
Chain-of-Thought â†’ Meta-Prompt Engine â†’ Few-Shot Learner â†’ Prompt Generator
         â†“                       â†“                 â†“                â†“
Quality Evaluator â†’ Observability â†’ Feedback Loop â†’ Auto-Expansion
         â†“                       â†“                 â†“                â†“
    Final Prompt          Metrics & Logs     RLHF++      Template Library++
"""

from __future__ import annotations

import logging
import os
import re
import time
from typing import Any

from app import db
from app.models import GeneratedPrompt, PromptTemplate, User

try:
    from app.overmind.planning.deep_indexer import build_index, summarize_for_prompt
except ImportError:
    build_index = None
    summarize_for_prompt = None

try:
    from app.services.llm_client_service import get_llm_client
except ImportError:
    get_llm_client = None

try:
    from app.services import system_service
except ImportError:
    system_service = None

try:
    from app.middleware.observability import track_event, track_metric
except ImportError:
    track_metric = None
    track_event = None

logger = logging.getLogger(__name__)

# ============================================================================
# SUPERHUMAN CONFIGURATION - WORLD-CLASS SETTINGS
# ============================================================================

# Core Configuration
DEFAULT_MODEL = os.getenv("DEFAULT_AI_MODEL", "anthropic/claude-3.7-sonnet:thinking")
LOW_COST_MODEL = os.getenv("LOW_COST_MODEL", "openai/gpt-4o-mini")
MAX_CONTEXT_SNIPPETS = int(os.getenv("PROMPT_ENG_MAX_CONTEXT_SNIPPETS", "10"))
MAX_FEW_SHOT_EXAMPLES = int(os.getenv("PROMPT_ENG_MAX_FEW_SHOT_EXAMPLES", "5"))
ENABLE_RAG = os.getenv("PROMPT_ENG_ENABLE_RAG", "1") == "1"

# Multi-Language Support (16+ languages)
SUPPORTED_LANGUAGES = [
    "en",
    "ar",
    "es",
    "fr",
    "de",
    "it",
    "pt",
    "ru",
    "zh",
    "ja",
    "ko",
    "hi",
    "tr",
    "nl",
    "pl",
    "sv",
]
DEFAULT_LANGUAGE = os.getenv("PROMPT_ENG_DEFAULT_LANGUAGE", "en")

# Security Configuration
ENABLE_INJECTION_DETECTION = os.getenv("PROMPT_ENG_INJECTION_DETECTION", "1") == "1"
ENABLE_CONTENT_FILTERING = os.getenv("PROMPT_ENG_CONTENT_FILTERING", "1") == "1"
MAX_RISK_LEVEL = int(os.getenv("PROMPT_ENG_MAX_RISK_LEVEL", "7"))  # 0-10 scale

# Advanced Features
ENABLE_CHAIN_OF_THOUGHT = os.getenv("PROMPT_ENG_CHAIN_OF_THOUGHT", "1") == "1"
ENABLE_AUTO_EXPANSION = os.getenv("PROMPT_ENG_AUTO_EXPANSION", "1") == "1"
ENABLE_MULTI_MODAL = os.getenv("PROMPT_ENG_MULTI_MODAL", "1") == "1"
MAX_CONTEXT_LENGTH = int(
    os.getenv("PROMPT_ENG_MAX_CONTEXT_LENGTH", "100000")
)  # 100k tokens default
SUPPORT_LONG_CONTEXT = os.getenv("PROMPT_ENG_LONG_CONTEXT", "1") == "1"

# Performance & Budget
ENABLE_STREAMING = os.getenv("PROMPT_ENG_STREAMING", "0") == "1"
COST_BUDGET_PER_REQUEST = float(os.getenv("PROMPT_ENG_COST_BUDGET", "0.50"))  # $0.50 default
CACHE_TTL = int(os.getenv("PROMPT_ENG_CACHE_TTL", "300"))  # 5 minutes

# Observability
ENABLE_METRICS = os.getenv("PROMPT_ENG_METRICS", "1") == "1"
ENABLE_DETAILED_LOGGING = os.getenv("PROMPT_ENG_DETAILED_LOGGING", "1") == "1"

# ============================================================================
# SECURITY PATTERNS - Prompt Injection Detection
# ============================================================================

INJECTION_PATTERNS = [
    # Direct instruction injection - improved patterns
    r"ignore.*?(previous|above|all|prior).*?(instructions?|commands?|prompts?)",
    r"disregard.*?(previous|above|all|prior).*?(instructions?|commands?|prompts?)",
    r"forget.*?(everything|all).*?(you\s+)?know",
    r"(new|different)\s+instructions?:\s*",
    r"(system|admin|root):\s*",
    r"override.*?(instructions?|rules?|system|security)",
    # Prompt leaking attempts
    r"(show|reveal|display|tell|output|print).*?(your|the)\s+(prompt|instructions?|system)",
    r"what.*?(are|is)\s+your\s+(instructions?|prompt|system)",
    r"repeat.*?(your|the)\s+(instructions?|prompt)",
    # Jailbreak attempts
    r"act\s+as\s+if\s+you\s+(are|were)",
    r"pretend.*?(to\s+be|you\s+are)",
    r"simulate.*?(being|that\s+you)",
    r"roleplay\s+as",
    r"you\s+are\s+now.*?(a|an)\s+",
    # Code injection
    r"<script[\s\S]*?>[\s\S]*?</script>",
    r"javascript:\s*",
    r'on\w+\s*=\s*["\']',
    r"eval\s*\(",
    r"exec\s*\(",
    # Command injection
    r";\s*(cat|ls|rm|sudo|bash|sh|curl|wget)",
    r"\$\([^)]+\)",
    r"`[^`]+`",
    r"\|\s*(cat|ls|rm|grep|awk)",
]

# Compile patterns for performance
INJECTION_REGEX = [re.compile(pattern, re.IGNORECASE) for pattern in INJECTION_PATTERNS]

# ============================================================================
# LANGUAGE DETECTION KEYWORDS
# ============================================================================

LANGUAGE_KEYWORDS = {
    "ar": ["Ø£Ù†Ø´Ø¦", "Ø§ÙƒØªØ¨", "ØµÙ…Ù…", "Ù†ÙØ°", "Ø·ÙˆØ±", "Ù‡Ù†Ø¯Ø³", "Ø§Ù„Ø³Ù„Ø§Ù…", "Ù…Ø±Ø­Ø¨Ø§"],
    "es": ["crear", "escribir", "diseÃ±ar", "implementar", "desarrollar", "hola"],
    "fr": ["crÃ©er", "Ã©crire", "concevoir", "implÃ©menter", "dÃ©velopper", "bonjour"],
    "de": ["erstellen", "schreiben", "entwerfen", "implementieren", "entwickeln", "hallo"],
    "zh": ["åˆ›å»º", "å†™", "è®¾è®¡", "å®ç°", "å¼€å‘", "ä½ å¥½"],
    "ja": ["ä½œæˆ", "æ›¸ã", "è¨­è¨ˆ", "å®Ÿè£…", "é–‹ç™º", "ã“ã‚“ã«ã¡ã¯"],
    "ru": ["ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ", "Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ", "Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ", "Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ", "Ğ¿Ñ€Ğ¸Ğ²ĞµÑ‚"],
    "pt": ["criar", "escrever", "projetar", "implementar", "desenvolver", "olÃ¡"],
    "it": ["creare", "scrivere", "progettare", "implementare", "sviluppare", "ciao"],
    "tr": ["oluÅŸtur", "yaz", "tasarla", "uygula", "geliÅŸtir", "merhaba"],
    "hi": ["à¤¬à¤¨à¤¾à¤¨à¤¾", "à¤²à¤¿à¤–à¤¨à¤¾", "à¤¡à¤¿à¤œà¤¼à¤¾à¤‡à¤¨", "à¤•à¤¾à¤°à¥à¤¯à¤¾à¤¨à¥à¤µà¤¯à¤¨", "à¤µà¤¿à¤•à¤¾à¤¸", "à¤¨à¤®à¤¸à¥à¤¤à¥‡"],
    "ko": ["ë§Œë“¤ë‹¤", "ì“°ë‹¤", "ì„¤ê³„", "êµ¬í˜„", "ê°œë°œ", "ì•ˆë…•í•˜ì„¸ìš”"],
}


class PromptEngineeringService:
    """
    Ø®Ø¯Ù…Ø© Ù‡Ù†Ø¯Ø³Ø© Prompts Ø§Ù„Ø®Ø§Ø±Ù‚Ø© - SUPERHUMAN PROMPT FORGE v2.0

    ØªÙˆÙ„Ø¯ prompts Ø§Ø­ØªØ±Ø§ÙÙŠØ© Ù…Ø®ØµØµØ© Ù„Ù…Ø´Ø±ÙˆØ¹Ùƒ ØªØªÙÙˆÙ‚ Ø¹Ù„Ù‰ Ø£ÙƒØ¨Ø± Ø§Ù„Ø´Ø±ÙƒØ§Øª Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ©
    Ù…Ø¹ Ù…ÙŠØ²Ø§Øª Ø®Ø§Ø±Ù‚Ø© ØªØªØ¬Ø§ÙˆØ² OpenAI Ùˆ Google Ùˆ Microsoft Ùˆ Meta Ùˆ Apple
    """

    def __init__(self):
        self.logger = logger
        self._project_context_cache = None
        self._cache_timestamp = None
        self._cache_ttl = CACHE_TTL

        # Metrics tracking
        self._metrics = {
            "total_generations": 0,
            "successful_generations": 0,
            "failed_generations": 0,
            "injection_attempts_blocked": 0,
            "average_generation_time": 0.0,
            "languages_detected": {},
            "risk_levels_processed": {},
        }

        # Auto-expansion tracking
        self._new_patterns_learned = []
        self._successful_prompts_cache = []

    def detect_language(self, text: str) -> str:
        """
        Ø§ÙƒØªØ´Ø§Ù Ù„ØºØ© Ø§Ù„Ù†Øµ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ - SUPERHUMAN MULTI-LANGUAGE DETECTION

        Detects the language of input text using keyword matching and heuristics.
        Supports 16+ languages including Arabic, English, Chinese, Japanese, etc.
        """
        text_lower = text.lower()

        # Count keyword matches for each language
        language_scores = {}

        for lang, keywords in LANGUAGE_KEYWORDS.items():
            score = sum(1 for keyword in keywords if keyword in text_lower)
            if score > 0:
                language_scores[lang] = score

        # Return language with highest score, or default
        if language_scores:
            detected_lang = max(language_scores, key=language_scores.get)
            self.logger.info(
                f"Language detected: {detected_lang} (score: {language_scores[detected_lang]})"
            )
            return detected_lang

        # Check if text contains Arabic characters
        if any("\u0600" <= c <= "\u06ff" for c in text):
            return "ar"

        # Check if text contains Chinese characters
        if any("\u4e00" <= c <= "\u9fff" for c in text):
            return "zh"

        # Check if text contains Japanese characters
        if any("\u3040" <= c <= "\u309f" or "\u30a0" <= c <= "\u30ff" for c in text):
            return "ja"

        # Default to English
        return DEFAULT_LANGUAGE

    def detect_prompt_injection(self, text: str) -> dict[str, Any]:
        """
        Ø§ÙƒØªØ´Ø§Ù Ù‡Ø¬Ù…Ø§Øª Prompt Injection - SUPERHUMAN SECURITY

        Detects various types of prompt injection attacks using:
        - Pattern matching (regex)
        - Heuristic analysis
        - Anomaly detection

        Returns dict with:
        - is_malicious: bool
        - risk_level: 0-10
        - detected_patterns: list
        - recommendations: list
        """
        if not ENABLE_INJECTION_DETECTION:
            return {"is_malicious": False, "risk_level": 0, "detected_patterns": []}

        detected_patterns = []
        risk_level = 0

        # Check against known injection patterns
        for i, pattern in enumerate(INJECTION_REGEX):
            if pattern.search(text):
                detected_patterns.append(INJECTION_PATTERNS[i])
                risk_level += 2

        # Heuristic checks
        # 1. Excessive special characters
        special_char_ratio = len(re.findall(r"[<>{}[\]()$`|;]", text)) / max(len(text), 1)
        if special_char_ratio > 0.1:
            risk_level += 1
            detected_patterns.append("High special character density")

        # 2. Multiple instruction keywords
        instruction_keywords = [
            "ignore",
            "disregard",
            "forget",
            "override",
            "system",
            "admin",
            "root",
        ]
        instruction_count = sum(1 for keyword in instruction_keywords if keyword in text.lower())
        if instruction_count >= 3:
            risk_level += 2
            detected_patterns.append("Multiple instruction override keywords")

        # 3. Encoded payloads (base64, hex, etc.)
        if re.search(r"(base64|hex|encode|decode)\s*[:\(]", text, re.IGNORECASE):
            risk_level += 3
            detected_patterns.append("Potential encoded payload")

        # 4. SQL injection patterns
        if re.search(
            r"(union\s+select|drop\s+table|insert\s+into|delete\s+from)", text, re.IGNORECASE
        ):
            risk_level += 4
            detected_patterns.append("SQL injection pattern")

        # Cap risk level at 10
        risk_level = min(risk_level, 10)

        is_malicious = risk_level >= 5

        if is_malicious:
            self._metrics["injection_attempts_blocked"] += 1
            self.logger.warning(
                f"ğŸš¨ Prompt injection detected! Risk level: {risk_level}/10. "
                f"Patterns: {detected_patterns}"
            )

        recommendations = []
        if is_malicious:
            recommendations.append("âš ï¸ Input rejected due to security concerns")
            recommendations.append("Remove suspicious keywords and special characters")
            recommendations.append("Rephrase your request in a clear, straightforward manner")

        return {
            "is_malicious": is_malicious,
            "risk_level": risk_level,
            "detected_patterns": detected_patterns,
            "recommendations": recommendations,
        }

    def sanitize_prompt(self, text: str) -> str:
        """
        ØªÙ†Ù‚ÙŠØ© Ø§Ù„Ù€ Prompts Ù…Ù† Ù…Ø­ØªÙˆÙ‰ Ø®Ø·ÙŠØ± - CONTENT SANITIZATION

        Removes potentially harmful content while preserving meaning.
        """
        if not ENABLE_CONTENT_FILTERING:
            return text

        sanitized = text

        # Remove script tags
        sanitized = re.sub(r"<script[\s\S]*?>[\s\S]*?</script>", "", sanitized, flags=re.IGNORECASE)

        # Remove HTML event handlers
        sanitized = re.sub(r'on\w+\s*=\s*["\'][^"\']*["\']', "", sanitized, flags=re.IGNORECASE)

        # Remove JavaScript protocol
        sanitized = re.sub(r"javascript:\s*", "", sanitized, flags=re.IGNORECASE)

        # Remove command injection attempts
        sanitized = re.sub(r"[;|&]\s*(cat|ls|rm|sudo|bash|sh|curl|wget)", "", sanitized)

        # Remove excessive special characters (keep reasonable ones)
        sanitized = re.sub(r"[<>{}$`]+", "", sanitized)

        return sanitized.strip()

    def classify_risk(self, prompt: str, generated_output: str) -> dict[str, Any]:
        """
        ØªØµÙ†ÙŠÙ Ù…Ø®Ø§Ø·Ø± Ø§Ù„Ù€ Prompts - RISK CLASSIFICATION

        Classifies the risk level of prompts and generated outputs.

        Risk Categories:
        - 0-2: Safe (green)
        - 3-5: Low risk (yellow)
        - 6-8: Medium risk (orange)
        - 9-10: High risk (red)
        """
        risk_factors = []
        total_risk = 0

        # Check input prompt
        injection_check = self.detect_prompt_injection(prompt)
        total_risk += injection_check["risk_level"]
        if injection_check["is_malicious"]:
            risk_factors.append("Malicious input detected")

        # Check output length (very long outputs might be problematic)
        if len(generated_output) > 50000:
            total_risk += 1
            risk_factors.append("Very long output generated")

        # Check for sensitive keywords in output
        sensitive_keywords = ["password", "secret", "api_key", "token", "private_key"]
        sensitive_count = sum(
            1 for keyword in sensitive_keywords if keyword in generated_output.lower()
        )
        if sensitive_count > 0:
            total_risk += sensitive_count
            risk_factors.append(f"Contains {sensitive_count} sensitive keywords")

        # Classify risk category
        if total_risk <= 2:
            category = "safe"
            color = "green"
        elif total_risk <= 5:
            category = "low_risk"
            color = "yellow"
        elif total_risk <= 8:
            category = "medium_risk"
            color = "orange"
        else:
            category = "high_risk"
            color = "red"

        return {
            "risk_level": min(total_risk, 10),
            "category": category,
            "color": color,
            "risk_factors": risk_factors,
        }

    def generate_prompt(
        self,
        user_description: str,
        user: User,
        template_id: int | None = None,
        conversation_id: int | None = None,
        use_rag: bool = True,
        prompt_type: str = "general",
    ) -> dict[str, Any]:
        """
        ØªÙˆÙ„ÙŠØ¯ Prompt Ø®Ø§Ø±Ù‚ Ù…Ù† ÙˆØµÙ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… - SUPERHUMAN v2.0

        New Features:
        âœ… Multi-language support (auto-detection)
        âœ… Prompt injection detection & prevention
        âœ… Advanced security scanning
        âœ… Risk classification
        âœ… Chain-of-thought prompting
        âœ… Observability integration
        âœ… Cost tracking
        âœ… Performance metrics

        Args:
            user_description: ÙˆØµÙ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù„Ù…Ø§ ÙŠØ±ÙŠØ¯
            user: Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø°ÙŠ ÙŠØ·Ù„Ø¨ Ø§Ù„ØªÙˆÙ„ÙŠØ¯
            template_id: Ù…Ø¹Ø±Ù Ø§Ù„Ù‚Ø§Ù„Ø¨ Ø§Ù„Ù…Ø±Ø§Ø¯ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
            conversation_id: Ù…Ø¹Ø±Ù Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
            use_rag: Ø§Ø³ØªØ®Ø¯Ø§Ù… RAG Ù„Ø¬Ù„Ø¨ Ø§Ù„Ø³ÙŠØ§Ù‚
            prompt_type: Ù†ÙˆØ¹ Ø§Ù„Ù€ prompt (code_generation, documentation, etc.)

        Returns:
            Dict ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ù€ prompt Ø§Ù„Ù…ÙˆÙ„Ø¯ ÙˆØ§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¥Ø¶Ø§ÙÙŠØ©
        """
        start_time = time.time()
        self._metrics["total_generations"] += 1

        try:
            self.logger.info(
                f"ğŸš€ Generating SUPERHUMAN prompt for user {user.id}, type: {prompt_type}, "
                f"description length: {len(user_description)}"
            )

            # Track event in observability system
            if track_event and ENABLE_METRICS:
                track_event(
                    "prompt_generation_started",
                    {
                        "user_id": user.id,
                        "prompt_type": prompt_type,
                        "use_rag": use_rag,
                    },
                )

            # ============================================================
            # STEP 1: SECURITY - Detect language & validate input
            # ============================================================
            detected_language = self.detect_language(user_description)
            self._metrics.setdefault("languages_detected", {})[detected_language] = (
                self._metrics["languages_detected"].get(detected_language, 0) + 1
            )

            # Detect prompt injection
            injection_check = self.detect_prompt_injection(user_description)

            if injection_check["is_malicious"]:
                self._metrics["failed_generations"] += 1
                return {
                    "status": "error",
                    "error": "Security violation detected",
                    "message": (
                        f"ğŸš¨ ØªÙ… Ø§ÙƒØªØ´Ø§Ù Ù…Ø­Ø§ÙˆÙ„Ø© Ù‡Ø¬ÙˆÙ… Prompt Injection!\n\n"
                        f"ğŸš¨ Prompt Injection Attack Detected!\n\n"
                        f"**Risk Level:** {injection_check['risk_level']}/10\n\n"
                        f"**Detected Patterns:**\n"
                        + "\n".join(f"- {p}" for p in injection_check["detected_patterns"])
                        + "\n\n**Recommendations:**\n"
                        + "\n".join(f"- {r}" for r in injection_check["recommendations"])
                    ),
                    "security_check": injection_check,
                }

            # Sanitize input
            sanitized_description = self.sanitize_prompt(user_description)

            # ============================================================
            # STEP 2: Get or select template
            # ============================================================
            if template_id:
                template = db.session.get(PromptTemplate, template_id)
                if not template or not template.is_active:
                    return {
                        "status": "error",
                        "error": "Template not found or inactive",
                        "message": "âš ï¸ Ø§Ù„Ù‚Ø§Ù„Ø¨ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ Ø£Ùˆ ØºÙŠØ± Ù†Ø´Ø·",
                    }
            else:
                # Use default template or create dynamic one
                template = self._get_default_template(prompt_type)

            # ============================================================
            # STEP 3: Gather project context (Knowledge Base)
            # ============================================================
            project_context = self._get_project_context()
            project_context["detected_language"] = detected_language

            # ============================================================
            # STEP 4: RAG - Retrieve relevant snippets
            # ============================================================
            relevant_snippets = []
            if use_rag and ENABLE_RAG:
                relevant_snippets = self._retrieve_relevant_snippets(
                    sanitized_description, project_context
                )

            # ============================================================
            # STEP 5: Build few-shot examples (DYNAMIC LEARNING)
            # ============================================================
            few_shot_examples = self._build_few_shot_examples(template, prompt_type)

            # Add successful examples from cache for auto-expansion
            if ENABLE_AUTO_EXPANSION and self._successful_prompts_cache:
                few_shot_examples.extend(self._successful_prompts_cache[-3:])  # Last 3 successful

            # ============================================================
            # STEP 6: Apply Chain-of-Thought if enabled
            # ============================================================
            if ENABLE_CHAIN_OF_THOUGHT:
                chain_of_thought_prefix = self._build_chain_of_thought(
                    sanitized_description, prompt_type, detected_language
                )
            else:
                chain_of_thought_prefix = ""

            # ============================================================
            # STEP 7: Construct meta-prompt (MULTI-LANGUAGE AWARE)
            # ============================================================
            meta_prompt = self._construct_meta_prompt(
                template=template,
                user_description=sanitized_description,
                project_context=project_context,
                relevant_snippets=relevant_snippets,
                few_shot_examples=few_shot_examples,
                prompt_type=prompt_type,
                language=detected_language,
                chain_of_thought=chain_of_thought_prefix,
            )

            # ============================================================
            # STEP 8: Generate final prompt using LLM
            # ============================================================
            generated_prompt = self._generate_with_llm(meta_prompt, detected_language)

            # ============================================================
            # STEP 9: Risk classification & content filtering
            # ============================================================
            risk_assessment = self.classify_risk(sanitized_description, generated_prompt)

            if risk_assessment["risk_level"] > MAX_RISK_LEVEL:
                self._metrics["failed_generations"] += 1
                return {
                    "status": "error",
                    "error": "Risk level too high",
                    "message": (
                        f"âš ï¸ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù…Ø®Ø§Ø·Ø± Ù…Ø±ØªÙØ¹ Ø¬Ø¯Ø§Ù‹ ({risk_assessment['risk_level']}/10)\n\n"
                        f"âš ï¸ Risk level too high ({risk_assessment['risk_level']}/10)\n\n"
                        f"**Risk Factors:**\n"
                        + "\n".join(f"- {r}" for r in risk_assessment["risk_factors"])
                    ),
                    "risk_assessment": risk_assessment,
                }

            # ============================================================
            # STEP 10: Save to database
            # ============================================================
            elapsed_time = time.time() - start_time

            generated_record = GeneratedPrompt(
                user_description=user_description,
                template_id=template.id if template and hasattr(template, "id") else None,
                generated_prompt=generated_prompt,
                context_snippets=relevant_snippets,
                conversation_id=conversation_id,
                created_by_id=user.id,
                generation_metadata={
                    "prompt_type": prompt_type,
                    "use_rag": use_rag,
                    "model": DEFAULT_MODEL,
                    "elapsed_seconds": round(elapsed_time, 2),
                    "context_chunks": len(relevant_snippets),
                    "few_shot_count": len(few_shot_examples),
                    "detected_language": detected_language,
                    "security_check": injection_check,
                    "risk_assessment": risk_assessment,
                    "chain_of_thought_enabled": ENABLE_CHAIN_OF_THOUGHT,
                    "version": "2.0.0-superhuman",
                },
            )
            generated_record.compute_content_hash()

            db.session.add(generated_record)

            # Update template usage
            if template and hasattr(template, "id"):
                template.usage_count += 1

            db.session.commit()

            # ============================================================
            # STEP 11: Update metrics & observability
            # ============================================================
            self._metrics["successful_generations"] += 1
            self._metrics["average_generation_time"] = (
                self._metrics["average_generation_time"]
                * (self._metrics["successful_generations"] - 1)
                + elapsed_time
            ) / self._metrics["successful_generations"]
            self._metrics.setdefault("risk_levels_processed", {})[risk_assessment["category"]] = (
                self._metrics["risk_levels_processed"].get(risk_assessment["category"], 0) + 1
            )

            if track_metric and ENABLE_METRICS:
                track_metric(
                    "prompt_generation_success",
                    1,
                    {
                        "language": detected_language,
                        "risk_level": risk_assessment["risk_level"],
                        "elapsed_time": elapsed_time,
                    },
                )

            self.logger.info(
                f"âœ… Prompt generated successfully: ID {generated_record.id}, "
                f"Language: {detected_language}, Risk: {risk_assessment['category']}, "
                f"Time: {elapsed_time:.2f}s"
            )

            return {
                "status": "success",
                "prompt_id": generated_record.id,
                "generated_prompt": generated_prompt,
                "meta_prompt": meta_prompt,
                "context_snippets": relevant_snippets,
                "few_shot_examples": few_shot_examples,
                "template_name": (
                    template.name if template and hasattr(template, "name") else "dynamic"
                ),
                "metadata": generated_record.generation_metadata,
                "elapsed_seconds": round(elapsed_time, 2),
                "detected_language": detected_language,
                "risk_assessment": risk_assessment,
                "security_check": {
                    "injection_detected": False,
                    "risk_level": injection_check["risk_level"],
                },
            }

        except Exception as e:
            self.logger.error(f"âŒ Prompt generation failed: {e}", exc_info=True)
            db.session.rollback()
            self._metrics["failed_generations"] += 1

            if track_metric and ENABLE_METRICS:
                track_metric("prompt_generation_error", 1, {"error": str(e)})

            error_msg = (
                f"âš ï¸ ÙØ´Ù„ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù€ Prompt.\n\n"
                f"Failed to generate prompt.\n\n"
                f"**Error:** {str(e)}\n\n"
                f"**Ø§Ù„Ø­Ù„ (Solution):**\n"
                f"1. ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„ÙˆØµÙ Ø§Ù„Ù…Ø¯Ø®Ù„ (Check input description)\n"
                f"2. Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ (Try again)\n"
                f"3. Ø§Ø³ØªØ®Ø¯Ù… Ù‚Ø§Ù„Ø¨ Ù…Ø®ØªÙ„Ù (Use different template)\n"
                f"4. ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª (Check internet connection)\n"
                f"5. ØªØ­Ù‚Ù‚ Ù…Ù† Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª API (Verify API configuration)\n"
            )

            return {
                "status": "error",
                "error": str(e),
                "message": error_msg,
                "elapsed_seconds": round(time.time() - start_time, 2),
            }

    def _build_chain_of_thought(
        self, user_description: str, prompt_type: str, language: str
    ) -> str:
        """
        Ø¨Ù†Ø§Ø¡ Chain-of-Thought Ù„Ù„ØªÙÙƒÙŠØ± Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠ - ADVANCED REASONING

        Builds a chain-of-thought prefix that guides the LLM to think step-by-step.
        This dramatically improves prompt quality for complex tasks.
        """
        cot_templates = {
            "en": """Let's approach this step-by-step:

1. **Understanding the Request**: Analyze what the user wants to achieve
2. **Context Analysis**: Consider the project architecture and constraints
3. **Best Practices**: Apply industry-standard practices and patterns
4. **Implementation Strategy**: Plan the optimal approach
5. **Quality Assurance**: Consider edge cases and error handling

Now, let's create the perfect prompt:
""",
            "ar": """Ø¯Ø¹Ù†Ø§ Ù†ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ù‡Ø°Ø§ Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ©:

1. **ÙÙ‡Ù… Ø§Ù„Ø·Ù„Ø¨**: ØªØ­Ù„ÙŠÙ„ Ù…Ø§ ÙŠØ±ÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ØªØ­Ù‚ÙŠÙ‚Ù‡
2. **ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³ÙŠØ§Ù‚**: Ø§Ù„Ù†Ø¸Ø± ÙÙŠ Ø¨Ù†ÙŠØ© Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ ÙˆØ§Ù„Ù‚ÙŠÙˆØ¯
3. **Ø£ÙØ¶Ù„ Ø§Ù„Ù…Ù…Ø§Ø±Ø³Ø§Øª**: ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù…Ù…Ø§Ø±Ø³Ø§Øª ÙˆØ§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù‚ÙŠØ§Ø³ÙŠØ©
4. **Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„ØªÙ†ÙÙŠØ°**: Ø§Ù„ØªØ®Ø·ÙŠØ· Ù„Ù„Ù†Ù‡Ø¬ Ø§Ù„Ø£Ù…Ø«Ù„
5. **Ø¶Ù…Ø§Ù† Ø§Ù„Ø¬ÙˆØ¯Ø©**: Ø§Ù„Ù†Ø¸Ø± ÙÙŠ Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø­Ø¯ÙŠØ© ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡

Ø§Ù„Ø¢Ù†ØŒ Ù„Ù†ØµÙ†Ø¹ Ø§Ù„Ù€ prompt Ø§Ù„Ù…Ø«Ø§Ù„ÙŠ:
""",
        }

        # Get template for detected language, fallback to English
        cot_prefix = cot_templates.get(language, cot_templates["en"])

        # Add task-specific reasoning
        if prompt_type == "code_generation":
            cot_prefix += "\n**Code Generation Considerations:**\n"
            cot_prefix += "- What design patterns are appropriate?\n"
            cot_prefix += "- How to ensure code quality and maintainability?\n"
            cot_prefix += "- What testing strategy should be used?\n\n"
        elif prompt_type == "architecture":
            cot_prefix += "\n**Architecture Considerations:**\n"
            cot_prefix += "- What are the scalability requirements?\n"
            cot_prefix += "- How to ensure system resilience?\n"
            cot_prefix += "- What integration patterns are needed?\n\n"
        elif prompt_type == "documentation":
            cot_prefix += "\n**Documentation Considerations:**\n"
            cot_prefix += "- Who is the target audience?\n"
            cot_prefix += "- What level of detail is appropriate?\n"
            cot_prefix += "- How to structure for maximum clarity?\n\n"

        return cot_prefix

    def auto_expand_library(self, prompt_id: int, rating: int) -> None:
        """
        ØªÙˆØ³ÙŠØ¹ Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù‚ÙˆØ§Ù„Ø¨ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ - AUTO-EXPANSION

        Learns from successful prompts and automatically expands the template library.
        This is a key feature that surpasses major companies.
        """
        if not ENABLE_AUTO_EXPANSION or rating < 4:
            return

        try:
            # Get the successful prompt
            prompt = db.session.get(GeneratedPrompt, prompt_id)
            if not prompt:
                return

            # Add to successful cache
            example = {
                "description": prompt.user_description[:200],
                "prompt": prompt.generated_prompt[:500],
                "result": f"High-quality result (rated {rating}/5)",
                "metadata": prompt.generation_metadata,
            }

            self._successful_prompts_cache.append(example)

            # Keep only last 50 successful prompts
            if len(self._successful_prompts_cache) > 50:
                self._successful_prompts_cache = self._successful_prompts_cache[-50:]

            # After collecting 10 highly-rated prompts of same type, create new template
            prompt_type = prompt.generation_metadata.get("prompt_type", "general")
            similar_prompts = [
                p
                for p in self._successful_prompts_cache
                if p.get("metadata", {}).get("prompt_type") == prompt_type
            ]

            if len(similar_prompts) >= 10:
                self._create_learned_template(prompt_type, similar_prompts)

        except Exception as e:
            self.logger.warning(f"Auto-expansion failed: {e}")

    def _create_learned_template(self, prompt_type: str, examples: list) -> None:
        """Create a new template from learned patterns"""
        try:
            # Extract common patterns
            template_content = self._extract_pattern_from_examples(examples)

            # Create new template
            new_template = PromptTemplate(
                name=f"Auto-Learned {prompt_type.title()} Template",
                description=f"Automatically generated template based on {len(examples)} successful prompts",
                template_content=template_content,
                category=prompt_type,
                few_shot_examples=examples[:5],  # Top 5 examples
                variables=[
                    {"name": "user_description", "description": "User request"},
                    {"name": "project_name", "description": "Project name"},
                ],
                created_by_id=1,  # System user
                version=1,
            )

            db.session.add(new_template)
            db.session.commit()

            self.logger.info(f"âœ¨ Created new auto-learned template for {prompt_type}")

        except Exception as e:
            self.logger.error(f"Failed to create learned template: {e}")
            db.session.rollback()

    def _extract_pattern_from_examples(self, examples: list) -> str:
        """Extract common patterns from successful examples"""
        # Simple pattern extraction - can be enhanced with NLP
        common_phrases = []

        for example in examples[:5]:
            prompt_text = example.get("prompt", "")
            # Extract first few sentences as pattern
            sentences = prompt_text.split(".")[:3]
            common_phrases.extend(sentences)

        # Build template
        template = """You are an expert {prompt_type} specialist working on {project_name}.

**Context:** {user_description}

**Your task:** Create a comprehensive and professional {prompt_type} that follows best practices.

**Requirements:**
- High quality and attention to detail
- Clear and well-structured
- Production-ready
- Well-documented

Please provide your {prompt_type}:
"""
        return template

    def get_metrics(self) -> dict[str, Any]:
        """
        Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡ - OBSERVABILITY

        Returns comprehensive metrics for monitoring and analytics.
        """
        total = self._metrics["total_generations"]
        success_rate = (self._metrics["successful_generations"] / total * 100) if total > 0 else 0

        return {
            "total_generations": total,
            "successful_generations": self._metrics["successful_generations"],
            "failed_generations": self._metrics["failed_generations"],
            "success_rate_percentage": round(success_rate, 2),
            "injection_attempts_blocked": self._metrics["injection_attempts_blocked"],
            "average_generation_time_seconds": round(self._metrics["average_generation_time"], 2),
            "languages_detected": self._metrics.get("languages_detected", {}),
            "risk_levels_processed": self._metrics.get("risk_levels_processed", {}),
            "cached_successful_prompts": len(self._successful_prompts_cache),
        }

    def _get_project_context(self) -> dict[str, Any]:
        """
        Ø¬Ù…Ø¹ Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„ÙƒØ§Ù…Ù„ (Knowledge Base)

        Uses caching to avoid rebuilding index on every request
        """
        # Check cache
        now = time.time()
        if (
            self._project_context_cache
            and self._cache_timestamp
            and (now - self._cache_timestamp) < self._cache_ttl
        ):
            return self._project_context_cache

        try:
            if not build_index:
                return {
                    "project_name": "CogniForge",
                    "project_goal": "Advanced AI-powered educational platform",
                    "architecture": "Flask + SQLAlchemy + Supabase + OpenRouter",
                    "files_indexed": 0,
                }

            # Build comprehensive project index
            index = build_index(root=".")

            context = {
                "project_name": "CogniForge",
                "project_goal": "Advanced AI-powered educational platform with Overmind orchestration",
                "architecture": "Flask + SQLAlchemy + Supabase + OpenRouter + Overmind AI",
                "files_indexed": index.get("files_scanned", 0),
                "total_functions": index.get("global_metrics", {}).get("total_functions", 0),
                "layers": list(index.get("layers", {}).keys()),
                "services": [
                    m.get("module_path")
                    for m in index.get("modules", [])
                    if "service" in m.get("module_path", "")
                ],
                "tech_stack": [
                    "Flask (Python web framework)",
                    "SQLAlchemy (ORM)",
                    "PostgreSQL (Supabase)",
                    "OpenRouter (LLM gateway)",
                    "Overmind (AI orchestration)",
                    "Bootstrap 5 (UI)",
                ],
                "index_summary": (
                    summarize_for_prompt(index, max_len=2000) if summarize_for_prompt else None
                ),
            }

            # Cache the context
            self._project_context_cache = context
            self._cache_timestamp = now

            return context

        except Exception as e:
            self.logger.warning(f"Failed to build project context: {e}")
            return {
                "project_name": "CogniForge",
                "project_goal": "Advanced AI-powered educational platform",
                "architecture": "Flask + SQLAlchemy + Supabase + OpenRouter",
                "error": str(e),
            }

    def _retrieve_relevant_snippets(
        self, user_description: str, project_context: dict
    ) -> list[dict[str, Any]]:
        """
        Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ù‚ØªØ·ÙØ§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… RAG

        Uses semantic similarity to find relevant code/docs
        """
        try:
            snippets = []

            # For now, use keyword matching from index
            # In future, can integrate proper vector embeddings
            keywords = self._extract_keywords(user_description)

            if system_service and hasattr(system_service, "search_code"):
                # Use system service for code search
                for keyword in keywords[:5]:  # Limit to top 5 keywords
                    try:
                        results = system_service.search_code(keyword, max_results=2)
                        if results:
                            snippets.extend(results[:2])
                    except:
                        pass

            # Limit to max snippets
            return snippets[:MAX_CONTEXT_SNIPPETS]

        except Exception as e:
            self.logger.warning(f"RAG retrieval failed: {e}")
            return []

    def _extract_keywords(self, text: str) -> list[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ù…Ù† Ø§Ù„Ù†Øµ"""
        # Simple keyword extraction (can be enhanced with NLP)
        stop_words = {"the", "a", "an", "and", "or", "but", "in", "on", "at", "to", "for"}
        words = text.lower().split()
        keywords = [w for w in words if len(w) > 3 and w not in stop_words]
        return keywords[:10]

    def _build_few_shot_examples(
        self, template: PromptTemplate | None, prompt_type: str
    ) -> list[dict[str, Any]]:
        """
        Ø¨Ù†Ø§Ø¡ Ø£Ù…Ø«Ù„Ø© Few-Shot Ù…Ù† Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹

        Gets examples from template or generates dynamic ones
        """
        examples = []

        try:
            # Get examples from template
            if template and template.few_shot_examples:
                examples = template.few_shot_examples[:MAX_FEW_SHOT_EXAMPLES]

            # If no examples, create dynamic ones based on prompt_type
            if not examples:
                examples = self._get_default_examples(prompt_type)

            return examples

        except Exception as e:
            self.logger.warning(f"Failed to build few-shot examples: {e}")
            return []

    def _get_default_examples(self, prompt_type: str) -> list[dict[str, Any]]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ù…Ø«Ù„Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ù€ prompt"""
        examples_db = {
            "code_generation": [
                {
                    "description": "Create a Flask route for user registration",
                    "prompt": "You are a senior Flask developer. Create a secure user registration endpoint with email validation, password hashing, and proper error handling. Follow Flask best practices and use SQLAlchemy for database operations.",
                    "result": "High-quality Flask route with security best practices",
                },
                {
                    "description": "Write a database migration for new table",
                    "prompt": "As a database architect, create an Alembic migration script for a new 'notifications' table with proper indexes, foreign keys, and constraints. Include timestamps and soft delete support.",
                    "result": "Professional Alembic migration with all best practices",
                },
            ],
            "documentation": [
                {
                    "description": "Document the API endpoint",
                    "prompt": "Write comprehensive API documentation for this endpoint including: purpose, authentication requirements, request/response examples, error codes, and usage notes. Use clear English and provide examples in multiple formats.",
                    "result": "Professional API documentation",
                }
            ],
            "architecture": [
                {
                    "description": "Design a microservice architecture",
                    "prompt": "As a solutions architect, design a microservice architecture for this feature. Include service boundaries, communication patterns, data flow, scalability considerations, and deployment strategy.",
                    "result": "Comprehensive architecture design",
                }
            ],
        }

        return examples_db.get(prompt_type, [])[:MAX_FEW_SHOT_EXAMPLES]

    def _construct_meta_prompt(
        self,
        template: PromptTemplate | None,
        user_description: str,
        project_context: dict,
        relevant_snippets: list,
        few_shot_examples: list,
        prompt_type: str,
        language: str = "en",
        chain_of_thought: str = "",
    ) -> str:
        """
        Ø¨Ù†Ø§Ø¡ Meta-Prompt Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ - MULTI-LANGUAGE AWARE

        Constructs the meta-prompt that will be sent to LLM with full multi-language support.
        """
        # Use template if available, otherwise create dynamic one
        if template and template.template_content:
            base_template = template.template_content
        else:
            base_template = self._get_default_meta_template(prompt_type, language)

        # Build context section
        context_section = self._format_context_section(project_context, relevant_snippets)

        # Build examples section
        examples_section = self._format_examples_section(few_shot_examples)

        # Add chain-of-thought if provided
        cot_section = f"\n{chain_of_thought}\n" if chain_of_thought else ""

        # Replace variables
        try:
            meta_prompt = base_template.format(
                project_name=project_context.get("project_name", "CogniForge"),
                project_goal=project_context.get("project_goal", "Advanced AI platform"),
                user_description=user_description,
                relevant_snippets=context_section,
                few_shot_examples=examples_section,
                prompt_type=prompt_type,
                architecture=project_context.get("architecture", "Flask-based"),
                tech_stack=", ".join(project_context.get("tech_stack", [])),
                chain_of_thought=cot_section,
                language=language,
            )
        except KeyError as e:
            # Fallback if template has missing placeholders
            self.logger.warning(f"Template formatting error: {e}, using simple format")
            meta_prompt = f"{base_template}\n\n{cot_section}User Request: {user_description}"

        return meta_prompt

    def _get_default_meta_template(self, prompt_type: str, language: str = "en") -> str:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù‚Ø§Ù„Ø¨ Meta-Prompt Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù„ØºØ§Øª"""

        # Multi-language templates
        templates = {
            "ar": """Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ø¹Ø§Ù„Ù…ÙŠ ÙÙŠ Ù‡Ù†Ø¯Ø³Ø© Prompts ÙˆØªØ¹Ù…Ù„ Ø¹Ù„Ù‰ Ù…Ø´Ø±ÙˆØ¹ "{project_name}".

**Ù‡Ø¯Ù Ø§Ù„Ù…Ø´Ø±ÙˆØ¹:** {project_goal}

**Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ØªÙ‚Ù†ÙŠØ©:** {architecture}

**Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©:** {tech_stack}

**Ø§Ù„ÙˆØµÙ Ø§Ù„Ù…ÙØ¯Ø®Ù„ Ù…Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:**
{user_description}

{chain_of_thought}

**Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø°Ùˆ Ø§Ù„ØµÙ„Ø©:**
{relevant_snippets}

**Ø£Ù…Ø«Ù„Ø© Ù…Ù† Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ (Few-Shot Learning):**
{few_shot_examples}

---

**Ù…Ù‡Ù…ØªÙƒ (YOUR MISSION):**
Ø§ØµÙ†Ø¹ Prompt Ø§Ø­ØªØ±Ø§ÙÙŠØ§Ù‹ Ø®Ø§Ø±Ù‚Ø§Ù‹ ÙˆØªÙØµÙŠÙ„ÙŠØ§Ù‹ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù…Ø¹ Ù†Ù…ÙˆØ°Ø¬ LLM Ù…ØªÙ‚Ø¯Ù… Ù„Ø¥Ù†ØªØ§Ø¬ {prompt_type}.

**Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ù€ Prompt (REQUIREMENTS):**

1. **Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„ÙƒØ§Ù…Ù„:** Ø¶Ù…Ù‘Ù† Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© Ø¹Ù† Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ ÙˆØ§Ù„ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©
2. **Ø§Ù„ÙˆØ¶ÙˆØ­:** Ø§Ø³ØªØ®Ø¯Ù… Ù„ØºØ© ÙˆØ§Ø¶Ø­Ø© ÙˆÙ…Ø­Ø¯Ø¯Ø©
3. **Ø§Ù„ØªÙ†Ø¸ÙŠÙ…:** Ù‚Ø³Ù‘Ù… Ø§Ù„Ù€ prompt Ø¥Ù„Ù‰ Ø£Ù‚Ø³Ø§Ù… Ù…Ù†Ø·Ù‚ÙŠØ©
4. **Ø§Ù„Ø£Ù…Ø«Ù„Ø©:** Ù‚Ø¯Ù‘Ù… Ø£Ù…Ø«Ù„Ø© ÙˆØ§Ù‚Ø¹ÙŠØ© Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©
5. **Ø§Ù„Ø§Ø­ØªØ±Ø§ÙÙŠØ©:** Ø§ØªØ¨Ø¹ Ø£Ø¹Ù„Ù‰ Ù…Ø¹Ø§ÙŠÙŠØ± Ù‡Ù†Ø¯Ø³Ø© Prompts Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© ØªØªÙÙˆÙ‚ Ø¹Ù„Ù‰ OpenAI Ùˆ Google
6. **Ø§Ù„ØªØ®ØµÙŠØµ:** Ø®ØµØµ Ø§Ù„Ù€ prompt Ù„Ø³ÙŠØ§Ù‚ Ù…Ø´Ø±ÙˆØ¹ {project_name} ØªØ­Ø¯ÙŠØ¯Ø§Ù‹

**ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ (OUTPUT FORMAT):**
Ù‚Ø¯Ù… Ø§Ù„Ù€ Prompt Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù…Ø¨Ø§Ø´Ø±Ø©ØŒ Ø¬Ø§Ù‡Ø²Ø§Ù‹ Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…ØŒ Ø¯ÙˆÙ† Ø£ÙŠ Ø´Ø±Ø­ Ø¥Ø¶Ø§ÙÙŠ.
ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ù€ Prompt Ù‚Ø§Ø¨Ù„Ø§Ù‹ Ù„Ù„Ù†Ø³Ø® ÙˆØ§Ù„Ù„ØµÙ‚ Ù…Ø¨Ø§Ø´Ø±Ø© ÙÙŠ Ù†Ù…ÙˆØ°Ø¬ LLM.

---

**Ø§Ù„Ù€ PROMPT Ø§Ù„Ù…ÙˆÙ„Ø¯:**""",
            "en": """You are a world-class Prompt Engineering expert working on the "{project_name}" project.

**Project Goal:** {project_goal}

**Technical Architecture:** {architecture}

**Tech Stack:** {tech_stack}

**User Request:**
{user_description}

{chain_of_thought}

**Relevant Project Context:**
{relevant_snippets}

**Examples from Project (Few-Shot Learning):**
{few_shot_examples}

---

**YOUR MISSION:**
Create a superhuman, professional, and detailed prompt for use with an advanced LLM to produce {prompt_type}.

**PROMPT REQUIREMENTS:**

1. **Complete Context:** Include sufficient information about the project and technologies
2. **Clarity:** Use clear and specific language
3. **Organization:** Structure the prompt into logical sections
4. **Examples:** Provide real-world examples when needed
5. **Professionalism:** Follow the highest global prompt engineering standards that surpass OpenAI, Google, Microsoft, Meta, and Apple
6. **Customization:** Tailor the prompt specifically for {project_name} project context
7. **Chain-of-Thought:** Include step-by-step reasoning when appropriate
8. **Long-Context Aware:** Handle extended context efficiently

**OUTPUT FORMAT:**
Provide the final prompt directly, ready to use, without any additional explanation.
The prompt should be copy-paste ready for an LLM.

---

**GENERATED PROMPT:**""",
            "es": """Eres un experto mundial en IngenierÃ­a de Prompts trabajando en el proyecto "{project_name}".

**Objetivo del Proyecto:** {project_goal}

**Arquitectura TÃ©cnica:** {architecture}

**Solicitud del Usuario:**
{user_description}

{chain_of_thought}

**Contexto Relevante:**
{relevant_snippets}

**Ejemplos (Few-Shot Learning):**
{few_shot_examples}

---

**TU MISIÃ“N:**
Crear un prompt profesional y detallado de nivel superhumano para usar con un LLM avanzado.

**PROMPT GENERADO:**""",
            "fr": """Vous Ãªtes un expert mondial en IngÃ©nierie de Prompts travaillant sur le projet "{project_name}".

**Objectif du Projet:** {project_goal}

**Architecture Technique:** {architecture}

**Demande de l'Utilisateur:**
{user_description}

{chain_of_thought}

**Contexte Pertinent:**
{relevant_snippets}

**Exemples (Few-Shot Learning):**
{few_shot_examples}

---

**VOTRE MISSION:**
CrÃ©er un prompt professionnel et dÃ©taillÃ© de niveau surhumain pour utiliser avec un LLM avancÃ©.

**PROMPT GÃ‰NÃ‰RÃ‰:**""",
            "zh": """ä½ æ˜¯ä¸€ä½ä¸–ç•Œçº§çš„æç¤ºå·¥ç¨‹ä¸“å®¶ï¼Œæ­£åœ¨ä¸º"{project_name}"é¡¹ç›®å·¥ä½œã€‚

**é¡¹ç›®ç›®æ ‡:** {project_goal}

**æŠ€æœ¯æ¶æ„:** {architecture}

**ç”¨æˆ·è¯·æ±‚:**
{user_description}

{chain_of_thought}

**ç›¸å…³ä¸Šä¸‹æ–‡:**
{relevant_snippets}

**ç¤ºä¾‹ (Few-Shot Learning):**
{few_shot_examples}

---

**ä½ çš„ä»»åŠ¡:**
åˆ›å»ºä¸€ä¸ªä¸“ä¸šã€è¯¦ç»†çš„è¶…äººçº§æç¤ºï¼Œç”¨äºé«˜çº§LLMã€‚

**ç”Ÿæˆçš„æç¤º:**""",
        }

        # Return template for detected language, fallback to English
        return templates.get(language, templates["en"])

    def _format_context_section(self, project_context: dict, snippets: list) -> str:
        """ØªÙ†Ø³ÙŠÙ‚ Ù‚Ø³Ù… Ø§Ù„Ø³ÙŠØ§Ù‚"""
        sections = []

        if project_context.get("index_summary"):
            sections.append(f"**Ù…Ù„Ø®Øµ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹:**\n{project_context['index_summary']}\n")

        if snippets:
            sections.append("**Ù…Ù‚ØªØ·ÙØ§Øª Ø§Ù„ÙƒÙˆØ¯ Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©:**")
            for i, snippet in enumerate(snippets[:3], 1):
                if isinstance(snippet, dict):
                    file = snippet.get("file", "unknown")
                    content = snippet.get("content", str(snippet))[:200]
                    sections.append(f"{i}. Ù…Ù† {file}:\n```\n{content}\n```\n")

        return "\n".join(sections) if sections else "Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø³ÙŠØ§Ù‚ Ø¥Ø¶Ø§ÙÙŠ."

    def _format_examples_section(self, examples: list) -> str:
        """ØªÙ†Ø³ÙŠÙ‚ Ù‚Ø³Ù… Ø§Ù„Ø£Ù…Ø«Ù„Ø©"""
        if not examples:
            return "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø£Ù…Ø«Ù„Ø© Ù…ØªØ§Ø­Ø©."

        formatted = []
        for i, example in enumerate(examples, 1):
            formatted.append(f"**Ù…Ø«Ø§Ù„ {i}:**")
            formatted.append(f"- Ø§Ù„ÙˆØµÙ: {example.get('description', 'N/A')}")
            formatted.append(f"- Ø§Ù„Ù€ Prompt: {example.get('prompt', 'N/A')}")
            if example.get("result"):
                formatted.append(f"- Ø§Ù„Ù†ØªÙŠØ¬Ø©: {example.get('result')}")
            formatted.append("")

        return "\n".join(formatted)

    def _generate_with_llm(self, meta_prompt: str, language: str = "en") -> str:
        """
        ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù€ Prompt Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… LLM - ENHANCED WITH RETRY & FALLBACK

        Generates the final prompt using LLM with proper error handling and fallbacks.
        """
        try:
            if not get_llm_client:
                self.logger.warning("LLM client not available, returning meta-prompt")
                return meta_prompt

            llm = get_llm_client()

            # Prepare system message based on language
            system_messages = {
                "ar": "Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ø¹Ø§Ù„Ù…ÙŠ ÙÙŠ Ù‡Ù†Ø¯Ø³Ø© Prompts. Ù…Ù‡Ù…ØªÙƒ Ø¥Ù†Ø´Ø§Ø¡ prompts Ø§Ø­ØªØ±Ø§ÙÙŠØ© Ø®Ø§Ø±Ù‚Ø©.",
                "en": "You are a world-class expert prompt engineer. Your task is to create superhuman professional prompts.",
                "es": "Eres un experto mundial en IngenierÃ­a de Prompts profesionales.",
                "fr": "Vous Ãªtes un expert mondial en IngÃ©nierie de Prompts professionnels.",
                "zh": "ä½ æ˜¯ä¸–ç•Œçº§çš„æç¤ºå·¥ç¨‹ä¸“å®¶ã€‚",
            }

            system_msg = system_messages.get(language, system_messages["en"])

            # Try with primary model
            try:
                response = llm.chat(
                    messages=[
                        {"role": "system", "content": system_msg},
                        {"role": "user", "content": meta_prompt},
                    ],
                    model=DEFAULT_MODEL,
                    temperature=0.7,
                    max_tokens=4000,
                )

                content = response.get("content", "")
                if content and len(content) > 50:  # Valid response
                    return content

            except Exception as primary_error:
                self.logger.warning(f"Primary model failed: {primary_error}, trying fallback")

                # Try with low-cost fallback model
                try:
                    response = llm.chat(
                        messages=[
                            {"role": "system", "content": system_msg},
                            {"role": "user", "content": meta_prompt},
                        ],
                        model=LOW_COST_MODEL,
                        temperature=0.7,
                        max_tokens=4000,
                    )

                    content = response.get("content", "")
                    if content and len(content) > 50:
                        return content

                except Exception as fallback_error:
                    self.logger.error(f"Fallback model also failed: {fallback_error}")

            # Last resort: return the meta-prompt itself
            self.logger.warning("All LLM attempts failed, returning meta-prompt")
            return meta_prompt

        except Exception as e:
            self.logger.error(f"LLM generation completely failed: {e}", exc_info=True)
            return meta_prompt

    def _get_default_template(self, prompt_type: str) -> PromptTemplate | None:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù‚Ø§Ù„Ø¨ Ø§ÙØªØ±Ø§Ø¶ÙŠ Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹"""
        try:
            template = (
                db.session.query(PromptTemplate)
                .filter_by(category=prompt_type, is_active=True)
                .first()
            )

            return template
        except:
            return None

    def create_template(
        self,
        name: str,
        template_content: str,
        user: User,
        description: str | None = None,
        category: str = "general",
        few_shot_examples: list | None = None,
        variables: list | None = None,
    ) -> dict[str, Any]:
        """
        Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ù„Ø¨ Meta-Prompt Ø¬Ø¯ÙŠØ¯
        """
        try:
            template = PromptTemplate(
                name=name,
                description=description,
                template_content=template_content,
                category=category,
                few_shot_examples=few_shot_examples or [],
                variables=variables or [],
                created_by_id=user.id,
            )

            db.session.add(template)
            db.session.commit()

            self.logger.info(f"Template created: {template.id} - {name}")

            return {
                "status": "success",
                "template_id": template.id,
                "message": f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù‚Ø§Ù„Ø¨ '{name}' Ø¨Ù†Ø¬Ø§Ø­",
            }

        except Exception as e:
            self.logger.error(f"Failed to create template: {e}", exc_info=True)
            db.session.rollback()
            return {"status": "error", "error": str(e), "message": f"âš ï¸ ÙØ´Ù„ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù‚Ø§Ù„Ø¨: {str(e)}"}

    def list_templates(
        self, category: str | None = None, active_only: bool = True
    ) -> list[dict[str, Any]]:
        """Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù‚ÙˆØ§Ù„Ø¨ Ø§Ù„Ù…ØªØ§Ø­Ø©"""
        try:
            query = db.session.query(PromptTemplate)

            if active_only:
                query = query.filter_by(is_active=True)
            if category:
                query = query.filter_by(category=category)

            templates = query.order_by(PromptTemplate.usage_count.desc()).all()

            return [
                {
                    "id": t.id,
                    "name": t.name,
                    "description": t.description,
                    "category": t.category,
                    "usage_count": t.usage_count,
                    "version": t.version,
                }
                for t in templates
            ]

        except Exception as e:
            self.logger.error(f"Failed to list templates: {e}")
            return []

    def rate_prompt(
        self, prompt_id: int, rating: int, feedback_text: str | None = None
    ) -> dict[str, Any]:
        """
        ØªÙ‚ÙŠÙŠÙ… Prompt Ù…ÙˆÙ„Ø¯ (RLHF++ feedback) - ENHANCED WITH AUTO-EXPANSION

        Rates a generated prompt and triggers auto-expansion for highly-rated prompts.
        """
        try:
            if not 1 <= rating <= 5:
                return {"status": "error", "error": "Rating must be between 1 and 5"}

            prompt = db.session.get(GeneratedPrompt, prompt_id)
            if not prompt:
                return {"status": "error", "error": "Prompt not found"}

            prompt.rating = rating
            prompt.feedback_text = feedback_text

            # Update template success rate
            if prompt.template:
                self._update_template_success_rate(prompt.template)

            db.session.commit()

            # Trigger auto-expansion for high ratings
            if rating >= 4:
                self.auto_expand_library(prompt_id, rating)

            # Track in metrics
            if track_metric and ENABLE_METRICS:
                track_metric("prompt_rated", 1, {"rating": rating})

            return {
                "status": "success",
                "message": f"âœ… Ø´ÙƒØ±Ø§Ù‹ Ø¹Ù„Ù‰ ØªÙ‚ÙŠÙŠÙ…Ùƒ ({rating}/5)! Ø³ÙŠØ³Ø§Ø¹Ø¯Ù†Ø§ ÙÙŠ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø®Ø¯Ù…Ø©.\n\nThank you for your rating ({rating}/5)! This helps improve the system.",
            }

        except Exception as e:
            self.logger.error(f"Failed to rate prompt: {e}")
            db.session.rollback()
            return {"status": "error", "error": str(e)}

    def _update_template_success_rate(self, template: PromptTemplate):
        """ØªØ­Ø¯ÙŠØ« Ù…Ø¹Ø¯Ù„ Ù†Ø¬Ø§Ø­ Ø§Ù„Ù‚Ø§Ù„Ø¨"""
        try:
            prompts = (
                db.session.query(GeneratedPrompt)
                .filter_by(template_id=template.id)
                .filter(GeneratedPrompt.rating.isnot(None))
                .all()
            )

            if prompts:
                avg_rating = sum(p.rating for p in prompts) / len(prompts)
                template.success_rate = (avg_rating / 5.0) * 100  # Convert to percentage

        except Exception as e:
            self.logger.warning(f"Failed to update template success rate: {e}")


# Singleton instance
_service_instance = None


def get_prompt_engineering_service() -> PromptEngineeringService:
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù†Ø³Ø®Ø© Ø§Ù„Ø®Ø¯Ù…Ø© (Singleton)"""
    global _service_instance
    if _service_instance is None:
        _service_instance = PromptEngineeringService()
    return _service_instance
