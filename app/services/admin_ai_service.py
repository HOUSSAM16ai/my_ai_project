# -*- coding: utf-8 -*-
"""
ADMIN AI SERVICE - SUPER INTELLIGENCE GATEWAY
Ultra Professional Hyper Edition
=====================================
File        : app/services/admin_ai_service.py
Version     : 1.0.0 â€¢ "OMNISCIENT-ADMIN / ULTRA-CONTEXT-AWARE"
Status      : Production / Hardened / Superintelligent
Author      : Overmind + Maestro Fusion System

MISSION (Ø§Ù„Ù…Ù‡Ù…Ø©)
-------
Ø®Ø¯Ù…Ø© Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø®Ø§Ø±Ù‚Ø© Ù„ØµÙØ­Ø© Ø§Ù„Ø£Ø¯Ù…Ù† ØªØ¯Ù…Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù‚Ø¯Ø±Ø§Øª:
  - ØªØ­Ù„ÙŠÙ„ Ø¹Ù…ÙŠÙ‚ Ù„Ù„Ù…Ø´Ø±ÙˆØ¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Deep Indexer
  - Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„ÙƒØ§Ù…Ù„ Ù…Ù† System Service
  - ØªÙ†ÙÙŠØ° Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Overmind
  - Ø­ÙØ¸ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
  - ØªÙˆÙÙŠØ± ÙˆØ§Ø¬Ù‡Ø© Ù…ÙˆØ­Ø¯Ø© Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ

CORE CAPABILITIES (Ø§Ù„Ù‚Ø¯Ø±Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©)
-----------------
1. project_analysis() - ØªØ­Ù„ÙŠÙ„ Ø¹Ù…ÙŠÙ‚ Ø´Ø§Ù…Ù„ Ù„Ù„Ù…Ø´Ø±ÙˆØ¹
2. answer_question() - Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ù…Ø¹ Ø³ÙŠØ§Ù‚ Ø°ÙƒÙŠ
3. execute_modification() - ØªÙ†ÙÙŠØ° ØªØ¹Ø¯ÙŠÙ„Ø§Øª Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹
4. get_conversation_context() - Ø¬Ù„Ø¨ Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„ÙƒØ§Ù…Ù„
5. save_message() - Ø­ÙØ¸ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
"""

from __future__ import annotations

import json
import os
import time
import logging
from typing import Any, Dict, List, Optional, Tuple
from datetime import datetime, timezone

from flask import current_app, has_app_context
from sqlalchemy import select, desc

from app import db
from app.models import (
    User,
    Mission, MissionStatus, utc_now,
    AdminConversation, AdminMessage, MessageRole
)

try:
    from app.services import generation_service as maestro
except ImportError:
    maestro = None

try:
    from app.services import system_service
except ImportError:
    system_service = None

try:
    from app.services import master_agent_service as overmind
except ImportError:
    overmind = None

try:
    from app.overmind.planning.deep_indexer import build_index, summarize_for_prompt
except ImportError:
    build_index = None
    summarize_for_prompt = None

try:
    from app.services.llm_client_service import get_llm_client
except ImportError:
    get_llm_client = None

logger = logging.getLogger(__name__)

MAX_CONTEXT_MESSAGES = int(os.getenv("ADMIN_AI_MAX_CONTEXT_MESSAGES", "10"))
MAX_RELATED_CONTEXT_CHUNKS = int(os.getenv("ADMIN_AI_MAX_CONTEXT_CHUNKS", "5"))
ENABLE_DEEP_INDEX = os.getenv("ADMIN_AI_ENABLE_DEEP_INDEX", "1") == "1"
DEFAULT_MODEL = os.getenv("DEFAULT_AI_MODEL", "openai/gpt-4o")


class AdminAIService:
    """
    Ø§Ù„Ø®Ø¯Ù…Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙÙŠ ØµÙØ­Ø© Ø§Ù„Ø£Ø¯Ù…Ù†.
    """
    
    def __init__(self):
        self.logger = logger
    
    def analyze_project(
        self, 
        user: User,
        conversation_id: Optional[int] = None,
        focus_areas: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        ØªØ­Ù„ÙŠÙ„ Ø¹Ù…ÙŠÙ‚ Ø´Ø§Ù…Ù„ Ù„Ù„Ù…Ø´Ø±ÙˆØ¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Deep Indexer.
        
        Args:
            user: Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø°ÙŠ ÙŠØ·Ù„Ø¨ Ø§Ù„ØªØ­Ù„ÙŠÙ„
            conversation_id: Ù…Ø¹Ø±Ù Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
            focus_areas: Ù…Ø¬Ø§Ù„Ø§Øª Ø§Ù„ØªØ±ÙƒÙŠØ² (Ø§Ø®ØªÙŠØ§Ø±ÙŠ) Ù…Ø«Ù„ ["security", "performance", "architecture"]
        
        Returns:
            Dict ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„
        """
        start_time = time.time()
        
        try:
            if build_index and ENABLE_DEEP_INDEX:
                self.logger.info(f"Building deep index for project analysis (user_id={user.id})")
                index = build_index(root=".")
                
                analysis = {
                    "status": "success",
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    "user_id": user.id,
                    "project_stats": {
                        "files_scanned": index.get("files_scanned", 0),
                        "total_functions": index.get("global_metrics", {}).get("total_functions", 0),
                        "total_classes": len([c for m in index.get("modules", []) for c in m.get("classes", [])]),
                        "complexity_hotspots": len(index.get("complexity_hotspots_top50", [])),
                        "duplicate_functions": len(index.get("duplicate_function_bodies", {})),
                    },
                    "architecture": self._analyze_architecture(index),
                    "hotspots": self._analyze_hotspots(index),
                    "recommendations": self._generate_recommendations(index),
                    "deep_index_summary": summarize_for_prompt(index, max_len=3000) if summarize_for_prompt else None,
                    "full_index": index,
                    "elapsed_seconds": round(time.time() - start_time, 2)
                }
                
                if conversation_id:
                    self._save_analysis_to_conversation(conversation_id, analysis)
                
                return analysis
            else:
                return {
                    "status": "error",
                    "error": "Deep indexer not available",
                    "elapsed_seconds": round(time.time() - start_time, 2)
                }
                
        except Exception as e:
            self.logger.error(f"Project analysis failed: {e}", exc_info=True)
            return {
                "status": "error",
                "error": str(e),
                "elapsed_seconds": round(time.time() - start_time, 2)
            }
    
    def _analyze_architecture(self, index: Dict) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„Ù…Ø¹Ù…Ø§Ø±ÙŠØ©"""
        layers = index.get("layers", {})
        return {
            "layers_detected": list(layers.keys()),
            "services_count": len(layers.get("service", [])),
            "models_count": len(layers.get("model", [])),
            "routes_count": len(layers.get("route", [])),
            "utils_count": len(layers.get("util", [])),
        }
    
    def _analyze_hotspots(self, index: Dict) -> List[Dict]:
        """ØªØ­Ù„ÙŠÙ„ Ù†Ù‚Ø§Ø· Ø§Ù„ØªØ¹Ù‚ÙŠØ¯"""
        hotspots = index.get("complexity_hotspots_top50", [])[:10]
        return [
            {
                "file": h.get("file"),
                "function": h.get("name"),
                "complexity": h.get("complexity"),
                "lines": h.get("loc"),
            }
            for h in hotspots
        ]
    
    def _generate_recommendations(self, index: Dict) -> List[str]:
        """ØªÙˆÙ„ÙŠØ¯ ØªÙˆØµÙŠØ§Øª Ø°ÙƒÙŠØ©"""
        recommendations = []
        
        hotspots_count = len(index.get("complexity_hotspots_top50", []))
        if hotspots_count > 20:
            recommendations.append(
                f"âš ï¸ ÙˆØ¬Ø¯ {hotspots_count} Ø¯Ø§Ù„Ø© Ø°Ø§Øª ØªØ¹Ù‚ÙŠØ¯ Ø¹Ø§Ù„ÙŠ. ÙŠÙÙ†ØµØ­ Ø¨Ø¥Ø¹Ø§Ø¯Ø© Ù‡ÙŠÙƒÙ„Ø© Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©."
            )
        
        duplicates_count = len(index.get("duplicate_function_bodies", []))
        if duplicates_count > 5:
            recommendations.append(
                f"âš ï¸ ÙˆØ¬Ø¯ {duplicates_count} Ø¯Ø§Ù„Ø© Ù…ÙƒØ±Ø±Ø©. ÙŠÙ…ÙƒÙ† Ø¯Ù…Ø¬Ù‡Ø§ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ØµÙŠØ§Ù†Ø©."
            )
        
        files = index.get("file_metrics", [])
        large_files = [f for f in files if f.get("loc", 0) > 500]
        if large_files:
            recommendations.append(
                f"ğŸ“„ ÙŠÙˆØ¬Ø¯ {len(large_files)} Ù…Ù„Ù ÙƒØ¨ÙŠØ± (>500 Ø³Ø·Ø±). ÙŠÙÙ†ØµØ­ Ø¨ØªÙ‚Ø³ÙŠÙ…Ù‡Ø§."
            )
        
        return recommendations
    
    def _save_analysis_to_conversation(self, conversation_id: int, analysis: Dict):
        """
        Ø­ÙØ¸ Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙÙŠ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© - SUPERHUMAN INTEGRATION
        
        Saves project analysis results to conversation for future reference.
        """
        try:
            conversation = db.session.get(AdminConversation, conversation_id)
            if conversation:
                # Update deep index summary if available
                if analysis.get("deep_index_summary"):
                    conversation.deep_index_summary = analysis["deep_index_summary"]
                
                # Save analysis as a system message
                self._save_message(
                    conversation_id,
                    "system",
                    f"ğŸ“Š Project Analysis Complete\n\n"
                    f"Files: {analysis.get('project_stats', {}).get('files_scanned', 0)}\n"
                    f"Functions: {analysis.get('project_stats', {}).get('total_functions', 0)}\n"
                    f"Hotspots: {analysis.get('project_stats', {}).get('complexity_hotspots', 0)}",
                    metadata_json={"analysis": analysis}
                )
                
                # Add 'analysis' tag
                if conversation.tags and 'analysis' not in conversation.tags:
                    conversation.tags.append('analysis')
                
                db.session.commit()
        except Exception as e:
            self.logger.error(f"Failed to save analysis to conversation: {e}", exc_info=True)
    
    def answer_question(
        self,
        question: str,
        user: User,
        conversation_id: Optional[int] = None,
        use_deep_context: bool = True
    ) -> Dict[str, Any]:
        """
        Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„ÙƒØ§Ù…Ù„ Ù„Ù„Ù…Ø´Ø±ÙˆØ¹.
        
        Args:
            question: Ø§Ù„Ø³Ø¤Ø§Ù„
            user: Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
            conversation_id: Ù…Ø¹Ø±Ù Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
            use_deep_context: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø¹Ù…ÙŠÙ‚
        
        Returns:
            Dict ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ÙˆØ§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¥Ø¶Ø§ÙÙŠØ©
        """
        start_time = time.time()
        
        try:
            conversation_history = []
            deep_index_summary = None
            
            if conversation_id:
                conversation_history = self._get_conversation_history(conversation_id)
                # Get deep index summary from conversation if available
                conversation = db.session.get(AdminConversation, conversation_id)
                if conversation:
                    deep_index_summary = conversation.deep_index_summary
            
            related_context = []
            if system_service and hasattr(system_service, 'find_related_context'):
                try:
                    ctx_result = system_service.find_related_context(
                        question, 
                        limit=MAX_RELATED_CONTEXT_CHUNKS
                    )
                    if ctx_result.ok:
                        related_context = ctx_result.data.get("results", [])
                except Exception as e:
                    self.logger.warning(f"Failed to get related context: {e}")
            
            system_prompt = self._build_super_system_prompt(
                deep_index_summary=deep_index_summary if use_deep_context else None,
                related_context=related_context
            )
            
            messages = [{"role": "system", "content": system_prompt}]
            messages.extend(conversation_history[-MAX_CONTEXT_MESSAGES:])
            messages.append({"role": "user", "content": question})
            
            if get_llm_client:
                client = get_llm_client()
                response = client.chat.completions.create(
                    model=DEFAULT_MODEL,
                    messages=messages,
                    temperature=0.7,
                    max_tokens=2000
                )
                
                answer = response.choices[0].message.content
                tokens_used = getattr(response.usage, 'total_tokens', None)
                model_used = response.model
                
            else:
                answer = "Ø®Ø¯Ù…Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ØºÙŠØ± Ù…ØªØ§Ø­Ø© Ø­Ø§Ù„ÙŠØ§Ù‹."
                tokens_used = None
                model_used = None
            
            elapsed = round(time.time() - start_time, 2)
            
            if conversation_id:
                self._save_message(conversation_id, "user", question)
                self._save_message(
                    conversation_id, 
                    "assistant", 
                    answer,
                    tokens_used=tokens_used,
                    model_used=model_used,
                    latency_ms=elapsed * 1000
                )
            
            return {
                "status": "success",
                "question": question,
                "answer": answer,
                "tokens_used": tokens_used,
                "model_used": model_used,
                "related_context_count": len(related_context),
                "used_deep_index": deep_index_summary is not None,
                "elapsed_seconds": elapsed
            }
            
        except Exception as e:
            self.logger.error(f"Question answering failed: {e}", exc_info=True)
            return {
                "status": "error",
                "error": str(e),
                "elapsed_seconds": round(time.time() - start_time, 2)
            }
    
    def _read_key_project_files(self) -> Dict[str, str]:
        """Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„Ø³ÙŠØ§Ù‚"""
        project_files = {}
        key_files = [
            'docker-compose.yml',
            'README.md',
            'requirements.txt',
            'pyproject.toml',
            'package.json',
            '.env.example',
            'Dockerfile'
        ]
        
        project_root = os.path.abspath('.')
        
        for filename in key_files:
            filepath = os.path.join(project_root, filename)
            if os.path.exists(filepath):
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        content = f.read()
                        if len(content) < 10000:
                            project_files[filename] = content
                        else:
                            project_files[filename] = content[:10000] + "\n[... truncated ...]"
                except Exception as e:
                    self.logger.warning(f"Failed to read {filename}: {e}")
        
        return project_files
    
    def _build_super_system_prompt(
        self,
        deep_index_summary: Optional[str] = None,
        related_context: Optional[List[Dict]] = None
    ) -> str:
        """Ø¨Ù†Ø§Ø¡ System Prompt Ø®Ø§Ø±Ù‚ Ù…Ø¹ ÙƒÙ„ Ø§Ù„Ø³ÙŠØ§Ù‚"""
        parts = [
            "Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø®Ø§Ø±Ù‚ ÙˆÙ…ØªØ®ØµØµ ÙÙŠ ØªØ­Ù„ÙŠÙ„ ÙˆÙÙ‡Ù… Ù…Ø´Ø§Ø±ÙŠØ¹ Ø§Ù„Ø¨Ø±Ù…Ø¬Ø©.",
            "Ù„Ø¯ÙŠÙƒ Ù…Ø¹Ø±ÙØ© Ø¹Ù…ÙŠÙ‚Ø© Ø¨Ø¨Ù†ÙŠØ© Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ ÙˆÙƒÙ„ ØªÙØ§ØµÙŠÙ„Ù‡.",
            "\n## Ù‚Ø¯Ø±Ø§ØªÙƒ:",
            "- ØªØ­Ù„ÙŠÙ„ Ø¹Ù…ÙŠÙ‚ Ù„Ù„ÙƒÙˆØ¯ ÙˆØ§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„Ù…Ø¹Ù…Ø§Ø±ÙŠØ©",
            "- Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© ØªÙ‚Ù†ÙŠØ© Ù…Ø¹Ù‚Ø¯Ø©",
            "- Ø§Ù‚ØªØ±Ø§Ø­ ØªØ­Ø³ÙŠÙ†Ø§Øª ÙˆØ­Ù„ÙˆÙ„",
            "- ØªÙˆØ¶ÙŠØ­ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø¨ÙŠÙ† Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ©",
            "\n## Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:",
            "- Ù…Ù†Ø¸Ù… ÙˆÙ…Ù‡Ù†ÙŠ",
            "- ÙŠØ³ØªØ®Ø¯Ù… Ø£Ù…Ø«Ù„Ø© Ù…Ù† Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„ÙØ¹Ù„ÙŠ",
            "- ÙŠØ´Ø±Ø­ Ø¨Ø§Ù„ØªÙØµÙŠÙ„ Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„ÙˆØ¶ÙˆØ­",
            "- ÙŠØ³ØªØ®Ø¯Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ø­Ø³Ø¨ Ø§Ù„Ø³ÙŠØ§Ù‚",
        ]
        
        project_files = self._read_key_project_files()
        if project_files:
            parts.append("\n## Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©:")
            for filename, content in project_files.items():
                parts.append(f"\n### {filename}:")
                parts.append(f"```\n{content}\n```")
        
        if deep_index_summary:
            parts.extend([
                "\n## Ø¨Ù†ÙŠØ© Ø§Ù„ÙƒÙˆØ¯ (ØªØ­Ù„ÙŠÙ„ Ù‡ÙŠÙƒÙ„ÙŠ):",
                deep_index_summary
            ])
        
        if related_context:
            parts.append("\n## Ø³ÙŠØ§Ù‚ Ø°Ùˆ ØµÙ„Ø©:")
            for i, ctx in enumerate(related_context[:3], 1):
                parts.append(f"\n### Ù…Ù‚Ø·Ø¹ {i} Ù…Ù† {ctx.get('file_path', 'unknown')}:")
                parts.append(ctx.get('content', '')[:500])
        
        return "\n".join(parts)
    
    def execute_modification(
        self,
        objective: str,
        user: User,
        conversation_id: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        ØªÙ†ÙÙŠØ° ØªØ¹Ø¯ÙŠÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Overmind.
        
        Args:
            objective: Ø§Ù„Ù‡Ø¯Ù Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
            user: Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
            conversation_id: Ù…Ø¹Ø±Ù Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
        
        Returns:
            Dict ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù†ØªÙŠØ¬Ø© Ø§Ù„ØªÙ†ÙÙŠØ°
        """
        start_time = time.time()
        
        try:
            if not overmind:
                return {
                    "status": "error",
                    "error": "Overmind service not available",
                    "elapsed_seconds": round(time.time() - start_time, 2)
                }
            
            self.logger.info(f"Creating mission for objective: {objective}")
            mission = overmind.start_mission(objective=objective, initiator=user)
            
            if conversation_id:
                self._save_message(
                    conversation_id,
                    "system",
                    f"ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù‡Ù…Ø© (Mission #{mission.id}) Ù„ØªÙ†ÙÙŠØ°: {objective}",
                    metadata_json={"mission_id": mission.id, "objective": objective}
                )
            
            return {
                "status": "success",
                "mission_id": mission.id,
                "objective": objective,
                "mission_status": mission.status.value,
                "message": f"ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ù‡Ù…Ø© Ø¨Ù†Ø¬Ø§Ø­. ÙŠÙ…ÙƒÙ†Ùƒ Ù…ØªØ§Ø¨Ø¹Ø© ØªÙ‚Ø¯Ù…Ù‡Ø§ Ù…Ù† ØµÙØ­Ø© Mission #{mission.id}",
                "elapsed_seconds": round(time.time() - start_time, 2)
            }
            
        except Exception as e:
            self.logger.error(f"Modification execution failed: {e}", exc_info=True)
            return {
                "status": "error",
                "error": str(e),
                "elapsed_seconds": round(time.time() - start_time, 2)
            }
    
    def create_conversation(
        self,
        user: User,
        title: str,
        conversation_type: str = "general"
    ) -> AdminConversation:
        """
        Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø­Ø§Ø¯Ø«Ø© Ø¬Ø¯ÙŠØ¯Ø© - SUPERHUMAN IMPLEMENTATION
        
        Creates a new conversation with intelligent defaults and metadata.
        Automatically captures project context for superior intelligence.
        
        Args:
            user: Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø°ÙŠ ÙŠØ¨Ø¯Ø£ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
            title: Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
            conversation_type: Ù†ÙˆØ¹ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© (general, project_analysis, modification, etc.)
        
        Returns:
            AdminConversation: Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
        """
        try:
            # Create conversation with enhanced metadata
            conversation = AdminConversation(
                title=title,
                user_id=user.id,
                conversation_type=conversation_type,
                tags=[conversation_type],  # Initial tag
                total_messages=0,
                total_tokens=0,
                is_archived=False
            )
            
            # Optionally capture deep index summary if enabled
            if ENABLE_DEEP_INDEX and build_index and summarize_for_prompt:
                try:
                    index = build_index(root=".")
                    conversation.deep_index_summary = summarize_for_prompt(index, max_len=2000)
                except Exception as e:
                    self.logger.warning(f"Failed to build deep index for conversation: {e}")
            
            db.session.add(conversation)
            db.session.commit()
            
            self.logger.info(f"Created conversation #{conversation.id} for user {user.id}: {title}")
            return conversation
            
        except Exception as e:
            self.logger.error(f"Failed to create conversation: {e}", exc_info=True)
            db.session.rollback()
            raise
    
    def _get_conversation_history(self, conversation_id: int) -> List[Dict[str, str]]:
        """
        Ø¬Ù„Ø¨ ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© - SUPERHUMAN RETRIEVAL
        
        Retrieves conversation history with intelligent formatting.
        Optimized query with proper indexing for blazing-fast performance.
        
        Args:
            conversation_id: Ù…Ø¹Ø±Ù Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
        
        Returns:
            List of message dicts in OpenAI format [{role, content}, ...]
        """
        try:
            messages = AdminMessage.query.filter_by(
                conversation_id=conversation_id
            ).order_by(AdminMessage.created_at).all()
            
            return [
                {
                    "role": msg.role,
                    "content": msg.content
                }
                for msg in messages
            ]
        except Exception as e:
            self.logger.error(f"Failed to get conversation history: {e}", exc_info=True)
            return []
    
    def _save_message(
        self,
        conversation_id: int,
        role: str,
        content: str,
        tokens_used: Optional[int] = None,
        model_used: Optional[str] = None,
        latency_ms: Optional[float] = None,
        metadata_json: Optional[Dict] = None
    ):
        """
        Ø­ÙØ¸ Ø±Ø³Ø§Ù„Ø© ÙÙŠ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© - SUPERHUMAN PERSISTENCE
        
        Saves a message with comprehensive metadata tracking.
        Automatically updates conversation statistics for analytics.
        Uses content hashing for deduplication and integrity.
        
        Args:
            conversation_id: Ù…Ø¹Ø±Ù Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
            role: Ø¯ÙˆØ± Ø§Ù„Ù…Ø±Ø³Ù„ (user, assistant, system, tool)
            content: Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø±Ø³Ø§Ù„Ø©
            tokens_used: Ø¹Ø¯Ø¯ tokens Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©
            model_used: Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
            latency_ms: Ø²Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø¨Ø§Ù„Ù…ÙŠÙ„ÙŠ Ø«Ø§Ù†ÙŠØ©
            metadata_json: Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
        """
        try:
            # Create message with all metadata
            message = AdminMessage(
                conversation_id=conversation_id,
                role=role,
                content=content,
                tokens_used=tokens_used,
                model_used=model_used,
                latency_ms=latency_ms,
                metadata_json=metadata_json
            )
            
            # Compute content hash for integrity and deduplication
            message.compute_content_hash()
            
            db.session.add(message)
            
            # Update conversation statistics
            conversation = db.session.get(AdminConversation, conversation_id)
            if conversation:
                conversation.update_stats()
                conversation.updated_at = utc_now()
            
            db.session.commit()
            
            self.logger.debug(
                f"Saved message to conversation #{conversation_id}: "
                f"role={role}, tokens={tokens_used}, model={model_used}"
            )
            
        except Exception as e:
            self.logger.error(f"Failed to save message: {e}", exc_info=True)
            db.session.rollback()
    
    def get_user_conversations(
        self,
        user: User,
        limit: int = 20,
        include_archived: bool = False,
        conversation_type: Optional[str] = None
    ) -> List[AdminConversation]:
        """
        Ø¬Ù„Ø¨ Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… - SUPERHUMAN QUERY
        
        Retrieves user conversations with intelligent filtering.
        Optimized with composite indexes for enterprise-grade performance.
        
        Args:
            user: Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
            limit: Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª
            include_archived: ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ø§Ù„Ù…Ø¤Ø±Ø´ÙØ©
            conversation_type: ÙÙ„ØªØ±Ø© Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹
        
        Returns:
            List of conversations ordered by last activity
        """
        try:
            query = AdminConversation.query.filter_by(user_id=user.id)
            
            if not include_archived:
                query = query.filter_by(is_archived=False)
            
            if conversation_type:
                query = query.filter_by(conversation_type=conversation_type)
            
            # Order by most recent activity
            query = query.order_by(AdminConversation.updated_at.desc())
            
            if limit:
                query = query.limit(limit)
            
            return query.all()
            
        except Exception as e:
            self.logger.error(f"Failed to get user conversations: {e}", exc_info=True)
            return []
    
    def archive_conversation(self, conversation_id: int) -> bool:
        """
        Ø£Ø±Ø´ÙØ© Ù…Ø­Ø§Ø¯Ø«Ø© - SUPERHUMAN ORGANIZATION
        
        Archives a conversation for intelligent organization.
        Archived conversations are excluded from default queries but remain searchable.
        
        Args:
            conversation_id: Ù…Ø¹Ø±Ù Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
        
        Returns:
            bool: True if successful
        """
        try:
            conversation = db.session.get(AdminConversation, conversation_id)
            if conversation:
                conversation.is_archived = True
                conversation.updated_at = utc_now()
                db.session.commit()
                self.logger.info(f"Archived conversation #{conversation_id}")
                return True
            return False
        except Exception as e:
            self.logger.error(f"Failed to archive conversation: {e}", exc_info=True)
            db.session.rollback()
            return False
    
    def get_conversation_analytics(self, conversation_id: int) -> Dict[str, Any]:
        """
        ØªØ­Ù„ÙŠÙ„Ø§Øª Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© - SUPERHUMAN ANALYTICS
        
        Provides comprehensive analytics for a conversation.
        Surpasses tech giants with detailed metrics and insights.
        
        Args:
            conversation_id: Ù…Ø¹Ø±Ù Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
        
        Returns:
            Dict with comprehensive analytics data
        """
        try:
            conversation = db.session.get(AdminConversation, conversation_id)
            if not conversation:
                return {"status": "error", "error": "Conversation not found"}
            
            messages = conversation.messages
            
            # Message distribution by role
            role_distribution = {}
            for msg in messages:
                role_distribution[msg.role] = role_distribution.get(msg.role, 0) + 1
            
            # Token usage by model
            model_tokens = {}
            for msg in messages:
                if msg.model_used and msg.tokens_used:
                    model_tokens[msg.model_used] = model_tokens.get(msg.model_used, 0) + msg.tokens_used
            
            # Response time statistics
            response_times = [m.latency_ms for m in messages if m.latency_ms and m.role == "assistant"]
            avg_latency = sum(response_times) / len(response_times) if response_times else None
            min_latency = min(response_times) if response_times else None
            max_latency = max(response_times) if response_times else None
            
            # Total cost calculation
            total_cost = sum(float(m.cost_usd or 0) for m in messages)
            
            return {
                "status": "success",
                "conversation_id": conversation_id,
                "title": conversation.title,
                "conversation_type": conversation.conversation_type,
                "created_at": conversation.created_at.isoformat(),
                "updated_at": conversation.updated_at.isoformat(),
                "total_messages": len(messages),
                "role_distribution": role_distribution,
                "total_tokens": conversation.total_tokens,
                "model_tokens": model_tokens,
                "avg_response_time_ms": avg_latency,
                "min_response_time_ms": min_latency,
                "max_response_time_ms": max_latency,
                "total_cost_usd": total_cost,
                "is_archived": conversation.is_archived
            }
            
        except Exception as e:
            self.logger.error(f"Failed to get conversation analytics: {e}", exc_info=True)
            return {"status": "error", "error": str(e)}


_service_instance = None

def get_admin_ai_service() -> AdminAIService:
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù†Ø³Ø®Ø© ÙˆØ§Ø­Ø¯Ø© Ù…Ù† Ø§Ù„Ø®Ø¯Ù…Ø©"""
    global _service_instance
    if _service_instance is None:
        _service_instance = AdminAIService()
    return _service_instance
