import ast
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path

from app.core.logging import get_logger

from .analyzers.complexity import ComplexityAnalyzer
from .analyzers.git import GitAnalyzer
from .analyzers.smells import StructuralSmellDetector
from .models import FileMetrics, ProjectAnalysis

logger = get_logger(__name__)


@dataclass(frozen=True)
class _LineStats:
    """ููุซู ุฅุญุตุงุฆูุงุช ุงูุฃุณุทุฑ ุงูุฃุณุงุณูุฉ ูููู ูุงุญุฏ."""

    code_lines: int
    comment_lines: int
    blank_lines: int


@dataclass(frozen=True)
class _ComplexityStats:
    """ูุญูู ุฅุญุตุงุฆูุงุช ุงูุชุนููุฏ ูุงูุชุฏุงุฎู ุจุดูู ููุญุฏ."""

    avg_complexity: float
    max_complexity: int
    max_func_name: str
    std_dev: float
    avg_nesting: float


@dataclass(frozen=True)
class _HotspotWeights:
    """ููุซู ุฃูุฒุงู ุญุณุงุจ ุงูููุงุท ุงูุณุงุฎูุฉ."""

    complexity: float
    volatility: float
    smell: float


@dataclass(frozen=True)
class _HotspotConfig:
    """ููุซู ุงูุฅุนุฏุงุฏุงุช ุงูุซุงุจุชุฉ ูุญุณุงุจ ุงูููุงุท ุงูุณุงุฎูุฉ."""

    weights: _HotspotWeights


@dataclass(frozen=True)
class _NormalizedRanks:
    """ูุญูู ุงูููู ุงููุทุจุนุฉ ุงูุฎุงุตุฉ ุจุญุณุงุจ ุงูููุงุท ุงูุณุงุฎูุฉ."""

    complexity: list[float]
    volatility: list[float]
    smell: list[float]


@dataclass(frozen=True)
class _ProjectStats:
    """ููุซู ุฅุญุตุงุฆูุงุช ุงููุดุฑูุน ุงูุฅุฌูุงููุฉ."""

    total_lines: int
    total_code: int
    total_functions: int
    total_classes: int
    avg_complexity: float
    max_complexity: int


@dataclass(frozen=True)
class _HotspotBuckets:
    """ูุญูู ููุงุฆู ุงูููุงุท ุงูุณุงุฎูุฉ ุญุณุจ ุงูุฃููููุฉ."""

    critical: list[str]
    high: list[str]


class StructuralCodeIntelligence:
    """Main Structural Intelligence Analyzer"""

    def __init__(self, repo_path: Path, target_paths: list[str]):
        self.repo_path = repo_path
        self.target_paths = target_paths
        self.git_analyzer = GitAnalyzer(repo_path)
        self.smell_detector = StructuralSmellDetector()

        # Exclusion patterns
        self.exclude_patterns = [
            "__pycache__",
            ".pyc",
            "venv",
            "site-packages",
            "migrations",
            ".git",
            "sandbox",
            "playground",
            "experiments",
            "test_",
            "_test.py",
        ]

    def should_analyze(self, file_path: Path) -> bool:
        """Should this file be analyzed?"""
        path_str = str(file_path)

        # Check exclusions
        for pattern in self.exclude_patterns:
            if pattern in path_str:
                return False

        # Must be Python file
        if file_path.suffix != ".py":
            return False

        # Must be in target paths
        return any(target in path_str for target in self.target_paths)

    def _count_lines(self, lines: list[str]) -> _LineStats:
        """
        ุญุณุงุจ ุฃููุงุน ุงูุฃุณุทุฑ ุงููุฎุชููุฉ.
        Count different types of lines.

        Args:
            lines: ูุงุฆูุฉ ุฃุณุทุฑ ุงูููู - List of file lines

        Returns:
            _LineStats: ุฅุญุตุงุฆูุงุช ุงูุฃุณุทุฑ ุงูุฃุณุงุณูุฉ
        """
        code_lines = 0
        comment_lines = 0
        blank_lines = 0

        for line in lines:
            stripped = line.strip()
            if not stripped:
                blank_lines += 1
            elif stripped.startswith("#"):
                comment_lines += 1
            else:
                code_lines += 1

        return _LineStats(
            code_lines=code_lines,
            comment_lines=comment_lines,
            blank_lines=blank_lines,
        )

    def _calculate_complexity_stats(self, functions: list[dict]) -> _ComplexityStats:
        """
        ุญุณุงุจ ุฅุญุตุงุฆูุงุช ุงูุชุนููุฏ ูุงูุชุฏุงุฎู.

        Args:
            functions: ูุงุฆูุฉ ูุนูููุงุช ุงูุฏูุงู

        Returns:
            _ComplexityStats: ููุฎุต ุงูุชุนููุฏ ูุงูุชุฏุงุฎู
        """
        function_complexities = [f["complexity"] for f in functions]
        nesting_depths = [f["nesting_depth"] for f in functions]

        if not function_complexities:
            return _ComplexityStats(
                avg_complexity=0.0,
                max_complexity=0,
                max_func_name="",
                std_dev=0.0,
                avg_nesting=0.0,
            )

        avg_complexity = sum(function_complexities) / len(function_complexities)
        max_complexity = max(function_complexities)

        # Find function with max complexity
        max_func_name = ""
        for f in functions:
            if f["complexity"] == max_complexity:
                max_func_name = f["name"]
                break

        # Calculate standard deviation
        std_dev = self._calculate_standard_deviation(function_complexities, avg_complexity)
        avg_nesting = sum(nesting_depths) / len(nesting_depths) if nesting_depths else 0.0

        return _ComplexityStats(
            avg_complexity=avg_complexity,
            max_complexity=max_complexity,
            max_func_name=max_func_name,
            std_dev=std_dev,
            avg_nesting=avg_nesting,
        )

    def _calculate_standard_deviation(self, values: list[float], mean: float) -> float:
        """
        ุญุณุงุจ ุงูุงูุญุฑุงู ุงููุนูุงุฑู ููุงุฆูุฉ ููู.

        Args:
            values: ุงูููู ุงููุฑุงุฏ ุญุณุงุจ ุงูุญุฑุงููุง ุงููุนูุงุฑู
            mean: ุงููุชูุณุท ุงูุญุณุงุจู ููููู

        Returns:
            float: ุงูุงูุญุฑุงู ุงููุนูุงุฑู
        """
        if len(values) <= 1:
            return 0.0
        variance = sum((value - mean) ** 2 for value in values) / len(values)
        return variance**0.5

    def _create_base_metrics(
        self,
        file_path: Path,
        lines: list[str],
        analyzer: ComplexityAnalyzer,
        line_stats: _LineStats,
        complexity_stats: _ComplexityStats,
    ) -> FileMetrics:
        """
        ุฅูุดุงุก ูุงุฆู FileMetrics ุงูุฃุณุงุณู.

        Args:
            file_path: ูุณุงุฑ ุงูููู
            lines: ุฃุณุทุฑ ุงูููู
            analyzer: ูุญูู ุงูุชุนููุฏ
            line_stats: ุฅุญุตุงุฆูุงุช ุงูุฃุณุทุฑ ุงูุฃุณุงุณูุฉ
            complexity_stats: ุฅุญุตุงุฆูุงุช ุงูุชุนููุฏ ูุงูุชุฏุงุฎู

        Returns:
            FileMetrics: ูุงุฆู ุงูููุงููุณ ุงูุฃุณุงุณูุฉ
        """
        relative_path = str(file_path.relative_to(self.repo_path))

        return FileMetrics(
            file_path=str(file_path),
            relative_path=relative_path,
            total_lines=len(lines),
            code_lines=line_stats.code_lines,
            comment_lines=line_stats.comment_lines,
            blank_lines=line_stats.blank_lines,
            num_classes=len(analyzer.classes),
            num_functions=len(analyzer.functions),
            num_public_functions=sum(1 for f in analyzer.functions if f["is_public"]),
            file_complexity=analyzer.file_complexity,
            avg_function_complexity=round(complexity_stats.avg_complexity, 2),
            max_function_complexity=complexity_stats.max_complexity,
            max_function_name=complexity_stats.max_func_name,
            complexity_std_dev=round(complexity_stats.std_dev, 2),
            max_nesting_depth=analyzer.max_nesting,
            avg_nesting_depth=round(complexity_stats.avg_nesting, 2),
            num_imports=len(analyzer.imports),
            function_details=analyzer.functions,
        )

    def _enrich_with_git_metrics(self, metrics: FileMetrics) -> None:
        """
        ุฅุซุฑุงุก ุงูููุงููุณ ุจูุนูููุงุช Git.

        Args:
            metrics: ูุงุฆู ุงูููุงููุณ ููุฅุซุฑุงุก
        """
        git_metrics = self.git_analyzer.analyze_file_history(metrics.relative_path)
        metrics.total_commits = git_metrics["total_commits"]
        metrics.commits_last_6months = git_metrics["commits_last_6months"]
        metrics.commits_last_12months = git_metrics["commits_last_12months"]
        metrics.num_authors = git_metrics["num_authors"]
        metrics.bugfix_commits = git_metrics["bugfix_commits"]
        metrics.branches_modified = git_metrics["branches_modified"]

    def _enrich_with_smells(self, metrics: FileMetrics, imports: list[dict]) -> None:
        """
        ุฅุซุฑุงุก ุงูููุงููุณ ุจุงูุฑูุงุฆุญ ุงูุจููููุฉ.

        Args:
            metrics: ูุงุฆู ุงูููุงููุณ ููุฅุซุฑุงุก
            imports: ูุงุฆูุฉ ุงูุงุณุชูุฑุงุฏุงุช
        """
        smells = self.smell_detector.detect_smells(metrics.relative_path, metrics, imports)
        metrics.is_god_class = smells["is_god_class"]
        metrics.has_layer_mixing = smells["has_layer_mixing"]
        metrics.has_cross_layer_imports = smells["has_cross_layer_imports"]

    def analyze_file(self, file_path: Path) -> FileMetrics | None:
        """
        ุชุญููู ุดุงูู ูููู ูุงุญุฏ.

        ุชู ุงูุชุญุณูู: ุชูุณูู ุงูุฏุงูุฉ ุฅูู helper methods ุญุณุจ KISS principle

        Args:
            file_path: ูุณุงุฑ ุงูููู ููุชุญููู

        Returns:
            FileMetrics ุฃู None: ููุงููุณ ุงูููู ุฃู None ุนูุฏ ุงููุดู
        """
        try:
            content, lines = self._read_file_content(file_path)

            # ุญุณุงุจ ุงูุฃุณุทุฑ
            line_stats = self._count_lines(lines)

            # ุชุญููู AST
            tree = ast.parse(content)
            analyzer = ComplexityAnalyzer()
            analyzer.visit(tree)

            # ุญุณุงุจ ุงูุฅุญุตุงุฆูุงุช
            complexity_stats = self._calculate_complexity_stats(analyzer.functions)

            # ุฅูุดุงุก ูุงุฆู ุงูููุงููุณ ุงูุฃุณุงุณู
            metrics = self._create_base_metrics(
                file_path,
                lines,
                analyzer,
                line_stats,
                complexity_stats,
            )

            # ุฅุซุฑุงุก ุจููุงููุณ Git
            self._enrich_with_git_metrics(metrics)

            # ุฅุซุฑุงุก ุจุงูุฑูุงุฆุญ ุงูุจููููุฉ
            self._enrich_with_smells(metrics, analyzer.imports)

            return metrics

        except (OSError, UnicodeDecodeError, SyntaxError, ValueError) as exc:
            logger.warning("ุชุนุฐุฑ ุชุญููู ุงูููู: %s ุจุณุจุจ %s", file_path, exc)
            return None

    def _read_file_content(self, file_path: Path) -> tuple[str, list[str]]:
        """
        ูุฑุงุกุฉ ูุญุชูู ุงูููู ูุชุญูููู ุฅูู ุฃุณุทุฑ.

        Args:
            file_path: ูุณุงุฑ ุงูููู

        Returns:
            tuple[str, list[str]]: ุงููุญุชูู ุงููุงูู ููุงุฆูุฉ ุงูุฃุณุทุฑ
        """
        with open(file_path, encoding="utf-8") as file_handle:
            content = file_handle.read()
        return content, content.split("\n")

    def calculate_hotspot_scores(self, all_metrics: list[FileMetrics]) -> None:
        """
        ุญุณุงุจ ุฏุฑุฌุงุช ุงูููุงุท ุงูุณุงุฎูุฉ | Calculate hotspot scores with normalization

        ูููู ุจุชุทุจูุน ุงูููู ูุญุณุงุจ ุงูุฏุฑุฌุงุช ุงูููุฒููุฉ
        Normalizes values and calculates weighted scores

        Args:
            all_metrics: ูุงุฆูุฉ ููุงููุณ ุงููููุงุช | List of file metrics
        """
        if not all_metrics:
            return

        # Extract and normalize values
        ranks = self._extract_and_normalize_metrics(all_metrics)

        # Calculate scores and assign priorities
        self._calculate_weighted_scores(all_metrics, ranks)

    def _extract_and_normalize_metrics(self, all_metrics: list[FileMetrics]) -> _NormalizedRanks:
        """
        ุงุณุชุฎุฑุงุฌ ูุชุทุจูุน ุงูููุงููุณ | Extract and normalize metrics

        Args:
            all_metrics: ูุงุฆูุฉ ุงูููุงููุณ | Metrics list

        Returns:
            _NormalizedRanks: ุงูููู ุงููุทุจุนุฉ ููู ูุฆุฉ
        """
        # Extract values
        complexities = [m.file_complexity for m in all_metrics]
        volatilities = [m.commits_last_12months for m in all_metrics]
        smells = [self._count_smells(m) for m in all_metrics]

        # Normalize
        return _NormalizedRanks(
            complexity=self._normalize_values(complexities),
            volatility=self._normalize_values(volatilities),
            smell=self._normalize_values(smells),
        )

    def _count_smells(self, metrics: FileMetrics) -> int:
        """
        ุนุฏ ุงูุฑูุงุฆุญ ุงูุจููููุฉ | Count structural smells

        Args:
            metrics: ููุงููุณ ุงูููู | File metrics

        Returns:
            ุนุฏุฏ ุงูุฑูุงุฆุญ | Number of smells
        """
        return (
            (1 if metrics.is_god_class else 0)
            + (1 if metrics.has_layer_mixing else 0)
            + (1 if metrics.has_cross_layer_imports else 0)
        )

    def _normalize_values(self, values: list[float]) -> list[float]:
        """
        ุชุทุจูุน ุงูููู ุฅูู ูุทุงู 0-1 | Normalize values to 0-1 range

        Args:
            values: ูุงุฆูุฉ ุงูููู | List of values

        Returns:
            ูุงุฆูุฉ ุงูููู ุงููุทุจุนุฉ | List of normalized values
        """
        if not values or max(values) == 0:
            return [0.0] * len(values)
        max_val = max(values)
        return [v / max_val for v in values]

    def _calculate_weighted_scores(
        self,
        all_metrics: list[FileMetrics],
        ranks: _NormalizedRanks,
    ) -> None:
        """
        ุญุณุงุจ ุงูุฏุฑุฌุงุช ุงูููุฒููุฉ | Calculate weighted scores

        Args:
            all_metrics: ูุงุฆูุฉ ุงูููุงููุณ | Metrics list
            ranks: ุงูููู ุงููุทุจุนุฉ | Normalized ranks
        """
        config = self._hotspot_config()
        for i, metrics in enumerate(all_metrics):
            self._assign_metric_ranks(metrics, ranks, i)
            score = self._calculate_hotspot_score(ranks, i, config.weights)
            metrics.hotspot_score = round(score, 4)
            metrics.priority_tier = self._determine_priority_tier(score)

    def _hotspot_config(self) -> _HotspotConfig:
        """
        ุฅูุดุงุก ุฅุนุฏุงุฏุงุช ุงูููุงุท ุงูุณุงุฎูุฉ ุจุทุฑููุฉ ููุญุฏุฉ.

        Returns:
            _HotspotConfig: ุฅุนุฏุงุฏุงุช ุงูููุงุท ุงูุณุงุฎูุฉ
        """
        return _HotspotConfig(
            weights=_HotspotWeights(complexity=0.4, volatility=0.4, smell=0.2),
        )

    def _assign_metric_ranks(
        self,
        metrics: FileMetrics,
        ranks: _NormalizedRanks,
        index: int,
    ) -> None:
        """
        ุญูุธ ุงูุฑุชุจ ุงููุนูุงุฑูุฉ ููู ููู.

        Args:
            metrics: ููุงููุณ ุงูููู | File metrics
            ranks: ุงูููู ุงููุทุจุนุฉ | Normalized ranks
            index: ููุถุน ุงูููู ูู ุงููุงุฆูุฉ
        """
        metrics.complexity_rank = round(ranks.complexity[index], 4)
        metrics.volatility_rank = round(ranks.volatility[index], 4)
        metrics.smell_rank = round(ranks.smell[index], 4)

    def _calculate_hotspot_score(
        self,
        ranks: _NormalizedRanks,
        index: int,
        weights: _HotspotWeights,
    ) -> float:
        """
        ุญุณุงุจ ุฏุฑุฌุฉ ุงูููุทุฉ ุงูุณุงุฎูุฉ ูููู ูุงุญุฏ.

        Args:
            ranks: ุงูููู ุงููุทุจุนุฉ | Normalized ranks
            index: ููุถุน ุงูููู ูู ุงููุงุฆูุฉ
            weights: ุฃูุฒุงู ุงูุญุณุงุจ

        Returns:
            float: ุงูุฏุฑุฌุฉ ุงูููุฒููุฉ
        """
        return (
            weights.complexity * ranks.complexity[index]
            + weights.volatility * ranks.volatility[index]
            + weights.smell * ranks.smell[index]
        )

    def _determine_priority_tier(self, score: float) -> str:
        """
        ุชุญุฏูุฏ ูุณุชูู ุงูุฃููููุฉ | Determine priority tier

        Args:
            score: ุฏุฑุฌุฉ ุงูููุทุฉ ุงูุณุงุฎูุฉ | Hotspot score

        Returns:
            ูุณุชูู ุงูุฃููููุฉ | Priority tier
        """
        if score >= 0.7:
            return "CRITICAL"
        if score >= 0.5:
            return "HIGH"
        if score >= 0.3:
            return "MEDIUM"
        return "LOW"

    def analyze_project(self) -> ProjectAnalysis:
        """
        ุชุญููู ุงููุดุฑูุน ุจุงููุงูู | Analyze entire project

        ูููู ุจุชุญููู ุฌููุน ุงููููุงุช ูุญุณุงุจ ุงูููุงููุณ
        Analyzes all files and calculates metrics

        Returns:
            ุชุญููู ุงููุดุฑูุน | Project analysis
        """
        self._print_analysis_header()
        all_metrics = self._collect_file_metrics()
        self._calculate_and_sort_hotspots(all_metrics)
        return self._build_project_analysis(all_metrics)

    def _print_analysis_header(self) -> None:
        """
        ุทุจุงุนุฉ ุฑุฃุณ ุงูุชุญููู | Print analysis header
        """
        logger.info("๐ Starting Structural Code Intelligence Analysis...")
        logger.info("๐ Repository: %s", self.repo_path)
        logger.info("๐ฏ Target paths: %s", ", ".join(self.target_paths))

    def _collect_file_metrics(self) -> list[FileMetrics]:
        """
        ุฌูุน ููุงููุณ ุงููููุงุช | Collect file metrics

        ูููู ุจุงูุนุซูุฑ ุนูู ุฌููุน ุงููููุงุช ูุชุญููููุง
        Finds and analyzes all files

        Returns:
            ูุงุฆูุฉ ุงูููุงููุณ | List of metrics
        """
        all_metrics: list[FileMetrics] = []

        for target in self.target_paths:
            target_path = self.repo_path / target
            if not target_path.exists():
                logger.warning("โ๏ธ  Path not found: %s", target_path)
                continue

            logger.info("๐ Analyzing %s...", target)
            self._analyze_target_path(target_path, all_metrics)

        logger.info("โ Analyzed %s files", len(all_metrics))
        return all_metrics

    def _iter_python_files(self, target_path: Path) -> list[Path]:
        """
        ุฅุฑุฌุงุน ูุงุฆูุฉ ูููุงุช ุจุงูุซูู ุฏุงุฎู ุงููุณุงุฑ ุงููุณุชูุฏู.

        Args:
            target_path: ุงููุณุงุฑ ุงููุณุชูุฏู

        Returns:
            list[Path]: ูุงุฆูุฉ ุงููููุงุช ุงูููุชุดูุฉ
        """
        return list(target_path.rglob("*.py"))

    def _analyze_target_path(self, target_path: Path, all_metrics: list[FileMetrics]) -> None:
        """
        ุชุญููู ูุณุงุฑ ูุณุชูุฏู | Analyze target path

        Args:
            target_path: ุงููุณุงุฑ ุงููุณุชูุฏู | Target path
            all_metrics: ูุงุฆูุฉ ุงูููุงููุณ | Metrics list
        """
        for py_file in self._iter_python_files(target_path):
            if self.should_analyze(py_file):
                metrics = self.analyze_file(py_file)
                if metrics:
                    all_metrics.append(metrics)
                    logger.info("  โ %s", metrics.relative_path)

    def _calculate_and_sort_hotspots(self, all_metrics: list[FileMetrics]) -> None:
        """
        ุญุณุงุจ ูุชุฑุชูุจ ุงูููุงุท ุงูุณุงุฎูุฉ | Calculate and sort hotspots

        Args:
            all_metrics: ูุงุฆูุฉ ุงูููุงููุณ | Metrics list
        """
        logger.info("๐ Calculating hotspot scores...")
        self.calculate_hotspot_scores(all_metrics)
        all_metrics.sort(key=lambda m: m.hotspot_score, reverse=True)

    def _build_project_analysis(self, all_metrics: list[FileMetrics]) -> ProjectAnalysis:
        """
        ุจูุงุก ุชุญููู ุงููุดุฑูุน | Build project analysis

        ูุญุณุจ ุงูุฅุญุตุงุฆูุงุช ุงูุฅุฌูุงููุฉ ููุญุฏุฏ ุงูููุงุท ุงูุณุงุฎูุฉ
        Calculates overall statistics and identifies hotspots

        Args:
            all_metrics: ูุงุฆูุฉ ุงูููุงููุณ | Metrics list

        Returns:
            ุชุญููู ุงููุดุฑูุน | Project analysis
        """
        stats = self._calculate_project_statistics(all_metrics)
        hotspots = self._identify_hotspots(all_metrics)

        return ProjectAnalysis(
            timestamp=datetime.now().isoformat(),
            total_files=len(all_metrics),
            total_lines=stats.total_lines,
            total_code_lines=stats.total_code,
            total_functions=stats.total_functions,
            total_classes=stats.total_classes,
            avg_file_complexity=stats.avg_complexity,
            max_file_complexity=stats.max_complexity,
            critical_hotspots=hotspots.critical,
            high_hotspots=hotspots.high,
            files=all_metrics,
        )

    def _calculate_project_statistics(self, all_metrics: list[FileMetrics]) -> _ProjectStats:
        """
        ุญุณุงุจ ุฅุญุตุงุฆูุงุช ุงููุดุฑูุน | Calculate project statistics

        Args:
            all_metrics: ูุงุฆูุฉ ุงูููุงููุณ | Metrics list

        Returns:
            _ProjectStats: ุฅุญุตุงุฆูุงุช ุงููุดุฑูุน
        """
        avg_complexity = self._calculate_average_complexity(all_metrics)
        return _ProjectStats(
            total_lines=sum(m.total_lines for m in all_metrics),
            total_code=sum(m.code_lines for m in all_metrics),
            total_functions=sum(m.num_functions for m in all_metrics),
            total_classes=sum(m.num_classes for m in all_metrics),
            avg_complexity=round(avg_complexity, 2),
            max_complexity=max((m.file_complexity for m in all_metrics), default=0),
        )

    def _calculate_average_complexity(self, all_metrics: list[FileMetrics]) -> float:
        """
        ุญุณุงุจ ูุชูุณุท ุชุนููุฏ ุงููููุงุช.

        Args:
            all_metrics: ูุงุฆูุฉ ุงูููุงููุณ | Metrics list

        Returns:
            float: ูุชูุณุท ุงูุชุนููุฏ
        """
        if not all_metrics:
            return 0.0
        return sum(m.file_complexity for m in all_metrics) / len(all_metrics)

    def _identify_hotspots(self, all_metrics: list[FileMetrics]) -> _HotspotBuckets:
        """
        ุชุญุฏูุฏ ุงูููุงุท ุงูุณุงุฎูุฉ | Identify hotspots

        Args:
            all_metrics: ูุงุฆูุฉ ุงูููุงููุณ ุงููุฑุชุจุฉ | Sorted metrics list

        Returns:
            _HotspotBuckets: ุงูููุงุฆู ุงููุตููุฉ ููููุงุท ุงูุณุงุฎูุฉ
        """
        return _HotspotBuckets(
            critical=[m.relative_path for m in all_metrics[:20]],
            high=[m.relative_path for m in all_metrics[20:40]],
        )
