import ast
from datetime import datetime
from pathlib import Path

from .analyzers.complexity import ComplexityAnalyzer
from .analyzers.git import GitAnalyzer
from .analyzers.smells import StructuralSmellDetector
from .models import FileMetrics, ProjectAnalysis


class StructuralCodeIntelligence:
    """Main Structural Intelligence Analyzer"""

    def __init__(self, repo_path: Path, target_paths: list[str]):
        self.repo_path = repo_path
        self.target_paths = target_paths
        self.git_analyzer = GitAnalyzer(repo_path)
        self.smell_detector = StructuralSmellDetector()

        # Exclusion patterns
        self.exclude_patterns = [
            "__pycache__",
            ".pyc",
            "venv",
            "site-packages",
            "migrations",
            ".git",
            "sandbox",
            "playground",
            "experiments",
            "test_",
            "_test.py",
        ]

    def should_analyze(self, file_path: Path) -> bool:
        """Should this file be analyzed?"""
        path_str = str(file_path)

        # Check exclusions
        for pattern in self.exclude_patterns:
            if pattern in path_str:
                return False

        # Must be Python file
        if file_path.suffix != ".py":
            return False

        # Must be in target paths
        return any(target in path_str for target in self.target_paths)

    def _count_lines(self, lines: list[str]) -> tuple[int, int, int]:
        """
        Ø­Ø³Ø§Ø¨ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ø³Ø·Ø± Ø§Ù„Ù…Ø®ØªÙ„ÙØ©.
        Count different types of lines.

        Args:
            lines: Ù‚Ø§Ø¦Ù…Ø© Ø£Ø³Ø·Ø± Ø§Ù„Ù…Ù„Ù - List of file lines

        Returns:
            tuple: (code_lines, comment_lines, blank_lines)
        """
        code_lines = 0
        comment_lines = 0
        blank_lines = 0

        for line in lines:
            stripped = line.strip()
            if not stripped:
                blank_lines += 1
            elif stripped.startswith("#"):
                comment_lines += 1
            else:
                code_lines += 1

        return code_lines, comment_lines, blank_lines

    def _calculate_complexity_stats(self, functions: list[dict]) -> tuple[float, int, str, float]:
        """
        Ø­Ø³Ø§Ø¨ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªØ¹Ù‚ÙŠØ¯.

        Args:
            functions: Ù‚Ø§Ø¦Ù…Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¯ÙˆØ§Ù„

        Returns:
            tuple: (avg_complexity, max_complexity, max_func_name, std_dev)
        """
        function_complexities = [f["complexity"] for f in functions]

        if not function_complexities:
            return 0.0, 0, "", 0.0

        avg_complexity = sum(function_complexities) / len(function_complexities)
        max_complexity = max(function_complexities)

        # Find function with max complexity
        max_func_name = ""
        for f in functions:
            if f["complexity"] == max_complexity:
                max_func_name = f["name"]
                break

        # Calculate standard deviation
        if len(function_complexities) > 1:
            mean = avg_complexity
            variance = sum((x - mean) ** 2 for x in function_complexities) / len(
                function_complexities
            )
            std_dev = variance**0.5
        else:
            std_dev = 0.0

        return avg_complexity, max_complexity, max_func_name, std_dev

    def _calculate_nesting_stats(self, functions: list[dict]) -> float:
        """
        Ø­Ø³Ø§Ø¨ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªØ¯Ø§Ø®Ù„.

        Args:
            functions: Ù‚Ø§Ø¦Ù…Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¯ÙˆØ§Ù„

        Returns:
            float: Ù…ØªÙˆØ³Ø· Ø¹Ù…Ù‚ Ø§Ù„ØªØ¯Ø§Ø®Ù„
        """
        nesting_depths = [f["nesting_depth"] for f in functions]
        return sum(nesting_depths) / len(nesting_depths) if nesting_depths else 0.0

    def _create_base_metrics(
        self,
        file_path: Path,
        lines: list[str],
        code_lines: int,
        comment_lines: int,
        blank_lines: int,
        analyzer: ComplexityAnalyzer,
        avg_complexity: float,
        max_complexity: int,
        max_func_name: str,
        std_dev: float,
        avg_nesting: float,
    ) -> FileMetrics:
        """
        Ø¥Ù†Ø´Ø§Ø¡ ÙƒØ§Ø¦Ù† FileMetrics Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ.

        Args:
            file_path: Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù
            lines: Ø£Ø³Ø·Ø± Ø§Ù„Ù…Ù„Ù
            code_lines: Ø¹Ø¯Ø¯ Ø£Ø³Ø·Ø± Ø§Ù„ÙƒÙˆØ¯
            comment_lines: Ø¹Ø¯Ø¯ Ø£Ø³Ø·Ø± Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª
            blank_lines: Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø³Ø·Ø± Ø§Ù„ÙØ§Ø±ØºØ©
            analyzer: Ù…Ø­Ù„Ù„ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯
            avg_complexity: Ù…ØªÙˆØ³Ø· Ø§Ù„ØªØ¹Ù‚ÙŠØ¯
            max_complexity: Ø£Ù‚ØµÙ‰ ØªØ¹Ù‚ÙŠØ¯
            max_func_name: Ø§Ø³Ù… Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø£ÙƒØ«Ø± ØªØ¹Ù‚ÙŠØ¯Ø§Ù‹
            std_dev: Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ
            avg_nesting: Ù…ØªÙˆØ³Ø· Ø§Ù„ØªØ¯Ø§Ø®Ù„

        Returns:
            FileMetrics: ÙƒØ§Ø¦Ù† Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        """
        relative_path = str(file_path.relative_to(self.repo_path))

        return FileMetrics(
            file_path=str(file_path),
            relative_path=relative_path,
            total_lines=len(lines),
            code_lines=code_lines,
            comment_lines=comment_lines,
            blank_lines=blank_lines,
            num_classes=len(analyzer.classes),
            num_functions=len(analyzer.functions),
            num_public_functions=sum(1 for f in analyzer.functions if f["is_public"]),
            file_complexity=analyzer.file_complexity,
            avg_function_complexity=round(avg_complexity, 2),
            max_function_complexity=max_complexity,
            max_function_name=max_func_name,
            complexity_std_dev=round(std_dev, 2),
            max_nesting_depth=analyzer.max_nesting,
            avg_nesting_depth=round(avg_nesting, 2),
            num_imports=len(analyzer.imports),
            function_details=analyzer.functions,
        )

    def _enrich_with_git_metrics(self, metrics: FileMetrics) -> None:
        """
        Ø¥Ø«Ø±Ø§Ø¡ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø¨Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Git.

        Args:
            metrics: ÙƒØ§Ø¦Ù† Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ù„Ù„Ø¥Ø«Ø±Ø§Ø¡
        """
        git_metrics = self.git_analyzer.analyze_file_history(metrics.relative_path)
        metrics.total_commits = git_metrics["total_commits"]
        metrics.commits_last_6months = git_metrics["commits_last_6months"]
        metrics.commits_last_12months = git_metrics["commits_last_12months"]
        metrics.num_authors = git_metrics["num_authors"]
        metrics.bugfix_commits = git_metrics["bugfix_commits"]
        metrics.branches_modified = git_metrics["branches_modified"]

    def _enrich_with_smells(self, metrics: FileMetrics, imports: list[dict]) -> None:
        """
        Ø¥Ø«Ø±Ø§Ø¡ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø¨Ø§Ù„Ø±ÙˆØ§Ø¦Ø­ Ø§Ù„Ø¨Ù†ÙŠÙˆÙŠØ©.

        Args:
            metrics: ÙƒØ§Ø¦Ù† Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ù„Ù„Ø¥Ø«Ø±Ø§Ø¡
            imports: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯Ø§Øª
        """
        smells = self.smell_detector.detect_smells(metrics.relative_path, metrics, imports)
        metrics.is_god_class = smells["is_god_class"]
        metrics.has_layer_mixing = smells["has_layer_mixing"]
        metrics.has_cross_layer_imports = smells["has_cross_layer_imports"]

    def analyze_file(self, file_path: Path) -> FileMetrics | None:
        """
        ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù…Ù„Ù ÙˆØ§Ø­Ø¯.

        ØªÙ… Ø§Ù„ØªØ­Ø³ÙŠÙ†: ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¯Ø§Ù„Ø© Ø¥Ù„Ù‰ helper methods Ø­Ø³Ø¨ KISS principle

        Args:
            file_path: Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù Ù„Ù„ØªØ­Ù„ÙŠÙ„

        Returns:
            FileMetrics Ø£Ùˆ None: Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ù…Ù„Ù Ø£Ùˆ None Ø¹Ù†Ø¯ Ø§Ù„ÙØ´Ù„
        """
        try:
            # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù
            with open(file_path, encoding="utf-8") as f:
                content = f.read()
                lines = content.split("\n")

            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø£Ø³Ø·Ø±
            code_lines, comment_lines, blank_lines = self._count_lines(lines)

            # ØªØ­Ù„ÙŠÙ„ AST
            tree = ast.parse(content)
            analyzer = ComplexityAnalyzer()
            analyzer.visit(tree)

            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            avg_complexity, max_complexity, max_func_name, std_dev = (
                self._calculate_complexity_stats(analyzer.functions)
            )
            avg_nesting = self._calculate_nesting_stats(analyzer.functions)

            # Ø¥Ù†Ø´Ø§Ø¡ ÙƒØ§Ø¦Ù† Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
            metrics = self._create_base_metrics(
                file_path,
                lines,
                code_lines,
                comment_lines,
                blank_lines,
                analyzer,
                avg_complexity,
                max_complexity,
                max_func_name,
                std_dev,
                avg_nesting,
            )

            # Ø¥Ø«Ø±Ø§Ø¡ Ø¨Ù…Ù‚Ø§ÙŠÙŠØ³ Git
            self._enrich_with_git_metrics(metrics)

            # Ø¥Ø«Ø±Ø§Ø¡ Ø¨Ø§Ù„Ø±ÙˆØ§Ø¦Ø­ Ø§Ù„Ø¨Ù†ÙŠÙˆÙŠØ©
            self._enrich_with_smells(metrics, analyzer.imports)

            return metrics

        except Exception:
            # Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø¨ØµÙ…Øª
            return None

    def calculate_hotspot_scores(self, all_metrics: list[FileMetrics]) -> None:
        """
        Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø§Øª Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø³Ø§Ø®Ù†Ø© | Calculate hotspot scores with normalization

        ÙŠÙ‚ÙˆÙ… Ø¨ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù‚ÙŠÙ… ÙˆØ­Ø³Ø§Ø¨ Ø§Ù„Ø¯Ø±Ø¬Ø§Øª Ø§Ù„Ù…ÙˆØ²ÙˆÙ†Ø©
        Normalizes values and calculates weighted scores

        Args:
            all_metrics: Ù‚Ø§Ø¦Ù…Ø© Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ù…Ù„ÙØ§Øª | List of file metrics
        """
        if not all_metrics:
            return

        # Extract and normalize values
        ranks = self._extract_and_normalize_metrics(all_metrics)

        # Calculate scores and assign priorities
        self._calculate_weighted_scores(all_metrics, ranks)

    def _extract_and_normalize_metrics(self, all_metrics: list[FileMetrics]) -> dict:
        """
        Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ | Extract and normalize metrics

        Args:
            all_metrics: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ | Metrics list

        Returns:
            Ù…Ø¹Ø¬Ù… Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…Ø·Ø¨Ø¹Ø© | Dictionary of normalized values
        """
        # Extract values
        complexities = [m.file_complexity for m in all_metrics]
        volatilities = [m.commits_last_12months for m in all_metrics]
        smells = [self._count_smells(m) for m in all_metrics]

        # Normalize
        return {
            "complexity": self._normalize_values(complexities),
            "volatility": self._normalize_values(volatilities),
            "smell": self._normalize_values(smells),
        }

    def _count_smells(self, metrics: FileMetrics) -> int:
        """
        Ø¹Ø¯ Ø§Ù„Ø±ÙˆØ§Ø¦Ø­ Ø§Ù„Ø¨Ù†ÙŠÙˆÙŠØ© | Count structural smells

        Args:
            metrics: Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ù…Ù„Ù | File metrics

        Returns:
            Ø¹Ø¯Ø¯ Ø§Ù„Ø±ÙˆØ§Ø¦Ø­ | Number of smells
        """
        return (
            (1 if metrics.is_god_class else 0)
            + (1 if metrics.has_layer_mixing else 0)
            + (1 if metrics.has_cross_layer_imports else 0)
        )

    def _normalize_values(self, values: list[float]) -> list[float]:
        """
        ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù‚ÙŠÙ… Ø¥Ù„Ù‰ Ù†Ø·Ø§Ù‚ 0-1 | Normalize values to 0-1 range

        Args:
            values: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù‚ÙŠÙ… | List of values

        Returns:
            Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…Ø·Ø¨Ø¹Ø© | List of normalized values
        """
        if not values or max(values) == 0:
            return [0.0] * len(values)
        max_val = max(values)
        return [v / max_val for v in values]

    def _calculate_weighted_scores(self, all_metrics: list[FileMetrics], ranks: dict) -> None:
        """
        Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¯Ø±Ø¬Ø§Øª Ø§Ù„Ù…ÙˆØ²ÙˆÙ†Ø© | Calculate weighted scores

        Args:
            all_metrics: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ | Metrics list
            ranks: Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…Ø·Ø¨Ø¹Ø© | Normalized ranks
        """
        # Weight configuration: Complexity + Volatility + Smells
        w1, w2, w3 = 0.4, 0.4, 0.2

        for i, metrics in enumerate(all_metrics):
            # Store individual ranks
            metrics.complexity_rank = round(ranks["complexity"][i], 4)
            metrics.volatility_rank = round(ranks["volatility"][i], 4)
            metrics.smell_rank = round(ranks["smell"][i], 4)

            # Calculate weighted hotspot score
            score = (
                w1 * ranks["complexity"][i] + w2 * ranks["volatility"][i] + w3 * ranks["smell"][i]
            )
            metrics.hotspot_score = round(score, 4)

            # Assign priority tier
            metrics.priority_tier = self._determine_priority_tier(score)

    def _determine_priority_tier(self, score: float) -> str:
        """
        ØªØ­Ø¯ÙŠØ¯ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© | Determine priority tier

        Args:
            score: Ø¯Ø±Ø¬Ø© Ø§Ù„Ù†Ù‚Ø·Ø© Ø§Ù„Ø³Ø§Ø®Ù†Ø© | Hotspot score

        Returns:
            Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© | Priority tier
        """
        if score >= 0.7:
            return "CRITICAL"
        if score >= 0.5:
            return "HIGH"
        if score >= 0.3:
            return "MEDIUM"
        return "LOW"

    def analyze_project(self) -> ProjectAnalysis:
        """
        ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ | Analyze entire project

        ÙŠÙ‚ÙˆÙ… Ø¨ØªØ­Ù„ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª ÙˆØ­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³
        Analyzes all files and calculates metrics

        Returns:
            ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ | Project analysis
        """
        self._print_analysis_header()
        all_metrics = self._collect_file_metrics()
        self._calculate_and_sort_hotspots(all_metrics)
        return self._build_project_analysis(all_metrics)

    def _print_analysis_header(self) -> None:
        """
        Ø·Ø¨Ø§Ø¹Ø© Ø±Ø£Ø³ Ø§Ù„ØªØ­Ù„ÙŠÙ„ | Print analysis header
        """
        print("ðŸ” Starting Structural Code Intelligence Analysis...")
        print(f"ðŸ“ Repository: {self.repo_path}")
        print(f"ðŸŽ¯ Target paths: {', '.join(self.target_paths)}")
        print()

    def _collect_file_metrics(self) -> list:
        """
        Ø¬Ù…Ø¹ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ù…Ù„ÙØ§Øª | Collect file metrics

        ÙŠÙ‚ÙˆÙ… Ø¨Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª ÙˆØªØ­Ù„ÙŠÙ„Ù‡Ø§
        Finds and analyzes all files

        Returns:
            Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ | List of metrics
        """
        all_metrics = []

        for target in self.target_paths:
            target_path = self.repo_path / target
            if not target_path.exists():
                print(f"âš ï¸  Path not found: {target_path}")
                continue

            print(f"ðŸ“‚ Analyzing {target}...")
            self._analyze_target_path(target_path, all_metrics)

        print(f"\nâœ… Analyzed {len(all_metrics)} files")
        return all_metrics

    def _analyze_target_path(self, target_path, all_metrics: list) -> None:
        """
        ØªØ­Ù„ÙŠÙ„ Ù…Ø³Ø§Ø± Ù…Ø³ØªÙ‡Ø¯Ù | Analyze target path

        Args:
            target_path: Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù | Target path
            all_metrics: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ | Metrics list
        """
        py_files = list(target_path.rglob("*.py"))
        for py_file in py_files:
            if self.should_analyze(py_file):
                metrics = self.analyze_file(py_file)
                if metrics:
                    all_metrics.append(metrics)
                    print(f"  âœ“ {metrics.relative_path}")

    def _calculate_and_sort_hotspots(self, all_metrics: list) -> None:
        """
        Ø­Ø³Ø§Ø¨ ÙˆØªØ±ØªÙŠØ¨ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø³Ø§Ø®Ù†Ø© | Calculate and sort hotspots

        Args:
            all_metrics: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ | Metrics list
        """
        print("\nðŸ“Š Calculating hotspot scores...")
        self.calculate_hotspot_scores(all_metrics)
        all_metrics.sort(key=lambda m: m.hotspot_score, reverse=True)

    def _build_project_analysis(self, all_metrics: list) -> ProjectAnalysis:
        """
        Ø¨Ù†Ø§Ø¡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ | Build project analysis

        ÙŠØ­Ø³Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ© ÙˆÙŠØ­Ø¯Ø¯ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø³Ø§Ø®Ù†Ø©
        Calculates overall statistics and identifies hotspots

        Args:
            all_metrics: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ | Metrics list

        Returns:
            ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ | Project analysis
        """
        stats = self._calculate_project_statistics(all_metrics)
        hotspots = self._identify_hotspots(all_metrics)

        return ProjectAnalysis(
            timestamp=datetime.now().isoformat(),
            total_files=len(all_metrics),
            total_lines=stats["total_lines"],
            total_code_lines=stats["total_code"],
            total_functions=stats["total_functions"],
            total_classes=stats["total_classes"],
            avg_file_complexity=stats["avg_complexity"],
            max_file_complexity=stats["max_complexity"],
            critical_hotspots=hotspots["critical"],
            high_hotspots=hotspots["high"],
            files=all_metrics,
        )

    def _calculate_project_statistics(self, all_metrics: list) -> dict:
        """
        Ø­Ø³Ø§Ø¨ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ | Calculate project statistics

        Args:
            all_metrics: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ | Metrics list

        Returns:
            Ù…Ø¹Ø¬Ù… Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª | Statistics dictionary
        """
        return {
            "total_lines": sum(m.total_lines for m in all_metrics),
            "total_code": sum(m.code_lines for m in all_metrics),
            "total_functions": sum(m.num_functions for m in all_metrics),
            "total_classes": sum(m.num_classes for m in all_metrics),
            "avg_complexity": round(
                sum(m.file_complexity for m in all_metrics) / len(all_metrics)
                if all_metrics
                else 0,
                2,
            ),
            "max_complexity": max((m.file_complexity for m in all_metrics), default=0),
        }

    def _identify_hotspots(self, all_metrics: list) -> dict:
        """
        ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø³Ø§Ø®Ù†Ø© | Identify hotspots

        Args:
            all_metrics: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ù…Ø±ØªØ¨Ø© | Sorted metrics list

        Returns:
            Ù…Ø¹Ø¬Ù… Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø³Ø§Ø®Ù†Ø© | Hotspots dictionary
        """
        return {
            "critical": [m.relative_path for m in all_metrics[:20]],
            "high": [m.relative_path for m in all_metrics[20:40]],
        }
